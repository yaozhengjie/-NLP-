9.2 Word2Vec简介

Word2Vec是Google公司于2013年发布的一个开源词向量工具包。该项目的算法理论参考了Bengio在2003年设计的神经网络语言模型。由于此神经网络模型使用了两次非线性变换，网络参数很多，训练缓慢，因此不适合大语料。Mikolov团队对其做了简化，实现了Word2Vec词向量模型。它简单、高效，特别适合从大规模、超大规模的语料中获取高精度的词向量表示。因此，项目一经发布就引起了业界的广泛重视，并在多种NLP任务中获得了良好的效果，成为NLP在语义相似度计算中的重大突破。

Word2Vec及同类的词向量模型都基于如下假设：衡量两个词在语义上的相似性，决定于其邻居词分布是否类似。显然这是源于认知语言学中的“距离象似性”原理：词汇与其上下文构成了一个“象”。当从语料中训练出相同或相近的两个“象”时，无论这两个“象”的中心词汇在字面上是否一致，它们在语义上都是相似的。

自从Word2Vec框架发布之后，无论是在国外还是在国内，该框架都引起了巨大的反响。由于Tomas Mikolov在相关的论文中并没有谈及太多的算法细节，因此对许多NLP的研究人员来说，对该算法的研究一度成为重要的课题。经过两三年的研究，到目前为止，根据发布出来的研究成果，对相关理论的研究已经非常充分，在网络上可以很容易地找到。同时，人们在研究的过程中还对源码做了详尽的注释，感兴趣的读者可以从http://www.threedweb.cn/thread-1598-1-1.html下载带注释的版本，便于学习。

为了使内容具有完整性，本章仅从神经网络的角度对Word2Vec进行简单介绍。

### 9.2.1 词向量及其表达

数学上，词向量可以表示为对词典D中的任意词w，指定一个固定长度的实值向量v(w)∈Rm。v(w)就称为w的词向量，m为词向量的长度。

使用向量来表示词最早是Hinton在1986年的论文Learning Distributed Representations of Concepts中提出的。几年以后，传统的神经网络算法逐渐走向了低潮期，这种思想渐渐被人们所忘却，到2000年之后又开始引起了人们的重视。

第4章简要介绍过One-Hot词向量的概念，这里所说的词向量的分布式表达（Distributed Representation）与第4章的概念不同，它克服了One-Hot词向量的两大缺陷。首先是人们所说的“词汇鸿沟”现象，One-Hot的基本假设是，词与词之间的语义或语法关系是相互独立的，仅从这两个向量中看不出两个词是否有关系，这种独立性不适合词汇语义的比较运算；其次是人们所说的“维度灾难”，随着词典规模的增大，句子构成的词袋模型的维度变得越来越大，非零值的分布却越来越稀疏。这种维度的激增会对计算提出更高的要求。

词向量的Distributed Representation的优势在于将语言中的每一个词映射成一个固定长度的短向量，形如[0.054 656 -0.220 857 0.120 756 -0.170 938 0.215 805…]，用来克服One-Hot表达的上述缺陷。在Word2Vec中，这个短向量的维度是自定义的，默认是100维，根据训练语料的规模可以扩充或收缩（规模大一些的100维，小一些的50维）。

所有这些向量构成一个词向量空间，而每一个向量则可视为该空间中的一个点，这样即可引入“距离”的概念，通过计算词之间的距离（如余弦相似度、欧氏距离等）来判断它们之间的语义相似度。

笔者参考一些相关的文献，给出如下例子。

图9.7所示为著名的跨语言同义词共现的一个案例，很多文献都在用，本书也不能免俗。主要因为这个案例能够透彻地说明问题。该案例源于Tomas Mikolov团队开发的一项语义映射技术，它是一种特殊的机器翻译系统。它通过为两种语言构建不同的语言空间，并在两个空间上建立映射关系，只要实现一个向量空间向另一向量空间的映射和转换，语言翻译即可实现。该技术效果非常不错，英语对西班牙语的机器翻译准确率可达90%。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784796499-03af0e4a-989e-4346-b0b3-3d0c7c885d36.jpeg)

图9.7 著名的跨语言同义词共现的一个案例

例如，将英语和西班牙语两种语言通过Word2Vec训练产生词的分布表示向量。不同语言的词向量分别得到它们对应的词向量空间English（图9.7左图）和Spanish（图9.7右图）。从英语中取出5个词one、two、three、four、five，为方便作图，利用主成分分析（PCA）降维，得到相应的二维向量Pone、Ptwo、Pthree、Pfour、Pfive。在二维平面上将这5个点描出来，如图9.7左图所示。

类似的，在西班牙语中取出uno、dos、tres、cuatro、cinco 5个点（与one、two、three、four、five对应的）。设其在图9.7右图中对应的词向量，用PCA降维后的二维向量分别为Puno、Pdos、Ptres、Pcuatro、Pcinco。将它们在二维平面上描出来（可能还需进行适当旋转），如图9.7右图所示，观察左、右两幅图，容易发现：5个词在两个向量空间中的相对位置差不多。这说明两种不同语言对应向量空间的结构之间具有相似性。从而进一步说明了在词向量空间中利用距离刻画词之间相似性的合理性。

词向量的评价大体上可以分成两种方式：一种是把词向量融入现有系统中，提升现有系统的性能；另一种是直接从语言学的角度对词向量进行分析，如相似度、语义偏移等。

训练一份好的词向量对于之前我们讲过的NLP系统很有价值，特别对于深度学习而言。基于词向量的LSTM架构在中文分词、词性标注、语义组块、命名实体识别方面都取得了良好的效果，在精度上均与概率图模型的算法不相上下，而产生的模型要小得多。后面的章节将给出具体的实现。

### 9.2.2 Word2Vec的算法原理

毫无疑问，Word2Vec属于一种神经网络架构的概率语言模型，对于任何一个神经网络的架构，都要从它的网络结构开始研究：

1．Word2Vec神经网络架构

图9.8所示为Word2Vec神经网络架构，包括：输入层、投影层、隐含层和输出层。其中，W表示投影层与隐含层之间的权值矩阵；U表示隐含层与输出层之间的权值矩阵；P表示隐含层上的偏置向量；q表示输出层上的偏置向量。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784797083-e5cbce65-4e9b-4d1a-95c7-eb7955e5d9cf.jpeg)

图9.8 Word2Vec神经网络架构

设输入语料为C，词向量长度为m，这些都从外部给出。从C中逐个遍历每一个词w，设n为w的上下文长度，则Context(w)就取为其前面的n-1个词（类似于n-gram)，这样(Context(w), w)就构成了一个二元对，这个二元对就构成了一个训练样本。

语料C和词向量长度m给定后，投影层和输出层的规模就确定了：输入层包含了Context(w)中n-1个词的词向量，而投影层的向量Xw是这样构造的：将输入层的n-1个词向量按顺序首尾相接地拼起来，形成一个长向量，其长度当然就是(n-1)m。也就是说，每个词的向量长度都为m，共有n-1个词，因此投影层为(n-1)m。

输出层是一棵Huffman树，每个词汇是这棵树中的一个叶子节点。因此，输出层的维度为No=|D|，即语料C中所有的词汇量数目。

隐藏层的规模Nh是可调参数，由用户从外部指定。

接下来，讨论样本(Context(w), w)经过上述神经网络时是如何参与运算的。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784797698-c66240c6-cf0f-4c95-97a6-520c3bd8360c.jpeg)

第一式为tanh双曲正切函数，用作隐藏层的激活函数。tanh作用在向量上表示它作用在向量的每一个分量上。

第二式计算得到的yw=（yw,1, yw,2, …, yw,N）只是一个长度为N的向量。由于yw的分量的计算结果不能在0～1之间，不能表示某个词w上下文为Context(w)的形式。

第三式通过softmax函数计算得到p(w |Context(w)，下一个词恰为词典D中第i个词的概率。

这里简单说一下softmax函数。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784798151-56e37c61-6750-4fe0-bac4-e0f797c5fe1f.jpeg)

其中，i表示词w在词典D中的索引。Softmax是前文讲过的Logistic函数对于多分类情况的一个扩展。前文讲过Logisitic可以用于二分类的情况，而在多分类的情况下深度学习中最常使用的函数是Softmax。

需要注意的是，上式的最后一项，一般的神经网络输入都是已知的，而Word2Vec中的输入是通过Softmax计算才得到的。

2．CBOW（Continuous Bag-of-Words Model）模型

CBOW是Word2Vec最重要的模型，输入是周围词的词向量，而输出是当前词的词向量。也就是说，通过周围的词来预测当前词。CBOW模型如图9.9所示。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784798682-1cb9af19-8735-4538-a20c-115e7b79c834.jpeg)

图9.9 CBOW模型

CBOW网络包括：输入层、投影层和输出层。还是以(Context(w), w)为例。Context(w)为w的上下文，单侧个数为c，总长度为2c。对网络说明如下。

❑ 输入层。Context(w)中2c个词的词向量v(Context(w)1), v(Context(w)2), …, v(Context (w)2c)∈Rm。这里，m定义为词向量的长度。

❑ 投影层：将输入层的2c个向量做求和累加，即![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784799184-092728bf-fbd9-487f-a20b-f265f395ab2b.jpeg)

❑ 输出层。输出层对应着一棵Huffman树，它是以语料中出现过的词当叶子节点，以各词在语料中出现的次数为权值构造出来的Huffman树。这棵树中，叶子节点共N（=|D|, D为词典的大小）个，非叶子节点为N-1个。

这里需要说明的是，CBOW模型中从输入层到投影层的操作是累加求和，而不是拼接。而且，CBOW中没有隐藏层，投影层直接连接着输出层。

3．Skip-Gram（Continuous Skip-Gram Model）模型

Skip-Gram与CBOW正相反，输入是当前词的词向量，而输出是周围词的词向量。也就是说，通过当前词来预测周围的词。Skip-Gram模型如图9.10所示。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784799754-d494155c-1349-4305-8376-256220e1cfc6.jpeg)

图9.10 Skip-Gram模型

理解了CBOW模型，Skip-Gram的网络结构就非常简单了。它也包括三个层次：输入层、投影层和输出层。还是以(Context(w), w)为例，对网络说明如下。

❑ 输入层。只含当前样本中心词w的词向量，v(w)∈Rm。

❑ 投影层。因为没有上下文，所以只能把w投影到w，这是一个恒等投影、多余步骤。

❑ 输出层。对应着一棵Huffman树。

有关两个网络的优化和推导过程，本节就不给出了。读者可详细阅读《Word2Vec中的数学原理详解》一文，该文对网络的每一个细节都给出了重要的说明。同时，读者可结合《Word2Vec源码解析》一文，了解Word2Vec实现的每个细节。毕竟NLP不仅仅是数学。

### 9.2.3 训练词向量

目前常用的Word2Vec实现共有三个版本，最著名的是Google发布的用C语言编写的Word2Vec，除从Google官网上下载之外，读者也可以从http://www.threedweb.cn/thread-1596-1-1.html下载最新版本的源码。历经三年的时间，此版本已经进行了多次升级，支持多种结构（词汇、短语等）的向量化计算。唯一不足的是该版本只能运行在Linux环境中。

第二个是基于Python的Gensim框架（https://radimrehurek.com/gensim/index.html）。该框架不仅将Word2Vec与Python做了整合，这使得Word2Vec的应用扩展到了Windows平台，而且，Gensim还提供了基于LSA、LDA、HDP的主体框架。该框架希望涵盖词向量表达的所有算法系统。

第三个是基于Java的Word2Vec实现（https://github.com/NLPchina/Word2VEC_java）。该框架有助于读者学习Word2Vec的源码。常用于小型语料的测试。

本节使用的词向量工具是Google公司发布的用C语言编写的Word2Vec。下面的章节会用到Python版本的Gensim框架。

使用Word2Vec进行训练，一般需要大规模的语料（5GB以上），这些语料还需要精准的分词，最终才能体现训练的效果。现在很多大型网站和商业机构都提供相关语料的下载。其中，比较著名的是搜狗实验室提供的互联网语料库完整版（SogouT）。该语料库可从http://www.datatang.com/data/43846下载。数据来自互联网各种类型的1.3亿个原始网页，压缩前的大小超过了5TB，压缩后共计465.00GB。数据版本为2008年完整的网页历史版。数据格式为网页原始内容，保留了HTML标记、页面ID、页面URL和页面原始内容。

这种语料规模的词向量训练需要通过云计算才能完成，很难想象一般性的研究机构或商业公司能在短时间内处理如此规模的互联网数据。本书使用的是微软的MSR分词语料库，规模不大，相比搜狗互联网语料库而言是一个微型的语料库，因为这个阶段的训练结果在后面的LSTM序列标注阶段要使用，所以训练过程仅作为演示使用，但即便这么小规模的语料仍然显示出算法在相似度间的显著效果。

给出如下命令行。

​        ./word2vec -train msr.utf8.txt -output vectors.bin -cbow 0 -size 200 -window 5

​    -negative 0 -hs 1 -sample 1e-3 -threads 12 -binary 1

命令行参数说明如下。

​            -train  <file> : 需要训练成模型的文件名（<file>）

​            -output  <file>：保存为词向量的文件名（<file>）

​            -size  <int>：词向量的维度，默认是100

​            -window  <int>：上下文窗口的大小，默认是5

​            -sample  <float>：设置词频采样的阈值，在训练数据中高于此阈值的词汇会被欠采样

​    （down-sampled），它是一个经验值，忽视掉频率过高的词的参数；默认是1e-3, 可用范围为 (0, 1e-5)。

​            -hs  <int>：使用层次（Hierarchical） Softmax（对罕见字有利），默认是0，

​            -negative  <int>：负采样数量（对常见词和低维向量有利）；默认是5，常用值为3～10（不能

​    为0）

​            -threads  <int>：线程数（默认是12）

​            -iter  <int>：迭代次数（默认是5）

​            -min-count  <int>：少于此词频的词汇会被忽略；默认是5

​            -alpha  <float>：学习参数，skip-gram默认是0.025; CBOW默认是0.05

​            -classes  <int> ：设置词汇的聚类个数

​            -debug  <int>：设置debug模式

​            -binary  <int>：存储词汇的结果向量为二进制模式，默认是0（关闭）

​            -save-vocab  <file>：存储学习出的词向量文件（<file>）

​            -read-vocab  <file>：读取已经学习出的词向量文件（<file>）

​            -cbow  <int>：设置是否使用cbow模型；默认是1（0为skip-gram模型）

输出结果可以有两种：一种是txt格式的词向量文件；另一种是二进制的文件。打开txt格式的文件，部分内容显示如下。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784800088-cf61e808-5ba2-40be-8bb7-b8b02775f317.jpeg)

使用二进制的结果展示词汇间的相似度，代码如下。

​        ./distance vector.bin

输入词汇：文化。

图9.11所示为“文化”语义相似词，是Word2Vec产生的结果。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784800421-b7bd9914-bb14-4a15-84e0-9daa70e0eb43.jpeg)

图9.11 “文化”语义相似词

训练大规模语料是一个既耗时又大量占用CPU资源的过程。很多研究机构为了迅速获得实验的结果，常使用已经训练好的词向量。主要原因还由于对大规模语料的预处理依赖一个同样大规模、高精度、高效率的中文分词器，否则很难达到预想的训练效果。如果你不打算自己训练词向量模型，那么也可以从网上找到相应的词向量模型。现在常用的词向量模型分为收费和免费两大类。即使是收费其价格也不贵，并附带项目的调用程序，内容如下。

（1）相似词、同义词知识库（基于30亿条微博数据，包含调用程序——基于Word2Vec）。读者可从http://www.datatang.com/data/46677下载。

基于30亿条微博数据计算获取的Word2Vec模型，由于训练语料库较大，使得获取到的相似词更精准、涵盖更广，并附带C语言程序，可以非常方便地批量获取指定词的相似词。所获得的结果格式是“词：权重”。

数据大小：755.38MB。

（2）相似词知识库，Word2Vec模型，基于5GB维基百科中文语料。读者可从http://www.datatang.com/data/47233下载。

内含Word2Vec源码、维基百科中文语料Python抽取器、词向量模型，以及基于词向量模型计算词语相似度的Java代码，通过Java工程可以批量计算词语组相似度。词向量模型是基于5GB多的维基百科中文语料训练出来的，精度可以得到保证。训练前已经用OpenCC进行了繁简转换。

数据大小：616.26MB。

如果你希望找到可以免费使用的词向量模型数据，可能规模要小一些，建议你从http://txt2vec.codeplex.com/下载到二进制包。网站给出的案例如下。



手串 1



​        佛珠 0.918781571749997

​        小叶紫檀 0.897870467450521

​        手钏 0.868526208199693

​        菩提子 0.85667693515943

​        紫檀 0.855529437116288

​        佛珠手链 0.849541378712106

​        雕件 0.847901026881494

​        砗磲 0.842016069107114

​        小叶檀 0.839194380950776

​        星月菩提子 0.838186634277951

​        檀香木 0.837212392914782

​        沉香木 0.83575322205817

​        星月菩提 0.83494878072285

​        黄花梨 0.831824567567293

​        平安扣 0.831679080640205

​        紫檀木 0.830029415546653

​        小叶紫檀手串 0.82838028045219

​        原籽 0.823008017930358

​        ……

上述文件是以二进制形式保存的，需要使用https://github.com/zhongkaifu/ RNNSharp解析。这是一个基于CRF-LSTM的高精度命名实体识别工具。项目使用C语言编写，代码并不难读，总词汇数将近700 000个词，生成的词向量数据为200维，部分解析出的结果如下。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784800777-05055ce6-9214-48a5-a447-28aa60a5fa63.jpeg)

解析后总文件大小为2.5GB，压缩成一个大小为1.08GB的文件包，文件名为wordembeding.rar。读者可从http://www.threedweb.cn/thread-1597-1-1.html下载。

### 9.2.4 大规模上下位关系的自动识别

第7章详细分析过HowNet知识库的架构，该知识库由董振东先生主持建设，共历经十年，才最终完成。由此可见，人工构建知识库是一种知识密集且费时的工作。然而，类似HowNet和WordNet这些语义资源却在实际使用中都极大地受限于它们覆盖的领域，效果并不好。因此，许多研究人员多年来都不断尝试自动提取语义关系或构建分类法。表9.1显示了有关研究的执行效果。

表9.1 Word2Vec之前的上下位关系识别方法

|              | P(%)  | R(%)  | F(%)  |
| ------------ | ----- | ----- | ----- |
| Mwiki+CilinE | 92.41 | 60.61 | 73.20 |
| Mpattern     | 97.47 | 21.41 | 35.11 |
| Msnow        | 60.88 | 25.67 | 36.11 |
| MbalApinc    | 54.96 | 53.38 | 54.16 |
| MinvCL       | 49.63 | 62.84 | 55.46 |
| Mfu          | 87.40 | 48.19 | 62.13 |

MWiki+CilinE是由Suchanek等人（2008年）提出的基于人工构建的层次可拓方法。在中文处理中，常用的方法是利用中文维基百科（Wikipedia）的类别分类扩展CilinE（哈工大同义词词林：http://www.ltp-cloud.com/download/）。如表9.1所示，该方法虽然实现了高精度，但召回较低，主要是因为维基百科语料的覆盖范围有限。

中文赫斯特风格词汇模式如表9.2所示。

表9.2 中文赫斯特风格词汇模式

| No   | Pattern           |
| ---- | ----------------- |
| 1    | w是[一个\|一种] h |
| 2    | w[、]等h          |
| 3    | h[, ]叫作w        |
| 4    | h[, ][像]如w      |
| 5    | h[, ]特别是w      |

MPattern是由赫斯特提出的基于模式的方法。在中文处理中，常用的方法是从中文百科语料库中提取上下位关系，百科语料库也被用于训练字嵌入。这里使用中文赫斯特模式（见表9.2），其中，w表示一个字，和h代表其一个上位词。结果表明，仅有一小部分上位词基于这些模式，因为只有少数上位词关系使用这种固定的模式表示，大量上下位关系被表示为更加灵活的形式。

MSnow方法最初是由Snow等人提出的（2005年）。从CilinE学习同样的训练数据用作种子上下位词对。词典式—句法模式使用种子从百科语料库提取。然后，开发一个Logistic回归分类来识别上下位关系。此方法必须依赖于一个精准的句法分析器，并且自动提取模式的质量难以保证。

MbalApinc（Kotlerman等，2010年）和MinvCL（Lenci和Benotto,2012年）很相似，它们将每个词汇表示为一个特征向量，其中每个维度是词汇的一个PMI值及其上下文。通过计算每个词汇对的评分，并应用一个阈值，以确定它是否为一个上下位关系。

Mfu是由Fu等在2013年设计的Web数据挖掘方法。此方法能够挖掘出来自多种来源的给定词汇w的上位词列表，并返回上位词的层次。通过从上位词列表中，选择超过每个词阈值之上的含有评分的上位词。此方法假定，多个来源的经常共现的名词或名词短语n，包含着w，则表明n有可能是w的上位词。如果w是一个实体，则这种方法效果很好。但如果是一个常用的语义概念，则效果不佳。主要的原因可能是，语料中有相当多关于实体介绍的页面，它们不是在网络中的常见的词汇。

由此可见，上述各种方法都存在着缺陷。这个问题是由哈工大的刘挺为首的《大词林》（URL: http://www.bigcilin.com/browser/）项目组最终解决的。下面介绍他们实现自动上下位关系抽取的全过程。

经过大量的研究，他们观察到，词嵌入通过捕捉大量的句法/语义关系，保留了很多语言规律。一个著名的例子：v(king) - v(queen) ≈ v(man) - v(woman)，表明词嵌入向量的差值确实代表两个词对之间的某种语义关系。

需要搞清楚的是，是否这种情况也适用于上下位关系。他们设计了一个简单的实验，使用一些随机抽取的汉语中的上下位词对，计算嵌入向量的差值以测量它们之间的相似性。

结果如表9.3所示。第一个例子表明一个词也可以利用Word向量的差值映射出其上位词。然而，从“木匠”到“工人”比从“金鱼”到“鱼”具有更大偏差，表明上下义关系应该比单独矢量的偏差更复杂。为了验证这个假设，这里计算了在训练集中所有的上下义关系词对的嵌入向量间的偏差，并使之可视化。通过考察图9.12训练集数据中关系聚类的分布，发现上下义关系可以分解为更细粒度的关系。此外，动物的关系在空间上更密切，而人的职业在空间上更分散。

表9.3 在上下位词对中的词嵌入的偏移计算

| No   | Examples                                |
| ---- | --------------------------------------- |
| 1    | v（虾）-v（对虾）≈v（鱼）-v（金鱼）     |
| 2    | v（工人）-v（木匠）≈v（演员）-v（小丑） |
| 3    | v（工人）-v（木匠）v（鱼）-v（金鱼）    |

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784801168-ba905962-6ba9-48cd-ad5d-4628565b30e2.jpeg)

图9.12 训练数据中向量偏差值的聚类

为了应对这一挑战，我们提出了使用投影矩阵学习的上下义关系。

1．均匀线性映射

直观地说，假设所有的词可以基于统一的转移矩阵被映射到其上位词。即，给定一个词x和其上位词y，存在一个矩阵Φ，从而y=Φx。为简单起见，使用与词相同的符号来表示嵌入向量。获得用于所有上下位词对的投影一致的精确Φ是困难的。取而代之的是，可以使用训练数据学习一个近似的Φ，使用式（1），最小化均方误差。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784801445-1ebafc36-765c-4555-b883-75ad45ffd241.jpeg)

其中，N是(x, y)的词汇对在训练数据的数量。这是一个典型的线性回归问题。唯一不同的是，我们的预测是多维向量，而不是标量值。使用随机梯度下降法进行优化。如图9.12所示，向量偏差分布在一些集群中。图9.12左图显示了关于动物的上下位关系。图9.12右图显示了关于人们的职业的关系。

2．分段线性映射

均匀线性映射可能对拟合所有的上下位词对仍有欠表达的情况。这是因为上下位关系是相当多样的。为了更好地对各种上位词—下义关系建模，这里应用分段线性回归的方法。

首先，输入空间被分割为多个区域。即所有词对(x, y)中的训练数据首先聚成几个簇，其中每个簇中的单词对的映射表现出相似的上下位关系。每个词对(x, y)被表示为其向量的偏差：y-x。然后执行聚类，这么做的原因如下：（1）Mikolov的工作已经表明，向量偏差暗示出固定水平的语义关系。（2）使用向量的偏差进行聚类效果更加良好，越接近的词对代表的关系更相似，如图9.12所示。然后对每个簇分别学习一个映射，内容如下。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784801822-62a8d070-c30b-4eac-a442-d3b6b9e384a1.jpeg) （2）

其中，Nk为第k个簇Ck的词对数量。使用K-means聚类算法，k是在开发数据集进行调整的。

3．识别上下位关系

要学习的这个映射矩阵，他们使用哈工大汉语同义词林（扩展版）（CilinE的简称），其中包括100 093个词提取的训练数据（车等，2010年）。CilinE组织为5个层次，其中的每个词都被上下位关系联系在一起构成层次结构。在CilinE每个词都有一个或多个语义代码（有些词汇是多义），指示其层次结构中的位置。

给定训练数据和相应的投影，可以找出两个词是否有上下位关系。给定两个词x和y，找到簇Ck，其中心距离y-x的偏移最近，并得到相应的投影φk。对于y被认为是x的上位词，必须具备如下两个条件中的一个。

条件1：投影φk的Φkx必须足够接近到y。从形式上看，Φkx和y之间的欧氏距离：d(Φkx, y)必须小于阈值δ。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784802137-48517876-ede4-47a0-ad71-64547f44479b.jpeg) （3）

条件2：存在满足![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784802389-fdce6305-bff9-4fff-8e06-0f4c7a69069e.jpeg)，以及![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784802831-ba6c1a75-600b-4c0e-a134-d8f835da1090.jpeg)的一个z。在这种情况下，使用上下位关系的传递性。

其中，![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784803182-9c756599-b8e1-45c1-b89d-d886de722666.jpeg)表示后一个是前一个的上位词。

此外，最终的层次应该是一个DAG（有向无环图）。然而，映射的方法在理论上不能保证，由于映射来自成对的上下位关系，而对整个层次结构并不了解。所有成对上下位关系识别方法都将会遇到这个问题。这是一个有趣的问题，如何构建一个符合DAG形式的全局最优的语义层次结构呢？这不是本文的重点。这里可以给出一些简要的说明，如果一些冲突发生，即存在关系圈，那么试探性去除或颠倒最弱路径（见图9.13）。如果一个圆只有两个节点，那么删除最弱的路径。如果一个圆有两个以上的节点，那么颠倒构成间接上下位关系的最弱路径的方向。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784803415-b3241817-1807-4ea4-9695-148ac9275c34.jpeg)

图9.13 三种路径去除或翻转

如图9.13（a）所示，如果d(Φjy,x)>d(Φkx,y)，那么移除从y到x的路径；如图9.13 （b）所示，如果d(Φjy, x) >d(Φkx, z)并且d(Φjy, x) > d(Φiz, y)，那么翻转从y到x的路径方向。

经过这样的训练过程，他们设计了三种模型，并给出了三种模型的训练结果，如表9.4所示。

表9.4 Word2Vec之后的上下位关系识别效果

|                   | P(%)  | R(%)  | F(%)  |
| ----------------- | ----- | ----- | ----- |
| MEmb              | 80.54 | 67.99 | 73.74 |
| MEmb+CilinE       | 80.59 | 72.42 | 76.29 |
| M Emb+wiki+CilinE | 79.78 | 80.81 | 80.29 |

表9.4中，MEmb仅使用了词嵌入向量对训练结果，而未引入外部手工构建的上下位词典资源。

MEmb和CilinE可以组合。组合策略是把这两种方法得到的正向（Positive）结果简单地合并到一起，然后基于上下位关系的传递性推断出新关系。简单的组合就使F值从73.74%进一步提升至76.29%。

需要注意的是，虽然组合的方法实现的召回超过MEmb 4.43%，但精度几乎没有变化。其原因是，基于自动识别关系的推理可能导致错误传播。例如，由M推断的关系![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784803925-f195a630-fe20-4fee-9fd4-5dd6221eedcd.jpeg)是错误的。当新关系![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784804471-bbfcdd5c-833c-4717-a096-b071b3c9245c.jpeg)从CilinE加入进来时，将造成新的错误关系![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784804988-a199aa47-b7e9-409b-bdb0-35794a11bba1.jpeg)。

MEmb与Wiki+CilinE结合比基准的Wiki+CilinE在F值上提升了7%。因此，MEmb方法与手动构建层次扩展方法的互补结果是最佳的选择。

MEmb是基于词嵌入了该方法。如表9.4所示，该方法实现了比所有以前方法都好得多的召回率和F1值。它超过之前的所有的方法成为最终的选择。
