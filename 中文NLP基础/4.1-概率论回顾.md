4.1 概率论回顾

下面先回顾一下有关概率论的几个概念。

### 4.1.1 多元概率论的几个基本概念

1．联合概率分布

多维随机变量X1, …, Xn，可以用联合概率分布P(X1, …, Xn)描述其各个状态的概率，简称为联合概率分布。它是一个定义在所有变量状态空间的笛卡儿乘积之上的函数。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784558770-06d626a8-ecef-4311-8023-015117c95f49.jpeg)

其中，所有函数值之和为1，即

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784560170-fbb9d203-5945-4af5-a843-2ca382f4eb5c.jpeg)

联合概率分布经常被表示为一张表，其中包含了![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784561217-a314ca50-ed33-4aa1-b3bb-b84be222f6d0.jpeg)个状态组合及其概率值。如果所有变量都只取两个状态，则联合分布表共有2n项，刻画了变量之间的各种关系。

2．边缘概率分布

记X={X1, …, Xn}, Y是X的真子集，即Y⊂X, Z=X\Y（\：表示逆集）。则相对于P(X), Y的边缘分布P(Y)定义为

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784561935-8c433624-2932-4e47-9ffb-270ffe9b6f27.jpeg)

从联合分布P(X)到边缘分布P(Y)的过程称为边缘化。

例1 考虑中国香港市场上所有的出租房屋，从中随机抽取一间，考查其月租（R）和类型（T）这两个随机变量，月租可分为4等：{（low（低于2 000元）, medium（2 000～6 000元）, upper medium（6 000～12 000元）, high（高于12 000元）}；类型有3种：{public（公屋）, private（私家屋）, others（其他）}。联合分布和边缘分布如表4.1所示。

表4.1 联合分布和边缘分布

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784562350-43178814-3e52-4b66-ab0b-85da51ee2337.jpeg)

3．条件概率分布

（1）条件概率：设A、B为两个随机事件且P(B)>0，事件A在给定事件B发生时的条件概率定义为

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784565541-67caa42d-5b36-47d6-b08a-c0c9ffd08d8f.jpeg)

直观上，P(A|B)是在已知B发生时，对A发生的信度；而P(A)则是在不知道B是否发生时，对A发生的信度，由上式可得：

P (A∩B)+P(A|B)P(B)

这称为概率的乘法定律，其含义非常直观：对A和B同时发生的信度，等于B发生的信度乘以已知B发生时对A发生的信度。乘法定律可以写为

P (A∩B)+P(A)P(B|A)

（2）条件概率分布：设X和Y是两个随机变量，x和y分别是它们的一个取值，考虑事件X=x在给定Y=y时的条件概率为

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784567854-0ba0cfa0-fbc6-460e-8ca6-0cacd0c5e497.jpeg)

在上式中，固定y，让x在Ωx上变动，则得到一个在Ωx上的函数。该函数称为在给定Y=y时变量X的条件概率分布，记为P(X|Y=y)。用P(X|Y)记{P(X|Y=y)|y∈Ωy}，即在Y取不同值时X的条件概率分布的集合。P(X|Y)称为给定Y时变量X的条件概率分布。在上式中，让x和y在Ωx和Ωy上变动，则得到一组等式，把这些等式缩写为

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784568792-090a4b30-008b-421f-a4a4-1cd96dd3590c.jpeg)

上式可视为P(X|Y)的直接意义。

一般的，设X={X1, …, Xn}和Y={Y1, …, Ym}为两个变量集合，P(X, Y)为X∪Y的联合概率分布，P(Y)为Y的边缘概率分布，则给定Y时X的条件概率分布定义为

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784577240-57cd2751-8482-44f2-8d50-35b3b9b50c26.jpeg)

根据例1提出问题。随机抽取一间私家屋，其租金为low的概率为多大？此即问给定T=private时R=low的条件概率：P(R=low | T=private)。按定义，有：

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784578244-169438cd-6e8b-4280-9c8f-48093b14de3a.jpeg)

若给定T时，变量R的条件分布如表4.2所示。

表4.2 条件分布

|              | public   | private   | others    |
| ------------ | -------- | --------- | --------- |
| low          | 0.17/0.7 | 0.01/0.25 | 0.02/0.05 |
| medium       | 0.44/0.7 | 0.03/0.25 | 0.01/0.05 |
| upper medium | 0.09/0.7 | 0.07/0.25 | 0.01/0.05 |
| high         | 0.00/0.7 | 0.14/0.25 | 0.01/0.05 |

转置表4.2得到条件概率分布如表4.3所示。

表4.3 条件概率分布

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784579107-871976b7-c4c3-4534-9165-69d708fed3d5.jpeg)

4．独立性与条件独立性

P(X|Y=y)是已知Y=y时，变量X的概率分布，而P(X)是未知Y的取值时X的概率分布。所以，变量x与y相互独立意味着：对变量Y的取值的了解不会改变变量X的概率分布。同样，对变量X的取值的了解也不会改变变量Y的概率分布。

一般的，称随机变量X1, X2, …, Xn相互（边缘）独立，如果P(X1, X2, …, Xn)=P(X1)P(X2)…P(Xn)。

条件独立性定义：假如P(A|B∩C)=P(A|C)或者P(B∩C)=0，则事件A在给定事件C时，在分布P上条件独立于事件B，记作P=(A⊥B|C)。也就是说，如果P满足(A⊥B|C)，当且仅当P(A∩B|C)=P(A|C) P( B|C)。

### 4.1.2 贝叶斯与朴素贝叶斯算法

贝叶斯公式最早是由英国神学家贝叶斯提出来的，用来描述两个条件概率直接的关系。在之前的条件概率定义中，我们知道

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784582518-463f6258-887a-4826-993e-399265065f6a.jpeg)

由上式进一步推导得到：

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784582917-9cd85ef4-9621-4e14-9edb-782750bd3c39.jpeg)

由此，推广到随机变量的范畴，设X, Y为两个随机变量，得到贝叶斯公式：

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784583294-c030158a-0d4a-44b3-bf16-43e21cb93486.jpeg)

其中，P(Y)叫作先验概率，P(Y|X)叫作后验概率，P(Y, X)是联合概率。

在机器学习的视角下，我们把X理解成“具有某种特征”，把Y理解成“类别标签”：

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784583913-fb1c5ba1-d657-4ef6-9429-abcddfb326e2.jpeg)

贝叶斯方法把计算“具有某特征的条件下属于某类”的概率转换成需要计算“属于某类的条件下具有某特征”的概率，属于有监督学习。

朴素贝叶斯理论源于随机变量的独立性，之所以称之为朴素是因为其思想基础的简单性：就文本分类而言，从朴素贝叶斯的角度来看，句子中的两两词之间的关系是相互独立的，即一个对象的特征向量中每个维度都是相互独立的。这是朴素贝叶斯理论的思想基础。在多维的情况下，朴素贝叶斯分类的表示为如下内容。

（1）设x={a1, a2, …, am}为一个待分类项，而每个a为x的一个属性特征。

（2）有类别集合C={y1, y2, …, yn}。

（3）计算p(y1|x), p(y2|x), …, p(yn|x)。

（4）如果p(yk|x)=max{p(y1|x), p(y2|x), …, p(yn|x)}，则x∈yk。

那么，现在的关键就是如何计算第（3）步中的各个条件概率。

（1）找到一个已知分类的待分类项集合，也就是训练集。

（2）统计得到在各类别下各个特征属性的条件概率估计，即

p(a1|y1), p(a2|y1), …, p(am|y1);

p(a1|y2), p(a2|y2), …, p(am|y2);

……

p(am|yn), p(am|yn), …, p(am|yn)。

（3）如果各个特征属性是条件独立的（或者假设它们之间是相互独立的），则根据贝叶斯定理有如下推导：

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784584438-67c8a16a-a4d8-43f1-803a-dd5425e6637c.jpeg)

因为分母对于所有类别为常数，只要将分子最大化皆可。又因为各特征属性是条件独立的，所以有：

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784585474-d9a02baf-3aff-413a-90b0-3b66c7355b46.jpeg)

根据上述分析，朴素贝叶斯分类的流程可以表示如下。

第一阶段，训练数据生成训练样本集：TF-IDF。

第二阶段，对每个类别计算P(yi)。

第三阶段，对每个特征属性计算所有划分的条件概率。

第四阶段，对每个类别计算p(x|yi)p(yi)。

第五阶段，以p(x|yi)p(yi)的最大项作为x的所属类别。

### 4.1.3 文本分类

在处理文本分类之前，引入如下两个概念。

1．One-Hot表达

文本分类的结构化方法就是One-Hot表达（Representation）模型。它是最直观，也是目前为止最常用的词表示方法，虽然越来越多的实践已经证明，这种模型存在局限性，但它仍在文本分类中得到广泛应用。

假设把语料库中的所有词都收集为一个词典D，词典容纳了语料库中所有句子的词汇。One-Hot方法就是把每个词表示为一个长长的向量。这个向量的维度是词典大小，其中绝大多数元素为0，只有一个维度的值为1。这个维度就代表了当前的词。例如，一个语料库有三个文本，代码如下。

​        文本1:My dog ate my homework。

​        文本2:My cat ate the sandwich。

​        文本3:A dolphin ate the homework。

这三个文本生成的词典共有9个词，代码如下。

​        [a, ate, cat, dolphin, dog, homework, my, sandwich, the]

这9个词依次表示为One-Hot向量的形式如下。

​                            “a”       →    [100000000]

​                            “ate”     →    [010000000]

​                              …        …         …

​                            “the”     →    [000000001]

其中，每个词都是“茫茫0海”中的一个1。这种稀疏的表达方式就是One-Hot表达。它的特点是相互独立地表示语料中的每个词。词与词在句子中的相关性被忽略了，这正符合朴素贝叶斯对文本的假设。

2．权重策略：TF-IDF方法

由One-Hot向量形式构成的句子就是一个词袋模型，它将文本中的词和模式串转换为数字，而整个文本集也都转换为维度相等的词向量矩阵。

仍旧使用上述三个文本，直观上，文本的词向量可以使用二值表示，内容如下。

文本1:0,1,0,0,1,1,1,0,0（注意，尽管“my”出现了两次，但二元向量表示中仍然是“1”）。

文本2:0,1,1,0,0,0,1,1,1。

文本3:1,1,0,1,0,1,0,0,1。

这种方式的问题是忽略了一个句子中出现多个相同词的词频信息，增加这些词频信息，就变成了整型计数方式。使用整形计数方式的词向量表示如下。

文本1:0,1,0,0,1,1,2,0,0（请注意，“my”在句子中出现了两次）。

文本2:0,1,1,0,0,0,1,1,1。

文本3:1,1,0,1,0,1,0,0,1。

接下来，对整型计数方式进行归一化。归一化可以避免句子长度不一致问题，便于算法计算，而且对于基于概率算法，词频信息就变为了概率分布。这就是文档的TF信息，内容如下。

文本1:0,1/5,0,0,1/5,1/5,2/5,0,0（请注意，“my”在句子中出现了两次）。

文本2:0,1/5,1/5,0,0,0,1/5,1/5,1/5。

文本3:1/5,1/5,0,1/5,0,1/5,0,0,1/5。

但是这里还有一个问题，如何体现生成的词袋中的词频信息呢？

原信息：a (1), ate (3), cat (1), dolphin (1), dog (1), homework (2), my (3), sandwich (1), the (2)。

注意：由于词袋收集了所有文档中的词，这些词的词频是针对所有文档的词频，因此，词袋的统计基数是文档数。

词条的文档频率：a (1/3), ate (3/3), cat (1/3), dolphin (1/3), dog (1/3), homework (2/3), my (2/3), sandwich (1/3), the (2/3)。

词袋模型的IDF权重如下。

IDF:a log(3/1), ate log(3/3), cat log(3/1), dolphin log(3/1), dog log(3/1), homework log (3/2), my log (3/2), sandwich log (3/1), the log (3/2)。

TF-IDF权重策略：计算文本的权重向量，应该选择一个有效的权重方案。最流行的方案是对TF-IDF权重的方法。TF-IDF的含义是词频—逆文档频率，其含义是如果某个词或短语在一篇文章中出现的频率TF高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。在前面的例子中，在文本1“my”的词频是2，因为它在文档中出现了两次。而在这三个文件集合中，“my”词条的文档频率也是2。TF-IDF的假设是，高频率词应该具有高权重，除非它也是高文档频率。“my”这个词在文本中是经常出现的词汇。它不仅多次发生在单一的文本中，几乎也发生在每个文档中。逆文档频率就是使用词条的文档频率来抵消该词的词频对权重的影响，而得到一个较低的权重。

下面给出定义和公式（来自百度百科）。

词频（Term Frequency, TF）是指某一个给定的词语在该文件中出现的频率。这个数字是对词数（Term Count）的归一化，以防止它偏向长的文件。对于在某一特定文件中的词语来说，它的重要性可表示为：

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784586036-22335476-55e8-4b5e-af20-8ba1193ea2d3.jpeg)

上述式子中分子是该词在文件中的出现次数，而分母则是在文件中所有字词的出现次数之和。

逆向文件频率（Inverse Document Frequency, IDF）是一个词语普遍重要性的度量。某一特定词语的IDF，可以由总文件数目除以包含该词语之文件的数目，再将得到的商取对数得到：

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784586585-8f3c0318-3d9d-4fc7-9e83-8e74a253c975.jpeg)

其中，|D|：语料库中的文件总数；j：包含词语的文件数目，如果该词语不在语料库中，就会导致分母为零。因此，一般情况下使用1+|{d∈D:t∈d}|作为分母；然后再计算TF与IDF的乘积。

某一特定文件内的高词语频率，以及该词语在整个文件集合中的低文件频率，TFIDFij=TFij I×IDFij可以产生出高权重的TF-IDF。因此，TF-IDF倾向于过滤掉常见的词语，保留重要的词语。

### 4.1.4 文本分类的实现

首先创建名为Nbayes_lib.py的文件，这个文件用来编写导入的数据和朴素贝叶斯类的代码。

为了将主要精力都集中在算法本身，使用最简单的语料作为数据集，代码如下。

​        def loadDataSet():

​            postingList=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],

​                      ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],

​                      ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him', 'my'],

​                      ['stop', 'posting', 'stupid', 'worthless', 'garbage'],

​                      ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],

​                      ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]

​            classVec = [0,1,0,1,0,1]    #1 is abusive, 0 not

​            return postingList, classVec

postList是训练集文本，classVec是每个文本对应的分类。根据前面所讲的步骤，逐步实现贝叶斯算法的全过程。

编写一个贝叶斯算法类，并创建默认的构造方法，代码如下。

​        class NBayes(object):

​            def __init__(self):

​                      self.vocabulary = [] # 词典

​                      self.idf=0          # 词典的idf权值向量

​                      self.tf=0           # 训练集的权值矩阵

​                      self.tdm=0         # P(x|yi)

​                      self.Pcates = {}      # P(yi)是一个类别词典

​                      self.labels=[]        # 对应每个文本的分类，是一个外部导入的列表

​                      self.doclength = 0    # 训练集文本数

​                      self.vocablen = 0     # 词典词长

​                      self.testset = 0      # 测试集

​            def train_set(self, trainset, classVec): #导入和训练数据集，生成算法必需的参数和数

​                                                    \# 据结构：

​                      self.cate_prob(classVec)   # 计算每个分类在数据集中的概率：P(yi)

​                      self.doclength = len(trainset)

​                      tempset = set()

​                      [tempset.add(word) for doc in trainset for word in doc ] # 生成词典

​                      self.vocabulary = list(tempset)

​                      self.vocablen = len(self.vocabulary)

​                      self.calc_wordfreq(trainset)  # 计算词频数据集

​                      self.build_tdm()          # 按分类累计向量空间的每维值：P(x|yi)

​            def cate_prob(self, classVec):       # 计算在数据集中每个分类的概率：P(yi)

​                      self.labels = classVec

​                      labeltemps = set(self.labels) # 获取全部分类

​                      for labeltemp in labeltemps:

​                        \# 统计列表中重复的分类：self.labels.count(labeltemp)

​                        self.Pcates[labeltemp] = float(self.labels.count(labeltemp))/

  float(len(self.labels))

​            \# 生成普通的词频向量

​            def calc_wordfreq(self, trainset):

​                      self.idf = np.zeros([1, self.vocablen]) # 1＊词典数

​                      self.tf = np.zeros([self.doclength, self.vocablen]) # 训练集文件数＊

​                                                                          \# 词典数

​                      for indx in xrange(self.doclength):   # 遍历所有的文本

​                        for word in trainset[indx]:         # 遍历文本中的每个词

​                              self.tf[indx, self.vocabulary.index(word)] +=1

​            \# 找到文本的词在词典中的位置+1

​                        for signleword in set(trainset[indx]):

​                              self.idf[0, self.vocabulary.index(signleword)] +=1

​            \#按分类累计向量空间的每维值：P(x|yi)

​            def build_tdm(self):

​                      self.tdm = np.zeros([len(self.Pcates), self.vocablen]) #类别行＊词典列

​                      sumlist = np.zeros([len(self.Pcates),1])  # 统计每个分类的总值

​                      for indx in xrange(self.doclength):

​                self.tdm[self.labels[indx]] += self.tf[indx]    # 将同一类别的词向

​                                                                \# 量空间值加总

​                \# 统计每个分类的总值——是一个标量

​                sumlist[self.labels[indx]]=

np.sum(self.tdm[self.labels[indx]])

​            self.tdm = self.tdm/sumlist      # 生成P(x|yi)

  def map2vocab(self, testdata): #将测试集映射到当前词典

​            self.testset = np.zeros([1, self.vocablen])

​            for word in testdata:

​                self.testset[0, self.vocabulary.index(word)] +=1

  def predict(self, testset): #预测分类结果，输出预测的分类类别

​            if np.shape(testset)[1]! =self.vocablen:# 如果测试集长度与词典不相等，

​                                                    \# 则退出程序

​                print "输入错误"

​                exit(0)

​            predvalue = 0    # 初始化类别概率

​            predclass = ""    # 初始化类别名称

​            for tdm_vect, keyclass in zip(self.tdm, self.Pcates):

​            \# P(x|yi) P(yi)

​                temp = np.sum(testset＊tdm_vect＊self.Pcates[keyclass])

  \# 变量tdm，计算最大分类值

​                if temp > predvalue:

​                    predvalue = temp

​                    predclass = keyclass

​            return predclass

  \# 生成 tf-idf

  def calc_tfidf(self, trainset):

​            self.idf = np.zeros([1, self.vocablen])

​            self.tf = np.zeros([self.doclength, self.vocablen])

​            for indx in xrange(self.doclength):

​                for word in trainset[indx]:

​                    self.tf[indx, self.vocabulary.index(word)] +=1

​                \# 消除不同句长导致的偏差

​                self.tf[indx] = self.tf[indx]/float(len(trainset[indx]))

​                for signleword in set(trainset[indx]):

​                    self.idf[0, self.vocabulary.index(signleword)] +=1

​            self.idf = np.log(float(self.doclength)/self.idf)

​            self.tf = np.multiply(self.tf, self.idf) # 矩阵与向量的点乘 tf x idf

执行代码如下。

​      \# -＊- coding: utf-8 -＊-

​        import sys

​        import os

​        from numpy import ＊

​        import numpy as np

​        from Nbayes_lib import ＊

​        dataSet, listClasses = loadDataSet()  # 导入外部数据集

​        \# dataset： 句子的词向量，

​        \# listClass是句子所属的类别 [0,1,0,1,0,1]

​        nb = NBayes()                 # 实例化

​        nb.train_set(dataSet, listClasses)   # 训练数据集

​        nb.map2vocab(dataSet[0])       # 随机选择一个测试句

​        print nb.predict(nb.testset)         # 输出分类结果

分类结果如下。

​        1
