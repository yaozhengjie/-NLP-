9.1 神经网络回顾

本节将从感知器网络开始介绍神经元的结构和最简单的神经网络：感知器网络，然后通过一个实例详细讲解梯度下降法实现神经网络的优化过程。

接下来，简要介绍BP神经网络的架构，以及反向传播的推导过程。为后面的深度学习中的RNN网络奠定基础。

### 9.1.1 神经网络框架

图9.1所示为一个最简单的感知器神经网络，也称为神经元。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784776359-acb2a150-1876-48b4-9822-aa0283dd6b3b.jpeg)

图9.1 神经元

后面所述的各种神经网络都是由这一最基本的结构构成的。神经网络中任意一个神经元都可以表示为上述的形式。一个神经元由如下两部分构成。

（1）带有权重的自由变量。该自由变量的取值来自前一层所有变量与权重的乘积，然后再求和，在数学上表示为如下形式。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784777040-a15e89ba-a78d-49e0-8fa8-a3bac911b5ab.jpeg)

其中，x为自由变量，xj为当前层，这里![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784777562-85e79d1e-aec1-4e96-b53a-4c9d3247fac2.jpeg)为前一层所有变量与权重的乘积，n为神经元个数。在实践中，神经网络的输入层由训练样本给定，隐含层和输出层的x取值由前一层计算得到。

（2）一个决策分类的激活函数。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784778494-369b6969-9e7f-48af-8f32-7a037cef24fe.jpeg)

图9.2 hardlim硬限幅函数

如图9.2所示，最早使用的激活函数是硬限幅（hardlim）函数。硬限幅的含义是，对于一个二元分类问题，可以设想成利用一个超平面划分两个高维凸集空间的情况：在超平面一侧的所有点集都被分类为“1”，另一侧则被分类为“0”。

在实践中，由于硬限幅函数在训练时难以确定决策边界，而且它是一个不连续的函数，无法对复杂网络数据模型求导，后期人们对该函数做了各种修正。

如图9.3所示，其中最成功、应用最广泛的一种激活函数是Logistic函数。该函数的优势是，首先它放大了决策边界，类别的决策过程使用一条平滑的“S”形曲线来代替间断的硬限幅函数。其次，Logistic曲线是连续的，在数学上保证了整个函数的连续性，便于训练阶段的求导。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784779160-55ef0a74-c1a2-422f-a554-5a158c729a3e.jpeg)

图9.3 Logistic函数

后面所讲的梯度下降法和BP神经网络都用这个函数作为激活函数。

神经网络发展至今，特别是在深度学习中，从Logistic激活函数衍生出多种激活函数，包括tanh、softmax等。

神经网络，也称为人工神经网络，是一个分层结构，在两个层次之间的神经元两两互联所构成的一个网络架构，如图9.4所示。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784779598-59f3b758-e896-4059-9968-7be816b62743.jpeg)

图9.4 神经网络示意图

神经网络是由多个神经元按层次互联构成的一种计算架构。网络的各层次的数学形式常表达为如下形式。

net = wTx+b

其中，w表示网络的节点权重向量；x为各层节点的数据（输入层为样本维度）; b为偏执向量，默认初始值一般为一个全1的列向量。

一般神经网络的层次分为如下三大类。

❑ 输入层：神经网络的第一层，网络的神经元的个数，称为输入层的维度。该维度来自训练样本的维度（列数），也就是特征数。权重在网络训练开始分配一个初始值，网络模型不同，对初始值的要求也不同。感知器网络一般分配的值为1, BP网络的分配值为一个0～1之间的随机数。

❑ 隐含层：可以是零层，也可以是一层或多层。不同网络模型的隐含层层数都不同，即便相同的网络也会有不同的隐含层设计。隐含层的神经元个数也没有一定之规。层数和神经元个数通常来自经验。

❑ 输出层：神经网络的最后一层，在NLP中用于输出训练后的预测标签。该预测结果与实际的分类标签比较得到类别的差值，该差值称为“损失（Loss）”，或称为“成本（Cost）”。计算损失的函数称为损失函数（Loss Function），网络的学习过程就是通过调整各层的权值使网络的整体损失最小的过程，换句话说，就是使损失函数最小化的过程。因此，在早期，神经网络借鉴了很多最优化问题的解决方法。

这样，整个神经网络构成了一个算法架构。该架构的执行分为如下两个阶段。

❑ 训练阶段。是指网络输入样本数据作为初始数据，通过激活函数与网络连接，迭代求得最小化的损失。这时最终学习出权重向量w，作为分类器的参数。数学上称这个过程为参数估计的过程。在NLP中如果用于序列标注，则可称为一个标注器。

❑ 分类阶段。拿这个学习出的分类器对实际的数据进行分类或标注，称为分类阶段。

### 9.1.2 梯度下降法推导

在作为神经网络的算法框架之前，梯度下降法一般作为最优化问题的一种求解方法，是求解无约束多元函数极值的最早的数值方法。下面讲解梯度下降法的基本原理。神经网络的基本公式如下。

net = wTx+b

把公式展开写成标量的形式，让b=w0，公式变形为如下。

net(x)+w0+w1x1+w2x2+…

其中，X=[1, x1, x2, …, xn-1]为行数为n的样本向量，W=[w0, w1, w2, …, wn-1]为列数为n的列向量。训练集的观测样本总数为m个，观测值分别为y1, y2, …, ym。

Logistic在纵坐标的取值范围是（0,1），横坐标的取值范围是从负无穷到正无穷，其函数表达式如下。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784780093-1e485b68-5c6a-44cd-805f-1509c4c2110b.jpeg)或![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784780579-a6e65005-f16d-4a48-aaf8-e04d11f90d03.jpeg)

为了方便，把训练集矩阵看作样本总体，对于总体的每个样本而言，它们彼此之间都是相互独立的，其输出标签为y={0, 1}，那么可以写为如下内容。

如果y=1，概率就是p, p=P(Y=1|x)，则令：

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784781155-d946f3c0-a9d4-4269-a226-a49a669ea474.jpeg)

如果y=0，概率就是1-p,1-p=P(Y=0|x)，那么：

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784781669-5a13f4b6-592a-4e99-9c5d-470e7cab098b.jpeg)

所以，事件的发生概率与不发生的概率之比为

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784782179-8931529f-52a8-49c0-a3e5-b5bc08c4f193.jpeg)

两边取对数：

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784782891-6e8a5c0b-fed1-4bf7-8dd1-4d646b770e38.jpeg)

这样可以得到一个观测值的概率为![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784783391-858cf7eb-db42-4f13-bc6b-fd512bcfbcb4.jpeg)。因为各个观测样本之间相互独立，所以它们的联合分布为各边缘分布的乘积，得到似然函数为

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784783806-8e108669-794d-4e2f-bb1d-cfc85823ebf2.jpeg)

我们的目标是求出使这一似然函数的值最大的参数估计，最大似然估计就是求出参数w0, w1, w2, …, wn-1，使得L(w)=ln(l)。上式变换为如下。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784784387-c6b67f47-64f0-47fc-a3c8-5d5ab1720113.jpeg)

继续对这n个wi分别求偏导，得到n个方程，关于wk的推导过程如下。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784784997-a5e32331-2545-43b4-8704-6173f9c0cc86.jpeg)

使用递推公式得到：

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784785458-13629281-bbfd-446e-a11d-175ed17116d7.jpeg)

其中，α就是下降的速度，一般是一个小的数值，可以从0.001开始尝试，其值越大表示下降越快、收敛越快。

迭代终止的条件如下。

||wi-wi-1||＜ε

### 9.1.3 梯度下降法的实现

本节给出梯度下降法的代码实现。使用的测试数据集可从http://www.threedweb.cn/thread-1603-1-1.html下载。

（1）辅助函数：位于common_libs中。

① 加载数据文件并转矩阵。

​        \# path: 数据文件路径

​        \# delimiter: 文件分隔符

​        def file2matrix(path, delimiter):

​            recordlist = []

​            fp = open(path, "rb")  # 读取文件内容

​            content = fp.read()

​            fp.close()

​            rowlist = content.splitlines()     # 按行转换为一维表

​        \# 逐行遍历，结果按分隔符分割为行向量

​            recordlist=[map(eval, row.split(delimiter)) for row in rowlist if row.strip()]

​            return mat(recordlist)    # 返回转换后的矩阵形式

② 绘制分类点。

​    \# 绘制分类点

​    def drawScatterbyLabel(plt, Input):

​        m, n=shape(Input)

​        target = Input[:, -1]

​        for i in xrange(m):

​            if target[i]==0:

​                  plt.scatter(Input[i,0], Input[i,1], c='blue', marker='o')

​            else:

​                  plt.scatter(Input[i,0], Input[i,1], c='red', marker='s')

③ 构建B+X矩阵，默认B为全1的列向量。

​    def buildMat(dataSet):

​        m, n=shape(dataSet)

​        dataMat = zeros((m, n))

​        dataMat[:,0] = 1

​        dataMat[:,1:] = dataSet[:, :-1]

​        return   dataMat

④ Logistic函数如下。

​    \# Logistic函数

​    def logistic(wTx):

​        return 1.0/(1.0+exp(-wTx))

（2）主程序。

​    \# -＊- coding: utf-8 -＊-

​    import os, sys

​    import numpy as np

​    from numpy import ＊

​    from common_libs import ＊

​    import matplotlib.pyplot as plt

​    reload(sys) # 设置 UTF-8输出环境

​    sys.setdefaultencoding('utf-8')

​    \# 1. 导入数据

​    Input = file2matrix("testSet.txt", "\t")

​    labels = Input[:, -1]    # 获取分类标签列表

​    [m, n] = shape(Input)

​    \# 2. 构建b+x 系数矩阵：b默认为1

​    dataMat = buildMat(Input)

​    \# print dataMat

​    \# 3. 定义步长和迭代次数

​    alpha = 0.001  # 步长

​    steps = 500   # 迭代次数

​    weights = ones((n,1))  # 初始化权重向量

​    errorlist = []

​    \# 4. 主程序

​    for k in xrange(steps):

​        net = dataMat＊mat(weights) # 待估计网络

​        output = logistic(net)  # logistic函数

​        loss = output-labels

​        error = 0.5＊sum(multiply(loss, loss)) # loss function

​        errorlist.append(error)

​        grad = dataMat.T＊loss  # 梯度

​        weights = weights - alpha＊grad #迭代

​    print weights     # 输出训练后的权重

​    \# 5. 绘制训练后超平面

​    drawScatterbyLabel(plt, Input) # 按分类绘制散点图

​    X = np.linspace(-5,5,100)

​    Y=-(double(weights[0])+X＊(double(weights[1])))/double(weights[2])

​    plt.plot(X, Y)

​    plt.show()

分类超平面如图9.5所示。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784785947-e5de6569-848a-4752-9dff-2dc26124078c.jpeg)

图9.5 分类超平面

输出的分类超平面向量如下。

​    [[ 4.17881308]  [ 0.50489874]  [ 0.61980264]]

### 9.1.4 BP神经网络介绍和推导

BP神经网络是由Geoffrey Hinton提出的一种神经网络算法，它由一个多层感知器结构及Back-Propagation训练算法构成，在20世纪80～90年代鼎盛一时。

BP算法的基本思想是，学习过程由信号的正向传播与误差的反向传播两个过程组成。正向传播时，输入样本从输入层传入，经各隐层逐层处理后，传向输出层。若输出层的实际输出与期望输出存在差异，则转入误差的反向传播阶段。误差反传是将输出误差以某种形式通过隐含层向输入层逐层反传，并将误差分摊给各层的所有单元，从而获得各层单元的误差信号，此误差信号即作为修正各单元权值的依据。这种信号正向传播与误差反向传播的各层权值调整过程，是周而复始地进行的。权值不断调整的过程，是由BP算法由数据流的前向计算（正向传播）和误差信号的反向传播两个过程共同构成的。BP神经网络如图9.6所示。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784786516-2a408dbe-378d-4039-8092-a756b332ee2e.jpeg)

图9.6 BP神经网络

1．正向传播

正向传播时，传播方向为输入层→隐含层→输出层，每层神经元的状态只影响下一层神经元。再通过误差函数计算实际输出与期望输出的误差，若在输出层得不到期望输出或与实际类别的差超出容限范围之外，则转向误差信号的反向传播流程。

1）正向传播的过程

假设BP神经网络的输入层有n个节点，隐含层有q个节点，输出层有m个节点，输入层与隐含层之间的权值为vih，隐含层与输出层之间的权值为who。隐含层和输出层的激活函数都为Logistic函数，则隐含层节点的输出如下。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784786960-fa4e92a1-5038-4b21-88ff-cd22796173cd.jpeg) （1）

输出层节点的输出为：

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784787811-ae48db57-b8f8-48fd-9bd8-36bd8aa759e0.jpeg) （2）

至此，BP神经网络就完成了n维空间向量对m维空间的近似映射。

2）定义误差函数

输入P个学习样本，用X=[x1, x2, …, xp]来表示。第p个样本输入到网络后得到输出ypj (j=1,2, …, m)。采用平方型误差函数，于是得到第p个样本的误差为![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784788396-14ca9127-1f09-4162-8e1e-d5ad657f80a6.jpeg)

上式中，tpj为预测输出。

对于P个样本，全局误差为![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784788657-35df9c89-ce58-4365-acf6-eb44bb1b0dcc.jpeg)（3）

2．反向传播

反向传播是将误差通过输出层→隐含层→输入层逐层反传，并通过激活函数的导函数将误差分摊给各层的所有单元，从而获得各层单元的修正信号，并以信号作为依据修正各单元权值。这种信号正向传播与误差反向传播的各层权值的调整过程，是周而复始地进行的，直至误差收敛在一个能够接受的范围之内。

1）激活函数的导函数

logistic'(net) = dlogit(net) = net(1-net)（4）

2）输出层权值的变化

采用累计误差BP算法调整wjk，使全局误差E变小，即

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784788987-eb233162-3751-4888-8e52-71cb5999293b.jpeg)

式中，α为学习率。

定义误差信号为![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784789497-8028ef12-e40d-4af7-92e9-1dab06deef70.jpeg)（5）

其中，第一项：![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784790020-970d5764-cd4c-49df-b6d7-d49ddabdb28c.jpeg)

第二项：![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784790547-24e22f3a-5d19-4704-b094-33523a8c4b1f.jpeg)是输出层传递函数的偏微分。

根据链定理，第一项与第二项相乘得到：![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784791025-535030c3-c243-4714-bc3f-79774f618e4a.jpeg)（6）

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784791259-059670f3-a966-41b9-a9d3-c15ccbca1c1e.jpeg)推导可得：![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784791703-f4fccd6e-fd18-484d-8c82-1b0b3128a8b0.jpeg)

于是，输出层各神经元的权值调整公式如下。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784792017-1a590850-d113-41d4-b368-42cf4eca4f7e.jpeg) （7）

3）隐含层权值的变化

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784792279-c3ce01bf-4ff0-4a70-a30b-88427b1a731e.jpeg)

定义误差信号为![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784792625-91b86e7d-be23-4238-b611-6ee893f9fca9.jpeg)（8）

其中，第一项：![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784793502-99528802-ea09-4bfe-9fd2-af01ab575c1c.jpeg)

依据链定理有：![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784793941-c244a0e3-e86b-4f2d-8578-a5260cc29bee.jpeg)

第二项：![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784794616-b78ffaaa-a0e8-4ddf-b4bf-ddb664e899fe.jpeg)是隐含层传递函数的偏微分。

第一项与第二项相乘得到：![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784795069-62c24fbb-58ae-4b00-b5d4-4f44e41f3761.jpeg)（9）

由链定理得：![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784795500-19ff5081-4280-4c3e-ad64-c1b17ca1638e.jpeg)

从而得到隐含层各神经元的权值调整公式为如下。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784796001-01dbc6e8-a762-49d9-aea4-1d3acba54c2b.jpeg) （10）

由于篇幅限制，这里就不给出BP网络的案例代码，对BP网络实现感兴趣的朋友可以参考笔者所著的另一本书——《机器学习算法原理与编程实践》中的实现。

3．应用不足

BP网络自诞生至今，大大小小的改进版本不计其数，一度是应用最广泛的神经网络算法。随着大量的应用也暴露出了如下问题。

首先，由于学习速率α是固定的，因此网络的收敛速度慢，需要较长的训练时间。对于一些复杂问题，BP算法需要的训练时间可能非常长，这主要是由于学习速率太小造成的，可采用变化的学习速率或自适应的学习速率加以改进。

其次，BP算法可以使权值收敛到某个值，但并不保证其为误差平面的全局最小值。这是因为采用梯度下降法可能产生一个局部最小值，网络容易陷入局部最优。

再次，网络隐含层的层数和单元数的选择尚无理论上的指导，一般是根据经验或者通过反复实验确定的。因此，网络往往存在很大的冗余性，在一定程度上也增加了网络学习的负担。

最后，网络的学习和记忆具有不稳定性。也就是说，如果增加了学习样本，训练好的网络就需要从头开始训练，对于以前的权值和阈值是没有记忆的，但是可以将预测、分类或聚类做得比较好的权值保存。
