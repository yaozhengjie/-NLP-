1.6 整合句法解析模块

句法分析是根据给定的语法体系自动推导出句子的语法结构，分析句子所包含的语法单元和这些语法单元之间的关系，将句子转化为一棵结构化的语法树。句法分析是所有自然语言处理的核心模块（一般拼音文字没有分词的问题）。其研究历史比较悠久，应用也很广泛。

目前句法分析有两种不同的理论：一种是短语结构语法；另一种是依存语法。关于两种理论更详细的内容在后面章节会详细分析，这里先不做讨论。句法分析的开源系统也很多，但迄今为止，这些解析技术都还不够理想，仍旧很难找到高精度处理中文的句法解析系统。

其中，比较突出的是Ltp 3.3中文句法分析系统，使用依存句法理论。在SANCL 2012互联网数据依存句法分析评测中获得第二名；在CoNLL 2009句法和语义依存分析评测中，中文依存句法分析获得第三名。

除此之外，最著名的句法解析器是Stanford句法解析器。截至2015年，Stanford的句法树包含了如下三大主要解析器。

❑ PCFG概率解析器。是一个高度优化的词汇化PCFG依存解析器。该解析器使用A*算法，是一个随机上下文无关文法解析器。除英语之外，该解析器还包含一个中文版本，使用宾州中文树库训练。解析器的输出格式包含依存关系输出和短语结构树输出。

❑ Shift-Reduce解析器。为了提高PCFG概率解析器的性能，Stanford提供了一个基于移进—归约算法（Shift-Reduce）的高性能解析器。其性能远高于任何PCFG解析器，而且精度上比其他任何版本（包括RNN）的解析器都更准确。

❑ 神经网络依存解析器。神经网络依存解析器是深度学习算法在句法解析中的一个重要应用。它通过中心词和修饰词之间的依存关系来构建出句子的句法树。有关此方面的研究是目前NLP的研究重点。

本节延续1.4节的系统，使用Ltp 3.3和Stanford Parser来进行中文的文本解析。因为Stanford的句法解析器比较多，这里仅实现比较有代表性的PCFG解析器。

### 1.6.1 Ltp 3.3句法依存树

继续使用Ltp 3.3中文句法解析模块。图1.5列出的语言包列表中，句法解析模块的文件名为parser.model。

为了简化，下面使用已经分好词的单句进行演示。

代码如下。

​        \# -＊- coding: utf-8 -＊-

​        import sys

​        import os

​        import nltk

​        from nltk.tree import Tree # 导入nltk tree结构

​        from nltk.grammar import DependencyGrammar #导入依存句法包

​        from nltk.parse import ＊

​        from pyltp import ＊ # 导入ltp应用包

​        import re

​        reload(sys)

​        sys.setdefaultencoding('utf-8')  # 设置 UTF-8输出环境

​        words = "罗马尼亚 的 首都 是 布加勒斯特 。".split(" ")  #例句

​        postagger = Postagger() # 首先对句子进行词性标注

​        postagger.load("X:\\ltp3.3\\pos.model")

​        postags = postagger.postag(words)

​        parser = Parser() # 将词性标注和分词结果都加入分析器中进行句法解析

​        parser.load("X:\\ltp3.3\\parser.model")

​        arcs = parser.parse(words, postags)

​        arclen = len(arcs)

​        conll = ""

​        for i in xrange(arclen): # 构建Conll标准的数据结构

​            if arcs[i].head ==0:

​                arcs[i].relation = "ROOT"

​            conll += "\t"+words[i]+"("+postags[i]+")"+"\t"+postags[i]+"\t"+str(arcs[i].

​    head)+"\t"+arcs[i].relation+"\n"

​        print conll

​        conlltree = DependencyGraph(conll) # 转换为依存句法图

​        tree = conlltree.tree() # 构建树结构

​        tree.draw()   # 显示输出的树

输出结果如下。

​            罗马尼亚(ns)   ns  3    ATT

​            的(u)    u    1    RAD

​            首都(n)   n    4    SBV

​            是(v)    v    0    ROOT

​            布加勒斯特(ns) ns  4    VOB

​            。(wp)   wp  4    WP

输出图如图1.16所示。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784428288-2cf2ed07-b39c-42b0-b6db-eaeae68ce164.jpeg)

图1.16 Ltp 3.3句法依存解析树

如图1.16所示，结合两个输出结果，将单句“罗马尼亚 的 首都 是 布加勒斯特”的解析结果给出如下结论：句法树是一棵依存关系树，根节点为其谓语动词“是”，主语是“首都”, “罗马尼亚”是修饰“首都”的定语，句子的宾语是“布加勒斯特”。

### 1.6.2 Stanford Parser类

如果仅使用斯坦福的中文句法解析模块（也称为Parser），可从http://nlp.stanford.edu/software/lex-parser.shtml下载。与命名实体识别相同，Parser模块的程序和中文模型库也分开存放，但都在一处下载。应用程序和模型都可从图1.17所示的截图处下载。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784429942-580c6370-e66e-46cd-99d6-7ba8137f8587.jpeg)

图1.17 截图

这里仍旧使用stanford-corenlp中的模型和程序。在图1.14目录列表中对应不同的句法解析器有不同的模型目录，分别为：lexparser、parser和srparser。其中，PCFG概率解析器模型为lexparser; Shift-Reduce解析器模型为srparser；神经网络依存解析器模型为parser。以lexparser为例，图1.18所示的5个模型分别用于lexparser解析器的调用，最常用的库是chinesePCFG.ser.gz文件。该文件支持高度精确的词汇解析器。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784432481-d7994345-aeab-4283-8798-1770b3812667.jpeg)

图1.18 Stanford句法依存模型文件

最后，仿照前两节，编写一个Python的StanfordParser接口类如下。

​        class StanfordParser(StanfordCoreNLP):

​            def __init__(self, modelpath, jarpath, opttype):

​                StanfordCoreNLP.__init__(self, jarpath)

​                self.modelpath = modelpath # 模型文件路径

​                self.classfier = "edu.stanford.nlp.parser.lexparser.LexicalizedParser"

​                self.opttype = opttype

​                self.__buildcmd()

​            \# 构建命令行

​            def __buildcmd(self):

​                self.cmdline = 'java -mx500m -cp "'+self.jarpath+'" '+self.classfier+'

​    -outputFormat "'+self.opttype+'" '+self.modelpath+' '

​            \#解析句子

​            def parse(self, sent):

​                self.savefile(self.tempsrcpath, sent)

​                tagtxt = os.popen(self.cmdline+self.tempsrcpath, "r").read()# 输出到变量中

​                self.delfile(self.tempsrcpath)

​                return   tagtxt

​            \# 输出到文件

​            def tagfile(self, sent, outpath):

​                self.savefile(self.tempsrcpath, sent)

​                os.system(self.cmdline+self.tempsrcpath+' > '+outpath )

​                self.delfile(self.tempsrcpath)

### 1.6.3 Stanford短语结构树

以短语结构的方式输出，内容如下。

​        \# -＊- coding: utf-8 -＊-

​        import sys

​        import os

​        from nltk.tree import Tree  # 导入nltk库

​        from stanford import ＊

​        \# 设置 UTF-8输出环境

​        reload(sys)

​        sys.setdefaultencoding('utf-8')

​        \# 配置环境变量

​        os.environ['JAVA_HOME'] = 'D:\\Java7\\jdk1.8.0_65\\bin\\java.exe'

​        \# 安装库

​        root = "E:/nltk_data/stanford-corenlp/"

​        modelpath= root+'models/lexparser/chinesePCFG.ser.gz'

​        opttype = 'penn'  # 宾州树库格式

​        parser = StanfordParser(modelpath, root, opttype)

​        result = parser.parse("罗马尼亚 的 首都 是 布加勒斯特 。")

​        print  result

​        tree = Tree.fromstring(result)

​        tree.draw()

输出结果如下。

​        (ROOT

​          (IP

​            (NP

​              (DNP

​              (NP (NR 罗马尼亚))

​              (DEG 的))

​            (NP (NN 首都)))

​          (VP (VC 是)

​            (NP (NR 布加勒斯特)))

​        (PU 。)))

输出图1.19。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784433484-88e986ca-2f45-42f9-a667-e951514358a5.jpeg)

图1.19 Stanford句法结构树

如图1.19所示，短语结构树的叶子节点是每个词的词形，词形上一级节点是词性，再上一级节点是由多个词构成的短语组块，“NP”这里表示名词性短语，“DNP”是由NP+“的”构成的短语结构，“VP”是动词短语，“IP”是简单句。

### 1.6.4 Stanford依存句法树

以依存句法的方式输出，内容如下。

​        \# -＊- coding: utf-8 -＊-

​        import sys

​        import os

​        from nltk.tree import Tree

​        from stanford import ＊

​        \# 设置 UTF-8输出环境

​        reload(sys)

​        sys.setdefaultencoding('utf-8')

​        root = "E:/nltk_data/stanford-corenlp/"

​        modelpath= root+'models/lexparser/chinesePCFG.ser.gz'

​        opttype = 'typedDependencies' # "penn, typedDependencies"

​        parser=StanfordParser(modelpath, root, opttype)

​        result = parser.parse("罗马尼亚 的 首都 是 布加勒斯特 。")

​        print result

输出结果如下。

​        assmod(首都-3, 罗马尼亚-1)

​        case(罗马尼亚-1, 的-2)

​        nsubj(布加勒斯特-5, 首都-3)

​        cop(布加勒斯特-5, 是-4)

​        root(ROOT-0, 布加勒斯特-5)

Stanford依存树库的标注系统与Ltp的标注系统不同，具有更为复杂的依存结构。“root”是根节点，一般对应一个谓语（VP+NP），这里对应着的“是”，它是一个“cop”——系表助动词结构。因此，用它后面的NP结构作为根节点，剩下的部分就很容易了。“nsubj”表示名词性主语，在这个主语的内部，“首都”被“罗马尼亚”修饰，“assmod”表示关联修饰，“的”依存于“罗马尼亚”, “case”表示句法依赖。

这个结果与Ltp的分析结果略有不同，这是由标注规范的不一致导致的。有关标注规范的更多细节，第6章将给出具体说明。
