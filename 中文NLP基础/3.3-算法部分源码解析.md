3.3 算法部分源码解析

本节介绍了HanLP的源文件结构及部分核心源代码解析。由于整个项目包含中文分词、未登录词识别、词性标注三大部分，代码量较大，细节也很多，因此，下文将把重点放在中文分词和未登录词识别两部分的源码解析内容。

### 3.3.1 系统配置

为了尽快使读者上手，我们简化一些构建项目的方法。有关这些资料，读者阅读HanLP的帮助文档即可。下面介绍系统配置。首先要修改系统的配置文件：hanlp.properties。然后将该文件复制到src目录下，并在执行之前根据自己的环境对该文件进行配置。该文件默认使用ANSI编码写成，如果运行在Linux上，建议转换为UTF-8编码。文件内容如下。

​        \#本配置文件中的路径的根目录，根目录+其他路径=绝对路径

​        \#Windows用户请注意，路径分隔符统一使用/

​        root=D:/JavaProjects/HanLP/

​        \#核心词典路径

​        CoreDictionaryPath=data/dictionary/CoreNatureDictionary.txt

​        \#二元语法词典路径

​        BiGramDictionaryPath=data/dictionary/CoreNatureDictionary.ngram.txt

​        \#停用词词典路径

​        CoreStopWordDictionaryPath=data/dictionary/stopwords.txt

​        \#同义词词典路径

​        CoreSynonymDictionaryDictionaryPath=data/dictionary/synonym/CoreSynonym.txt

​        \#人名词典路径

​        PersonDictionaryPath=data/dictionary/person/nr.txt

​        \#人名词典转移矩阵路径

​        PersonDictionaryTrPath=data/dictionary/person/nr.tr.txt

​        \#繁简词典路径

​        TraditionalChineseDictionaryPath=data/dictionary/tc/TraditionalChinese.txt

​        \#自定义词典路径，用；隔开多个自定义词典，空格开头表示在同一个目录，使用“文件名 词性”形式则表

​    示这个词典的词性默认是该词性。优先级递减。

​        \#另外data/dictionary/custom/CustomDictionary.txt是一个高质量的词库，请不要删除

​        CustomDictionaryPath=data/dictionary/custom/CustomDictionary.txt; 现代汉语补充词

​    库．txt; 全国地名大全．txt ns; 人名词典．txt; 机构名词典．txt; 上海地名．txt ns; data/

​    dictionary/person/nrf.txt nrf

​        \#CRF分词模型路径

​        CRFSegmentModelPath=data/model/segment/CRFSegmentModel.txt

​        \#HMM分词模型

​        HMMSegmentModelPath=data/model/segment/HMMSegmentModel.bin

​        \#分词结果是否展示词性

​        ShowTermNature=true

如果系统运行仅需要修改一项，则将“root=”后面的内容修改为项目根目录。例如，改为本书的项目根目录为“root=/home/your_username/workspace /Hanlp-1.28/”。

### 3.3.2 Main方法与例句

HanLP源码并未提供测试的入口方法，但该框架提供的文档比较全，读者可从http://hanlp.linrunsoft.com/doc.html页面查看。为此我们新建了“com.test.Hanlp; ”包，并创建一个测试类：ICTCLASSeg。暂且以此为例进行简单的测试。主方法的源代码如下。

​        package com.test.Hanlp;

​        import com.hankcs.hanlp.seg.Segment;

​        import com.hankcs.hanlp.seg.NShort.NShortSegment;

​        import com.hankcs.hanlp.seg.Viterbi.ViterbiSegment;

​        public class ICTCLASSeg {

​            public static void main(String[] args) {

​                // 实例化NShort分词器，并启用地名、组织名词识别模块

​                Segment  nShortSegment  =  new  NShortSegment().enableCustomDictionary

​    (false).enablePlaceRecognize(true).enableOrganizationRecognize(true);

​                // 输入例句

​        String[] testCase = new String[]{

​                                  "张强在铁岭对王小红说的确实在理。",

​                };

​                // 输出最短路径分词结果

​                System.out.println("N-最短分词：" + nShortSegment.seg(testCase[0]) );

​            }

​        }

参考教程的NShortSegment代码，简单实现了一个入口方法，可以看到执行结果如下。

​        N-最短分词：[乔布斯/nrf, 说/v, iphone/nx, 是/vshi, 最好/d, 用/p, 的/ude1, 手机/n, 。/w]

这里enableCustomDictionary()、enablePlaceRecognize()、enableOrganizationRecognize()为命名实体识别的启用或关闭设置。这部分的详细代码在NShortSegment类中，比较简单，在此不做讲解。

### 3.3.3 句子切分

句子切分是中文分词的预处理阶段，这个节点的主要功能是对输入字符串按照分隔符（全角分隔符、半角分隔符）来分隔句子。这个阶段使用的类是com.hankcs.hanlp.seg. Segment，功能函数为seg()函数。

这些分隔符位于SentenceUtility类中，如下为参考源代码。

​        public static List<String> toSentenceList(char[] chars)  {

​              StringBuilder sb = new StringBuilder();

​              List<String> sentences = new LinkedList<String>();

​              for (int i = 0; i < chars.length; ++i)  {

​                  if (sb.length() == 0 && (Character.isWhitespace(chars[i]) || chars[i]

​    == ' '))  {

​                      continue;

​                  }

​                  sb.append(chars[i]);

​                  switch (chars[i])  {

​                      case '.':

​                          if (i < chars.length - 1 && chars[i + 1] > 128)  {

​                            insertIntoList(sb, sentences);

​                            sb = new StringBuilder();

​                          }

​                          break;

​                      case '…':  {

​                          if (i < chars.length - 1 && chars[i + 1] == '…')   {

​                            sb.append('…');

​                            ++i;

​                            insertIntoList(sb, sentences);

​                            sb = new StringBuilder();

​                          }

​                      }break;

​                      case ' ':

​                      case '  ':

​                      case '? :

​                      case '。':

​                      case ', ':

​                      case ', ':

​                          insertIntoList(sb, sentences);

​                          sb = new StringBuilder();

​                          break;

​                      case '; ':

​                      case '; ':

​                          insertIntoList(sb, sentences);

​                          sb = new StringBuilder();

​                          break;

​                      case '! ':

​                      case '! ':

​                          insertIntoList(sb, sentences);

​                          sb = new StringBuilder();

​                          break;

​                      case '? ':

​                      case '? ':

​                          insertIntoList(sb, sentences);

​                          sb = new StringBuilder();

​                          break;

​                      case '\n':

​                      case '\r':

​                          insertIntoList(sb, sentences);

​                          sb = new StringBuilder();

​                          break;

​                    }

​                }

​                if (sb.length() > 0)  {

​                    insertIntoList(sb, sentences);

​                }

​                return sentences;

​            }

这里所谓的句子，就是被一些标句符号分隔的字符串，这些标句符号包括：'…'、'。'、', '、', '、'; '、'; '、'! '、'! '、'? '、'? '、'\n'、'\r’等（上述标点符号通过顿号来分隔）。句子切分的执行函数为toSentenceList，该函数的逻辑很简单，就是将输入的一个字符串（可以理解为一个大的字符串）按照上述标句符号切分为字符串数组（这里是一个链表结构）。

如果输入的文本比较大，Seg函数还支持多线程的调用来处理大型文本，源代码如下。

​            public List<Term> seg(String text) {

​        char[] charArray = text.toCharArray();  //原子切分

​                if (HanLP.Config.Normalization) {

​                  CharTable.normalization(charArray);

​                }

​                if (config.threadNumber > 1 && charArray.length > 10000) { // 小文本多线程

​    没意义，反而变慢了

​                  List<String> sentenceList = SentencesUtil.toSentenceList(charArray);

​    //将篇章切分为句子链表

​                  String[] sentenceArray = new String[sentenceList.size()];

​                  sentenceList.toArray(sentenceArray);

​                  List<Term>[] termListArray = new List[sentenceArray.length];

​                  final int per = sentenceArray.length / config.threadNumber;

​                  WorkThread[] threadArray = new WorkThread[config.threadNumber];  //创

​    建工作线程组

​                  for (int i = 0; i < config.threadNumber - 1; ++i) {

​                      int from = i ＊ per;

​                      threadArray[i] = new WorkThread(sentenceArray, termListArray, from,

​    from + per); //创建线程

​                      threadArray[i].start(); //启动线程->run方法

​                  }

​                  threadArray[config.threadNumber - 1] = new WorkThread(sentenceArray,

​    termListArray, (config.threadNumber - 1) ＊ per, sentenceArray.length);

​                  threadArray[config.threadNumber - 1].start(); //启动线程->run方法

​                  try {

​                      for (WorkThread thread : threadArray) {

​                        thread.join(); //合并执行完的线程

​                      }

​                  } catch (InterruptedException e) {

​                      logger.severe("线程同步异常：" + TextUtility.exceptionToString(e));

​                      return Collections.emptyList();

​                  }

​                  List<Term> termList = new LinkedList<Term>();

​                  if (config.offset || config.indexMode) {

​                      int sentenceOffset = 0;  // 由于分割了句子，所以需要重新校正offset

​                      for (int i = 0; i < sentenceArray.length; ++i) {

​                        for (Term term : termListArray[i]) {

​                            term.offset += sentenceOffset;

​                            termList.add(term);

​                        }

​                        sentenceOffset += sentenceArray[i].length();

​                      }

​                  } else {

​                      for (List<Term> list : termListArray) {

​                        termList.addAll(list);

​                      }

​                  }

​                  return termList;

​              }

如下函数（run）就是start调用的多线程分词方法。

​              @Override

​              public void run()  {

​                  for (int i = from; i < to; ++i) {

​                    termListArray[i] = segSentence(sentenceArray[i].toCharArray());

​                  }

​              }

​          }

分词后的结果都被保存在termListArray中。

### 3.3.4 分词流程

对输入的文本预处理完成（句子切分阶段结束）之后，即进入中文分词流程。NShortSegment类完成了NShort中文分词算法的全过程。这个类继承自WordBasedGenerativeModelSegment类，而WordBasedGenerativeModelSegment类又继承自Segment类。

HanLP不仅提供了NShort分词算法，还提供了基于HMM、CRF、Dijkstra等的多种切分和标注方法。所有这些分词方法都继承自Segment类，而NShort和HMM在概率图模型中均属于产生式模型。因此，这两个类继承自WordBasedGenerativeModelSegment类。通过如此复杂的继承关系，系统将若干个分词算法形成一个整体。

在CResult::Processing函数中，这里主要分析中文分词的完整流程。其流程如图3.3所示。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784549743-ebc3a6ab-0b34-43a0-b4df-de9826c934df.jpeg)

图3.3 中文分词的流程

如图3.3所示，分词的过程包含4个大的步骤，分别为：一元词网、二元词图、输出最短路径、应用后处理规则。

（1）一元词网。根据输入的字符串，查询核心词典，将原子分词的结果与词典中的词进行最大匹配。匹配的结果包括词字符串、词性、词频等信息形成一个二维数组，并将此数组存入WordNet二维数组中。

（2）二元词图。用一元分词的结果（二维数组）查询二元词典，与二元词典进行最大匹配。匹配的结果为一个二维数组，并将此结果存入新的Graph中，形成一个词图。

（3）输出最短路径。将查出的每个结果按平滑算法计算一元分词和二元分词的词频数得到词网中每个节点的权值（概率的倒数），应用NShort算法累加词网中每个节点构成的所有路径，权值最小（概率最大）的那条路径对应的词网节点就是初分的结果。

（4）应用后处理规则。对粗分结果应用规则，识别特殊组合的名词性结构。

NShortSegment类的segSentence()是控制分词流程的主函数。如下是经过注释的粗分词阶段的函数代码片段。

​            @Override

​            public List<Term> segSentence(char[] sentence)

​            {

​                WordNet wordNetOptimum = new WordNet(sentence); //最优词网

​                WordNet wordNetAll = new WordNet(sentence); //一元词网

​                // 粗分主函数

​                List<List<Vertex>> coarseResult = BiSegment(sentence, 2, wordNetOptimum,

​    wordNetAll);

​                boolean NERexists = false;

​                for (List<Vertex> vertexList : coarseResult)

​                {

​                  if (HanLP.Config.DEBUG)

​                  {

​                      System.out.println("粗分结果" + convert(vertexList, false));

​                    }

​        ……

### 3.3.5 一元词网

构建一元词网的过程是由WordBasedGenerativeModelSegment类的GenerateWordNet方法完成的。部分源代码如下。

​            /＊＊

​             ＊ 二元语言模型分词

​             ＊ @param sSentence  待分词的字符数组

​             ＊ @param nKind    返回的结果数

​             ＊ @param wordNetOptimum   最优词网

​             ＊ @param wordNetAll   全部词网

​             ＊ @return  一系列粗分结果

​             ＊/

​            public List<List<Vertex>> BiSegment(char[] sSentence, int nKind, WordNet

​    wordNetOptimum, WordNet wordNetAll)

​            {

​                List<List<Vertex>> coarseResult = new LinkedList<List<Vertex>>(); //粗分结

​    果链表

​                  ////////////////生成词网////////////////////

​                GenerateWordNet(wordNetAll);

​                if (HanLP.Config.DEBUG)

​                      System.out.printf("打印词网：\n" + wordNetAll);

​    …

构建一元词网的流程分为如下三个部分。

1．最大匹配查询词典

最大匹配是查找词典的一种方法。我们希望从词典中查找某个字符串成词的情况，如字符串“中华人民共和国”，从“中”开始查询词典，找到两个词“中”、“中华”；然后从“华”开始再查询词典，找到一个词“华人”；依次向下直至找到所有的词为止。HanLP完成最大匹配的过程比较有特色。GenerateWordNet中对该函数的调用代码如下。

​        ……

​        // 核心词典查询——一种最大匹配的特色实现

​              DoubleArrayTrie<CoreDictionary.Attribute>.Searcher searcher = CoreDictionary.

​    trie.getSearcher(charArray, 0); //构建搜索器

​              while (searcher.next())//不断向下一步搜索

​              {     //在词网中加入搜索到的节点

​                    // Vertex：词内容，value：词性、词频对， index：该词在一元词典中的序号

​                  wordNetStorage.add(searcher.begin + 1, new Vertex(new String(charArray,

​    searcher.begin, searcher.length), searcher.value, searcher.index));

​              }

​        ……

词典的数据结构是一个双数组Tri树。因此，首先构造一个Tri树的查询器，从起始位置依次向后遍历整个字符数组，返回了词的字符串内容、词性、词频、词典中的位置等信息。有时候可以查询到一个词汇有多个词性，每个词性都有不同的词频。这样，查询结果需要记录每个词性和词频信息，并汇总所有的词频计算出总的词频数。

最后，使用这些信息构造一个图的顶点Vertex，将其保存在WordNet一元词网中。如下为节点主构造方法的代码，其他构造都是它的一个变种。

​            public Vertex(String word, String realWord, CoreDictionary.Attribute attribute,

​    int wordID)

​            {

​                if (attribute == null) attribute = new CoreDictionary.Attribute(Nature.n,

​    1);   // 安全起见

​                this.wordID = wordID;

​                this.attribute = attribute;

​                  //将原词变为等效词串

​                if (word == null) word = compileRealWord(realWord, attribute);

​                assert realWord.length() > 0 : "构造空白节点会导致死循环！";

​                this.word = word;

​                this.realWord = realWord;

​            }

这里有一个重要的函数compileRealWord。该函数生成了realWord。生成的类型与词性对照如表3.6所示。

表3.6 生成的类型与词性对照

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784551048-e3ee9dec-0eda-427c-a77d-589b476aa0d3.jpeg)

2．原子切分和确定类型

一元词网初步构建后，GenerateWordNet剩下的过程是对一元词网中的每个词进行原子切分，并确定其类型。

​        ……

​        // 原子分词，保证图连通

​              LinkedList<Vertex>[] vertexes = wordNetStorage.getVertexes();

​              for (int i = 1; i < vertexes.length; )

​              {

​                  if (vertexes[i].isEmpty())

​                      {

​                      int j = i + 1;

​                      for (; j < vertexes.length - 1; ++j)

​                            {

​                          if (! vertexes[j].isEmpty()) break;

​                            }

​                      //快速原子切分，charArray：字符数组， i - 1：起始下标， j - 1：终止下标

​                      wordNetStorage.add(i, quickAtomSegment(charArray, i - 1, j - 1));

​                      i = j;

​                  }

​                  else i += vertexes[i].getLast().realWord.length();

​              }

​        }

生成原子词的方法为quickAtomSegment，函数调用了CharType类，为每个从词典中查询出的词给出原子词的类型。

系统使用UTF-8编码方式。因为UTF-8下的汉字编码和全角符号编码没有规律性，不能用一个简单的程序实现，所以所有汉字、全角、半角符号类型的CharType都被保存在一个表中，文件路径为“data/dictionary/other/CharType.dat.yes”。在CharType类初始化时该文件加载到内存。解析该文件的静态代码段如下。

​        static

​            {

​              type = new byte[65536];

​              logger.info("字符类型对应表开始加载 " + HanLP.Config.CharTypePath);

​              long start = System.currentTimeMillis();

​              ByteArray byteArray = ByteArray.createByteArray(HanLP.Config.CharTypePath);

​              if (byteArray == null) {

​                  System.err.println("字符类型对应表加载失败：" + HanLP.Config.CharTypePath);

​                  System.exit(-1);

​              } else{

​                  while (byteArray.hasMore()) {

​                      int b = byteArray.nextChar();

​                      int e = byteArray.nextChar();

​                      byte t = byteArray.nextByte();

​                      for (int i = b; i <= e; ++i) {

​                          type[i] = t;

​                      }

​                  }

​                  logger.info("字符类型对应表加载成功，耗时" + (System.currentTimeMillis() -

​    start) + " ms");

​                }

​            }

quickAtomSegment函数在查询每个字符的CharType时，都要调用内存中的这个表，生成每个字符的原子词类型。一些非汉字的原子词类型需要在quickAtomSegment进行合并处理，处理的结果类型定义在CharType类中。综合所有非汉字的CharType类型将其总结为表3.7。

表3.7 CharType类型标签

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784552412-16ed81e0-1fed-431a-9d33-56f4649a3a44.jpeg)

生成查询出每个顶点的CharType编码之后，对其中汉字类型不做任何改变，但合并浮点类型的数字（“3, .,1,4”合并为3.14）；合并ASCII的字符为一个原子词（将“i, p, h, o, n, e”合并为一个词iphone）；合并数字为一个原子类型（将“1,0,0”合并为100）。经过这些修改之后，原来的句子发生了一些变化，这个过程称为原子切分。本节最后将显示原子切分的结果。

最后，将生成的原子词序列放入一元词网，生成新的节点，代码如下。

​            /＊＊

​             ＊ 添加顶点，由原子分词顶点添加

​             ＊/

​            public void add(int line, List<AtomNode> atomSegment)

​            {

​                int offset = 0;

​                for (AtomNode atomNode : atomSegment)//Init the cost array

​                {

​                  String sWord = atomNode.sWord; //init the word

​                  Nature nature = Nature.n;

​                  switch (atomNode.nPOS)

​                  {

​                      case Predefine.CT_CHINESE:

​                        break;

​                      case Predefine.CT_INDEX:

​                      case Predefine.CT_NUM:

​                        nature = Nature.m;

​                        sWord = "未##数";

​                        break;

​                      case Predefine.CT_DELIMITER:

​                        nature = Nature.w;

​                        break;

​                      case Predefine.CT_LETTER:

​                        nature = Nature.nx;

​                        sWord = "未##串";

​                        break;

​                      case Predefine.CT_SINGLE://12021-2129-3121

​                        nature = Nature.nx;

​                        sWord = "未##串";

​                        break;

​                      default:

​                        break;

​                  }

​                  add(line + offset, new Vertex(sWord, atomNode.sWord, new CoreDictionary.

​    Attribute(nature, 1)));

​                  offset += atomNode.sWord.length();

​              }

​            }

该函数处理各种CharType类型的原子词，汉字不变，其他类型都给出了新的词性及CharType类型的字符串标识。

3．构建一元词网

对句子完成了原子切分和类型合并，下一步需要利用这个切分结果构建出一元词网。为了便于读者的深刻理解，我们回到NShortSegment类的BiSegment主函数的代码片段，查看如下执行的结果。

​        //一元分词

​            /＊＊

​            ＊ 二元语言模型分词

​            ＊ @param sSentence 待分词的字符数组

​            ＊ @param nKind    需要几个结果

​            ＊ @param wordNetOptimum  最优词网

​            ＊ @param wordNetAll  全部词网

​            ＊ @return 一系列粗分结果

​            ＊/

​           public List<List<Vertex>> BiSegment(char[] sSentence, int nKind, WordNet

​    wordNetOptimum, WordNet wordNetAll)

​           {

​              List<List<Vertex>> coarseResult = new LinkedList<List<Vertex>>(); //粗分节点

​                  ////////////////生成词网////////////////////

​              GenerateWordNet(wordNetAll);

​              if (HanLP.Config.DEBUG)

​                      System.out.printf("打印词网：\n" + wordNetAll);

​    …

为了更清晰地观察，这里修改了WordNet.toString()方法，以便给出这个阶段的输出结果的更多细节。

​            @Override

​            public String toString()

​            {

​                StringBuilder sb = new StringBuilder();

​                int line = 0;

​                for (List<Vertex> vertexList : vertexes)

​                {

​                  sb.append(String.valueOf(line++) + ':' );

​                  //sb.append(String.valueOf(line++)  +  ':'  +  vertexList.toString()).

​    append("\n");

​                  for (Vertex vertex:vertexList){

​                      sb.append("["+vertex.getRealWord()+"\t"+vertex.word+"]");

​                  }

​                  sb.append('\n' );

​                }

​                return sb.toString();

​            }

例句1测试代码输出结果如下。

​        打印词网：

​        0:[ 始##始]

​        1:[石    石]

​        2:[国    国]

​        3:[祥    祥]

​        4:[会    会][会见  会见]

​        5:[见    见]

​        6:[乔    乔][乔布  未##人][乔布斯 未##人]

​        7:[布    布][布斯  未##人]

​        8:[斯    斯]

​        9:[说    说]

​        10:[iphone   未##串]

​        11:

​        12:

​        13:

​        14:

​        15:

​        16:[是   是]

​        17:[最   最][最好  最好]

​        18:[好   好]

​        19:[用   用]

​        20:[的   的]

​        21:[手   手][手机  手机]

​        22:[机   机]

​        23:[。   。]

​        24:[     末##末]

以行“6:[乔 乔][乔布 未##人][乔布斯 未##人]”为例。输出表的第一列为序号（6:），即输入的句子字符串转换为字符数组的索引位置。第二列为一个或多个“[]”，如[乔 乔]、[乔布 未##人]、[乔布斯 未##人]。它们是查询词典后的结果，存储为一元词网的子链节点。“[]”内的文字被空格分隔为两部分：一部分为节点的realword属性；另一部分为word属性。word的属性来源于CharType表的查询结果（具体见上文），可能是汉字本身，也可能是某个CharType类型的字符串标识形式。除此之外，顶点还保存了查询出的词汇的词频和词性信息。

第一个顶点和最后一个顶点为标识句首和句尾的顶点，word属性为“始##始”、“末##末”，没有realword属性，因为realword属性值来源于切分后的原子词。

表中11～15行为空行，因为这里的字符都被合并到第10行，这是由原子切分所产生的结果。

### 3.3.6 二元词图

通过一元词网得到一个二维数组（严格意义上是一张广义表）。该数组包含所有的原子词，以及相应的词频和词性信息。二元分词是将这些原子词和二元词典词按顺序排列组合，产生词与词相关联的词图，并计算这个关联词图中每个二元关联词的词频概率，通过这个过程消除分词中的歧义问题，并为后面的NShort最短路径计算做准备。

​                  ///////////////生成词图////////////////////

​                Graph graph = GenerateBiGraph(wordNetAll);

​                if (HanLP.Config.DEBUG)

​                {

​                   System.out.printf("打印词图：%s\n", graph.printByTo());

​                }

如果把一元的分词结果当作一个节点，那么二元关联词即可看作节点之间的一条连线，也就是边，而含有节点和边的数据结构即可被看作一张图。下面来详细了解如何生成一张二元词图的过程。

词图的数据结构包含两个类，一个是Graph类，代码如下。

​        public class Graph

​        {

​            public Vertex[] vertexes;  // 顶点

​            public List<EdgeFrom>[] edgesTo;  // 边，到达下标i

​        ……

仅包含顶点列表和顶点的到达边列表。

一个是Edge类及其子类EdgeFrom类。Edge类的代码如下。

​        /＊＊

​         ＊ 基础边，不允许构造

​        ＊/

​        public class Edge

​        {

​            public double weight;  // 花费

​        String name;  // 节点名字，调试用

​        ……

EdgeFrom类的代码如下。

​        /＊＊

​         ＊ 记录了起点的边

​        ＊/

​        public class EdgeFrom extends Edge

​        {

​        public int from; // 起点的索引

​        ……

这是标准的图类型的数据结构，并不复杂。

二元分词的算法是通过toGraph这个函数完成的。其源代码如下。

​            /＊＊

​             ＊ 词网转词图

​             ＊ @return 词图

​             ＊/

​            public Graph toGraph()

​            {

​                Graph graph = new Graph(getVertexesLineFirst());

​                for (int row = 0; row < vertexes.length - 1; ++row)

​                {

​                  List<Vertex> vertexListFrom = vertexes[row];

​                  for (Vertex from : vertexListFrom) {

​                      assert from.realWord.length() > 0 : "空节点会导致死循环！";

​                      int toIndex = row + from.realWord.length(); //计算边连接节点的下标

​                      for (Vertex to : vertexes[toIndex]) //可能连接多个节点

​                      {   //构建词图的边，并计算权重

​                          graph.connect(from.index, to.index, MathTools.calculateWeight

​    (from, to));

​                      }

​                  }

​                }

​                return graph;

​            }

该函数从词网首行顶点开始创建词图。主循环遍历顶点列表，计算连接每个顶点的各条边的到达位置索引，然后使用graph.connect方法创建出对应的边，以及计算出该边的权重信息。权重信息的计算公式原理来自贝叶斯公式。公式的计算参见《基于N-最短路径方法的中文词语粗分模型》的一元粗分模型的求解与实现一节。

计算权重信息的calculateWeight方法如下。

​            /＊＊

​             ＊ 从一个词到另一个词的词的花费

​             ＊ @param from 前面的词

​             ＊ @param to   后面的词

​             ＊ @return 分数

​             ＊/

​            public static double calculateWeight(Vertex from, Vertex to) {

​                int frequency = from.getAttribute().totalFrequency;

​                if (frequency == 0) {

​                  frequency = 1;  // 防止发生除零错误

​              }

​              int nTwoWordsFreq = CoreBiGramTableDictionary.getBiFrequency(from.wordID,

​    to.wordID);

​              double value = -Math.log(dSmoothingPara ＊ frequency / (MAX_FREQUENCY) + (1

​    \- dSmoothingPara) ＊ ((1 - dTemp) ＊ nTwoWordsFreq / frequency + dTemp));

​              if (value < 0.0) {

​                  value = -value;

​              }

​              return value;

​            }

其中，frequency是边的起始顶点的词频；nTwoWordsFreq是二元节点的词频；value为最终的计算结果。这个结果作为二元词图的边的权重，参与到最终的NShort路径计算。value的计算需要参考如下几个参数。

​            public static final int MAX_FREQUENCY = 25146057;   // 语料库总词频25146057

​            public static final double dTemp = (double) 1 / MAX_FREQUENCY + 0.00001;

​    // Smoothing 平滑因子

​            public static final double dSmoothingPara = 0.1;    //平滑参数

构建二元词图边的程序如下。

​            /＊＊

​             ＊ 连接两个节点

​             ＊ @param from 起点

​             ＊ @param to 终点

​             ＊ @param weight 权重

​             ＊/

​            public void connect(int from, int to, double weight)

​            {

​                edgesTo[to].add(new EdgeFrom(from, weight, vertexes[from].word + '@' +

​    vertexes[to].word));

​            }

开启调试功能，运行程序，可以看到二元分词过程中的输出如下。

​        打印词图：========按终点打印========

​        to:  1, from:  0, weight:04.60, word：始##始@石

​        to:  2, from:  1, weight:11.36, word：石@国

​        to:  3, from:  2, weight:10.85, word：国@祥

​        to:  4, from:  3, weight:11.57, word：祥@会

​        to:  5, from:  3, weight:11.57, word：祥@会见

​        to:  6, from:  4, weight:08.81, word：会@见

​        to:  7, from:  5, weight:11.40, word：会见@乔

​        to:  7, from:  6, weight:10.91, word：见@乔

​        to:  8, from:  5, weight:02.92, word：会见@未##人

​        to:  8, from:  6, weight:10.91, word：见@未##人

​        to:  9, from:  5, weight:02.92, word：会见@未##人

​        to:  9, from:  6, weight:10.91, word：见@未##人

​        to: 10, from:  7, weight:11.60, word：乔@布

​        to: 11, from:  7, weight:11.60, word：乔@未##人

​        to: 12, from:  8, weight:11.61, word：未##人@斯

​        to: 12, from: 10, weight:11.52, word：布@斯

​        to: 13, from:  9, weight:06.60, word：未##人@说

​        to: 13, from: 11, weight:09.20, word：未##人@说

​        to: 13, from: 12, weight:11.38, word：斯@说

​        to: 14, from: 13, weight:04.02, word：说@未##串

​        to: 15, from: 14, weight:06.80, word：未##串@是

​        to: 16, from: 15, weight:04.67, word：是@最

​        to: 17, from: 15, weight:06.06, word：是@最好

​        to: 18, from: 16, weight:09.49, word：最@好

​        to: 19, from: 17, weight:03.72, word：最好@用

​        to: 19, from: 18, weight:05.48, word：好@用

​        to: 20, from: 19, weight:02.71, word：用@的

​        to: 21, from: 20, weight:05.41, word：的@手

​        to: 22, from: 20, weight:05.32, word：的@手机

​        to: 23, from: 21, weight:10.91, word：手@机

​        to: 24, from: 22, weight:02.81, word：手机@

​        to: 24, from: 23, weight:02.25, word：机@

​        to: 25, from: 24, weight:11.61, word:@末##末

word是拼接后的二元关联词，中间用@进行连接；weight是平滑计算后的词频；from是位于@前面词的行下标（起始节点）; to是位于@后面词的列下标（终止节点）。

一元分词表中的任何一个词都被看作一个节点，两个相邻行的任意节点之间都存在着一种共现关系。二元关联词就是描述这一共现关系的词汇组合，二元关联词的概率能反映在语料中两个一元词汇之间搭配的频繁程度，而此共现关系的定量化就是要计算二元关联词在语料中的概率，由此判断出哪个一元词汇更有可能在此句中是正确的中文词，从而消除前文所讲的两种歧义现象。

图3.4非常清晰地描述了二元词图的实际功能。算法首先按行从上到下，将本行的每个元素与位于下一行的每个元素建立一个关联词，这种关联关系就构成了图中的一条边。例如，“祥”下面有两个节点：一个是“会”；另一个是“会见”。于是建立了两条边“祥@会”和“祥@会见”，如果上层节点有多个，如“斯”、“乔布斯”，那么下层要分别连接上层的每个节点：“乔布”→“乔布@斯”, “布”→“布@斯”; “会见”→“会见@乔布斯”, “见”→“见@乔布斯”。之后，根据生成的结果从二元词典中查询并计算二元词图的边的概率。最后，将生成的结果保存在一个新的二元链表中。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784554685-5ed311ea-ec69-4f92-bc73-4b778aca03f9.jpeg)

图3.4 二元词图（部分）

### 3.3.7 NShort算法原理

下面，到了最关键的时刻，计算NShort最短路径的算法流程。NShort最短路径的调用程序如下。

​        …

​              ///////////////N-最短路径////////////////////

​              NShortPath nShortPath=new NShortPath(graph, nKind);  // 根据词图生成NShort

​    路径

​              List<int[]> spResult = nShortPath.getNPaths(nKind ＊ 2);    // 获得最短路径

​              if (spResult.size() == 0)  {

​                  throw new RuntimeException(nKind + "-最短路径求解失败，请检查上述词网是否存

​    在负圈或悬孤节点");

​              }

​        …

最短路径的计算分为如下两个步骤。第一个步骤是从词图中构建出节点和权重排序数组，第二个步骤是从词图中过滤掉排序数组中权重较高的路径，并根据权重较低的路径生成最终的分词结果。

NShortPath的calculate方法完成了上述第一个步骤，如下代码为其上半部分。

​            /＊＊

​             ＊ 计算出所有节点上可能的路径，以及各条路径的累计权重

​             ＊ 为路径数据提供数据准备

​             ＊ @param inGraph 输入图

​             ＊ @param nValueKind 前N个结果，粗分阶段返回前两个结果。

​             ＊/

​            private void calculate(Graph inGraph, int nValueKind)  {

​                initNShortPath(inGraph, nValueKind); //构建节点和权重队列——堆

​                QueueElement tmpElement;

​                CQueue queWork = new CQueue();

​                double eWeight;

​                for (int nCurNode = 1; nCurNode < vertexCount; ++nCurNode)   {

​                  // 将所有到当前节点（nCurNode)可能到达的边根据eWeight排序，然后压入队列

​                  enQueueCurNodeEdges(queWork, nCurNode);

​                  System.out.print(queWork.toString()); // 临时变量保存了到当前节点的所有可

​    能的边及其累计权重

​                  System.out.println("-----curNode:"+nCurNode+"------\n");

​        …

输出结果如表3.8所示。

表3.8 累计路径表（部分）

| 终止节点 | 起始节点，边权重                                             |
| -------- | ------------------------------------------------------------ |
| to:1     | from:0 weight:4.6024452831351015                             |
| to:2     | from:1 weight:15.95834780869178                              |
| to:3     | from:2 weight:26.812938442096577                             |
| to:4     | from:3 weight:38.38123868491938                              |
| to:5     | from:3 weight:38.38123868491938                              |
| to:6     | from:4 weight:47.19009705655819                              |
| to:7     | from:5 weight:49.784824884439445from:6 weight:58.098833319646246 |
| to:8     | from:5 weight:41.29918832849048from:6 weight:58.098833319646246 |
| to:9     | from:5 weight:41.29918832849048from:6 weight:58.098833319646246 |
| to:10    | from:7 weight:61.3812582347129from:7 weight:69.6952666699197 |
| to:11    | from:7 weight:61.3812582347129from:7 weight:69.6952666699197 |
| to:12    | from:8 weight:52.91306541302703from:8 weight:69.71271040418279from:10 weight:72.90516419305263from:10 weight:81.21917262825943 |
| to:13    | from:9 weight:47.900136643769365from:12 weight:64.29618617352028from:9 weight:64.69978163492513from:11 weight:70.58489622392035from:11 weight:78.89890465912715from:12 weight:81.09583116467604 |
| to:14    | from:13 weight:51.9238658522582from:13 weight:68.31991538200913 |
| to:15    | from:14 weight:58.726250585808735from:14 weight:75.12230011555967 |
| to:16    | from:15 weight:63.40040559925871from:15 weight:79.79645512900964 |
| to:17    | from:15 weight:64.78416669522657from:15 weight:81.1802162249775 |
| to:18    | from:16 weight:72.88800497247385from:16 weight:89.28405450222479 |
| to:19    | from:17 weight:68.50376658858679from:18 weight:78.36726655289716from:17 weight:84.89981611833771from:18 weight:94.7633160826481 |
| to:20    | from:19 weight:71.21175447914247from:19 weight:81.07525444345285 |
| to:21    | from:20 weight:76.62603550377729from:20 weight:86.48953546808767 |
| to:22    | from:20 weight:76.53552508147455from:20 weight:86.39902504578492 |
| to:23    | from:21 weight:87.54087580023351from:21 weight:97.40437576454389 |
| to:24    | from:22 weight:79.34102376343522from:22 weight:89.2045237277456from:23 weight:89.78644517666935from:23 weight:99.64994514097972 |
| to:25    | from:24 weight:90.95490084797176from:24 weight:100.81840081228214 |

根据路径选择2，过滤掉无效的权重，保留有效的权重，如下代码为其下半部分。

​      …

​      // 初始化当前节点所有边的eWeight值

​                for  (int  i  =  0;  i  <  N;  ++i)      weightArray[nCurNode  -  1][i]  =

  Double.MAX_VALUE;

​                tmpElement = queWork.deQueue();   // 将queWork中的内容装入fromArray

​                if (tmpElement ! = null)  {

​                    for (int i = 0; i < N; ++i)  {

​                        eWeight = tmpElement.weight;

​                        weightArray[nCurNode - 1][i] = eWeight;

​                do {

​                              fromArray[nCurNode - 1][i].enQueue(new QueueElement(tmpElement.

​    from, tmpElement.index, 0));

​                              tmpElement = queWork.deQueue();

​                              if (tmpElement == null)  {

​                                i = N;

​                                break;

​                              }

​                          } while (tmpElement.weight == eWeight);

​                      }

​                  }

​               }

​            }

因为处于粗分阶段，对照源码，其中nKind值为2，即返回前两个结果，过滤掉权重较大的结果，则输出结果如表3.9所示。

表3.9 优化累计路径表（部分）

| 终止节点 | 起始节点，边权重                                             |
| -------- | ------------------------------------------------------------ |
| to:1     | from:0 weight:4.6024452831351015                             |
| to:2     | from:1 weight:15.95834780869178                              |
| to:3     | from:2 weight:26.812938442096577                             |
| to:4     | from:3 weight:38.38123868491938                              |
| to:5     | from:3 weight:38.38123868491938                              |
| to:6     | from:4 weight:47.19009705655819                              |
| to:7     | from:5 weight:49.784824884439445from:6 weight:58.098833319646246 |
| to:8     | from:5 weight:41.29918832849048from:6 weight:58.098833319646246 |
| to:9     | from:5 weight:41.29918832849048from:6 weight:58.098833319646246 |
| to :10   | from:7 weight:61.3812582347129from:7 weight:69.6952666699197 |
| to :11   | from:7 weight:61.3812582347129from:7 weight:69.6952666699197 |
| to:12    | from:8 weight:52.91306541302703from:8 weight:69.71271040418279 |
| to:25    | from:24 weight:90.95490084797176from:24 weight:100.81840081228214 |
| to:13    | from:9 weight:47.900136643769365from:12 weight:64.29618617352028 |
| to:14    | from:13 weight:51.9238658522582from:13 weight:68.31991538200913 |
| to:15    | from:14 weight:58.726250585808735from:14 weight:75.12230011555967 |
| to:16    | from:15 weight:63.40040559925871from:15 weight:79.79645512900964 |
| to:17    | from:15 weight:64.78416669522657from:15 weight:81.1802162249775 |
| to:18    | from:16 weight:72.88800497247385from:16 weight:89.28405450222479 |
| to:19    | from:17 weight:68.50376658858679from:18 weight:78.36726655289716 |
| to:20    | from:19 weight:71.21175447914247from:19 weight:81.07525444345285 |
| to:21    | from:20 weight:76.62603550377729from:20 weight:86.48953546808767 |
| to:22    | from:20 weight:76.53552508147455from:20 weight:86.39902504578492 |
| to:23    | from:21 weight:87.54087580023351from:21 weight:97.40437576454389 |
| to:24    | from:22 weight:79.34102376343522from:22 weight:89.2045237277456 |

将返回的节点和权重列表拼成最短路径并输出，结果如下。

​      /＊＊

​       ＊ 获取前index+1短的路径

​       ＊ @param index index = 0 : 最短的路径； index = 1 : 次短的路径， 依次类推。index <=

this.N

​       ＊ @return

​       ＊/

​      public List<int[]> getPaths(int index)  {

​          assert (index <= N && index >= 0);

​          Stack<PathNode> stack = new Stack<PathNode>();

​          int curNode = vertexCount - 1, curIndex = index;

​          QueueElement element;

​          PathNode node;

​          int[] aPath;

​          List<int[]> result = new ArrayList<int[]>();

​          element = fromArray[curNode - 1][curIndex].GetFirst();

​          while (element ! = null)  {

​            // ---------- 通过压栈得到路径 -----------

​            stack.push(new PathNode(curNode, curIndex));

​            stack.push(new PathNode(element.from, element.index));

​            curNode = element.from;

​            while (curNode ! = 0)  {

​                element = fromArray[element.from - 1][element.index].GetFirst();

​                System.out.println(element.from + " " + element.index); //测试检查最

优路径

​                stack.push(new PathNode(element.from, element.index));

​                curNode = element.from;

​            }

​                  // -------------- 输出路径 --------------

​                  PathNode[] nArray = new PathNode[stack.size()];

​                  for (int i = 0; i < stack.size(); ++i)  {

​                      nArray[i] = stack.get(stack.size() - i - 1);

​                  }

​                  aPath = new int[nArray.length];

​                  for (int i = 0; i < aPath.length; i++)    aPath[i] = nArray[i].from;

​                  result.add(aPath);

​                  // -------------- 出栈以检查是否还有其他路径 --------------

​                  do {

​                      node = stack.pop();

​                      curNode = node.from;

​                      curIndex = node.index;

​                  } while (curNode < 1 || (stack.size() ! = 0 && ! fromArray[curNode -

​    1][curIndex].CanGetNext()));

​                  element = fromArray[curNode - 1][curIndex].GetNext();

​              }

​              return result;

​            }

输出结果如下。

最优路径（1）:22->20->19->17->15->14->13->9->5->3->2->1->0。

最优路径（2）:22->20->19->18->16->15->14->13->9->5->3->2->1->0。

选取最优的计算结果，即最优路径1），如图3.5所示，图解的部分最优路径1），标有数字的边就是计算后的最短路径。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784556956-3325c900-0f93-4d27-8730-af4f08c446dd.jpeg)

图3.5 NShort最短路径（部分）

### 3.3.8 后处理规则集

HanLP重写了ICTCLAS版的GenerateWord方法，使该函数的结构变得更加清晰，可以算作ICTCLAS中行数最多的函数。该函数主要编写了分词必需的后处理规则集。函数代码量虽然很大，但是逻辑并不复杂。该函数的简化的流程如下。

​              //////////////日期、数字合并策略

​              for (int[] path : spResult)  {

​                  List<Vertex> vertexes = graph.parsePath(path); // 将路径解析为节点数组

​                  GenerateWord(vertexes, wordNetOptimum);    // 应用后处理规则

​                  coarseResult.add(vertexes);                // 输出粗分结果

​              }

​              return coarseResult;

​        }

fixResultByRule函数是ICTCLAS中Generate函数的简化版，可以很清晰地看到函数包含了大量的规则。下面具体看看都有哪些规则。通过对源代码的分析，从函数列出的规则中抽取出几个有代表性的，并做如下简单的说明。

​            // 通过规则修正一些结果

​            protected static void fixResultByRule(List<Vertex> linkedArray)  {

​                //Merge all seperate continue num into one number

​                mergeContinueNumIntoOne(linkedArray);

​                //The delimiter "--"

​                ChangeDelimiterPOS(linkedArray);

​                //如果前一个词是数字，当前词以“-”或“-”开始，并且不止这一个字符，

​                //那么将此“-”符号从当前词中分离出来。

​                //例如 “3 / -4 / 月”需要拆分成“3 / - / 4 / 月”

​                SplitMiddleSlashFromDigitalWords(linkedArray);

​                //1．如果当前词是数字，下一个词是“月、日、时、分、秒、月份”中的一个，则合并，且当前

​    词的词性是时间。

​                //2．如果当前词是可以作为年份的数字，下一个词是“年”，则合并，词性为时间，否则为数字。

​                //3．如果最后一个汉字是"点" ，则认为当前数字是时间。

​                //4．如果当前串最后一个汉字不是"∶·./"和半角的’.''/'，那么是数。

​                //5．当前串最后一个汉字是"∶·./"和半角的’.''/'，且长度大于1，那么去掉最后一个字符。

​    例如"1."

​                CheckDateElements(linkedArray);

​            }

粗分结果如下。

​        粗分结果[石/ng, 国/n, 祥/ag, 会见/v, 乔布斯/nrf, 说/v, iphone/nx, 是/vshi, 最好/d, 用

​    /p, 的/ude1, 手机/n, 。/w]

### 3.3.9 命名实体识别

初分阶段结束，系统提供了一系列的命名实体识别程序来识别不同类型的命名实体，包括中国人名、日本人名、译名、地名和组织机构名。HanLP的命名实体识别使用标准的HMM模型和Viterbi算法。有关算法的细节，将在第4章详细讲解。本节以人名识别模块为例，给出整个模块的基本执行过程。

​        ……

​        // 实体命名识别

​                  if (config.ner)  {

​                      wordNetOptimum.addAll(vertexList);

​                      int preSize = wordNetOptimum.size();

​                      if (config.nameRecognize)  {

​                          PersonRecognition.Recognition(vertexList, wordNetOptimum, wordNetAll);

​                      }

​                      if (config.translatedNameRecognize)  {

​                          TranslatedPersonRecognition.Recognition(vertexList, wordNetOptimum,

​    wordNetAll);

​                      }

​                      if (config.japaneseNameRecognize)  {

​                          JapanesePersonRecognition.Recognition(vertexList, wordNetOptimum,

​    wordNetAll);

​                      }

​                      if (config.placeRecognize)  {

​                          PlaceRecognition.Recognition(vertexList, wordNetOptimum, wordNetAll);

​                      }

​                      if (config.organizationRecognize)  {

​                          // 层叠隐马模型——生成输出作为下一级隐马输入

​                          vertexList = Dijkstra.compute(GenerateBiGraph(wordNetOptimum));

​                          wordNetOptimum.addAll(vertexList);

​                          OrganizationRecognition.Recognition(vertexList,  wordNetOptimum,

​    wordNetAll);

​                      }

​                      if (! NERexists && preSize ! = wordNetOptimum.size())  {

​                          NERexists = true;

​                      }

​                  }

​              }

​        …

3.2.4节以中国人名词典为例介绍了HMM类型的词典结构。下面简要看一下命名实体的基本算法过程。中国人名识别的算法由PersonRecognition类的Recognition方法完成。函数总体上分为三个部分，代码中的黑体字部分为调试输出的查看内容。

（1）查询句子中每个词的人名识别标签。

​        public   static   boolean   Recognition(List<Vertex>   pWordSegResult,   WordNet

​    wordNetOptimum, WordNet wordNetAll)

​            {

​              List<EnumItem<NR>> roleTagList = roleTag(pWordSegResult) ;  //返回当前词对

​    应的人名标签列表

​              if (HanLP.Config.DEBUG)

​              {

​                  StringBuilder sbLog = new StringBuilder();

​                  Iterator<Vertex> iterator = pWordSegResult.iterator();

​                  for (EnumItem<NR> nrEnumItem : roleTagList)

​                  {

​                      sbLog.append('[');

​                      sbLog.append(iterator.next().realWord);

​                      sbLog.append(' ');

​                      sbLog.append(nrEnumItem);

​                      sbLog.append(']');

​                  }

​                  System.out.printf("人名角色观察：%s\n", sbLog.toString());

​              }

​         …

将输入的句子查询后缀为trie.dat的人名词典，得到粗分结果中每个词对应的人名标签和词频。执行输出，代码如下。

​        人名角色观察：

​        [  A 42634591 ]

​        [石 B 798 C 177 D 140 E 100 K 2 L 1 ]

​        [国 C 2368 D 1390 B 51 E 33 L 4 ]

​        [祥 D 1473 C 484 E 110 K 1 L 1 ]

​        [会见 L 63 K 12 M 3 ]

​        [乔布斯 A 42634591 ]

​        [说 L 10922 K 186 M 40 E 9 C 1 ]

​        [iphone A 42634591 ]

​        [是 K 2507 L 2504 M 123 C 10 E 1 ]

​        [最好 L 4 K 2 ]

​        [用 L 363 K 73 C 41 D 25 M 2 ]

​        [的 L 15411 K 11354 M 96 C 1 ]

​        [手机 L 29 ]

​        [。 L 3667 ]

​        [  A 42634591 ]

（2）查询后缀为value.dat的人名HMM词典，使用Viterbi算法得到最有可能的人名标签。

​        …

​        List<NR> nrList = viterbiExCompute(roleTagList); // Viterbi算法



if (HanLP.Config.DEBUG)





{





StringBuilder sbLog = new StringBuilder();





Iterator<Vertex> iterator = pWordSegResult.iterator();





sbLog.append('[');





for (NR nr : nrList)





{





sbLog.append(iterator.next().realWord);





sbLog.append('/');





sbLog.append(nr);





sbLog.append(" , ");





}



​                  if (sbLog.length() > 1) sbLog.delete(sbLog.length() - 2, sbLog.length());

​                  sbLog.append(']');

​                  System.out.printf("人名角色标注：%s\n", sbLog.toString());

​              }

​        …

输出结果如下。

​        人名角色标注：[ /A ，石/B ，国/C ，祥/D ，会见/L ，乔布斯/A ，说/K , iphone/A ，是/K ，最好/L ,

​    用/K ，的/L ，手机/L , 。/L , /A]

（3）使用parsePattern解析出最终的人名模式。

​        …

​        //解析出最终的模式

​              PersonDictionary.parsePattern(nrList,  pWordSegResult,  wordNetOptimum,

​    wordNetAll);

​              return true;

​            }

PersonDictionary类的parsePattern是一个很大的方法。本节仅给出简要的介绍，而不探讨内部的细节，原因如下。（1）本节的内容主要集中于分词算法，对于一些辅助函数，因为实现的多样化，没有必要详细说明。（2）命名实体类的算法近来变化较大，此类算法的代表性不强，仅对小样本、专业问题的解决可能会有良好的效果，若读者感兴趣，则可以参考HanLP的相关文档。

该方法的主要功能是根据识别出的元模式构成最有可能的人名模式，简而言之，就是得到从元模式到人名模式的一个映射函数。

执行结果如下。

​        识别出人名：石国 BC

​        识别出人名：石国祥 BCD

这里需要说明的是，解析过程并不简单，还调用了Aho Corasick自动机，有兴趣的读者可参考“http://www.hankcs.com/program/algorithm/aho-corasick-double-array-trie.html”中的文章。

### 3.3.10 细分阶段与最短路径

下面讲解分词流程的最后一个环节：细分阶段。

在粗分阶段，把输入的字符串通过一元词典进行原子切分，然后通过二元词典进行消歧，得到了一个基于词典的分词结果。在命名实体识别阶段，进一步对这个结果进行人名、地名、组织机构名的识别，找到了原有句子中的命名实体。

现在，需要将这些结果合并起来，完成最终的分词流程。细分阶段的代码片段如下。

​        …

​        List<Vertex> vertexList = coarseResult.get(0);

​              if (NERexists)

​              {

​                  Graph graph = GenerateBiGraph(wordNetOptimum);

​                  vertexList = Dijkstra.compute(graph);

​                  if (HanLP.Config.DEBUG)

​                  {

​                      System.out.printf("细分词网%s\n", wordNetOptimum);

​                      System.out.printf("细分词图%s\n", graph.printByTo());

​                  }

​              }

​        …

​              return convert(vertexList, config.offset);

​        }

细分词网与未进行命名实体识别阶段的细分词网的不同如下。

​        细分词网：

​        0:[ 始##始]

​        1:[石    石][石国  未##人][石国祥 未##人]

​        2:[国    国]

​        3:[祥    祥]

​        4:[会见  会见]

​        5:[见    见]

​        6:[乔布斯 未##人]

​        7:

​        8:

​        9:[说    说]

​        10:[iphone   未##串]

​        11:

​        12:

​        13:

​        14:

​        15:

​        16:[是   是]

​        17:[最好 最好][最  最]

​        18:[好   好]

​        19:[用   用]

​        20:[的   的]

​        21:[手机 手机]

​        22:

​        23:[。   。]

​        24:[     末##末]

这里明显多了[石国 未##人]、[石国祥 未##人]两个新词。这是来自命名实体识别的结果。将新结果加入一元词网后，与粗分阶段相同，执行GenerateBiGraph方法得到二元词图，代码如下。

​        细分词图：========按终点打印========

​        to:  1, from:  0, weight:04.60, word：始##始@石

​        to:  2, from:  0, weight:03.19, word：始##始@未##人

​        to:  3, from:  0, weight:03.19, word：始##始@未##人

​        to:  4, from:  1, weight:11.36, word：石@国

​        to:  5, from:  2, weight:11.61, word：未##人@祥

​        to:  5, from:  4, weight:10.85, word：国@祥

​        to:  6, from:  3, weight:04.78, word：未##人@会见

​        to:  6, from:  5, weight:11.57, word：祥@会见

​        to:  8, from:  6, weight:02.92, word：会见@未##人

​        to:  8, from:  7, weight:10.91, word：见@未##人

​        to:  9, from:  8, weight:06.60, word：未##人@说

​        to: 10, from:  9, weight:04.02, word：说@未##串

​        to: 11, from: 10, weight:06.80, word：未##串@是

​        to: 12, from: 11, weight:06.06, word：是@最好

​        to: 13, from: 11, weight:04.67, word：是@最

​        to: 14, from: 13, weight:09.49, word：最@好

​        to: 15, from: 12, weight:03.72, word：最好@用

​        to: 15, from: 14, weight:05.48, word：好@用

​        to: 16, from: 15, weight:02.71, word：用@的

​        to: 17, from: 16, weight:05.32, word：的@手机

系统使用Dijkstra的最短路径法（函数名称为compute）计算得到最终的最优路径。最后使用convert将结果返回。

执行结果如下。

​        N-最短分词：[石国祥/nr, 会见/v, 乔布斯/nrf, 说/v, iphone/nx, 是/vshi, 最好/d, 用/p, 的

​    /ude1, 手机/n, 。/w]
