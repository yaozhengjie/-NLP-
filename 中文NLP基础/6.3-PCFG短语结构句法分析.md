6.3 PCFG短语结构句法分析

有关句法分析的算法，全书给出两种不同的算法思想，分别用于处理短语结构分析和依存句法。在短语结构文法中，目前最成熟、精度最高的算法是PCFG算法。该算法的思想是基于概率图模型的，前文讲过有关概率图模型的算法理论。因此，本节主要介绍该算法的详细内容。

另一种基于依存句法理论的分析算法，目前最新、最高效的是基于转换（Transition-Based）的LSTM算法，最新版的LTP依存句法分析和Stanford依存句法分析都实现了该算法。该算法的思想是基于深度学习算法理论的，在后面将详细介绍。

### 6.3.1 PCFG短语结构

完全基于乔姆斯基的短语结构文法推导得到的结果，往往得到很多不精确甚至有各种歧义的结果，难以保证计算精度的要求。为此，人们做了大量的研究，有的选择其他的句法思想，有的则通过开发树库资源来调整算法。前文介绍过的宾州树库就是基于人工标注的短语结构树库。树库资源提供了两个很重要的优势：一方面可以从现有的大规模语料中学习出新规则；另一方面，可以很容易地获得每条规则概率的信息。而后者就发展成了PCFG短语结构算法基础。

PCFG即Probabilistic CFG，也称为Stochastic CFG，直观的意义就是基于概率的短语结构分析。前文讲到，乔姆斯基的短语结构文法表示为一个四元组：G= (X, V, S, R)。而在PCFG中增加了一个概率信息，变为如下五元组。

PCFG=(X, V, S, R, P)

X是一个有限词汇的集合（词典）。它的元素称为词汇或终结符。

V是一个有限标注的集合，叫作非终结符集合。它的元素称为变量或非终结符集合。

S∈V，称为文法的开始符号。

R是有序偶对(α, β)的集合，也就是产生的规则集。

P代表每个产生规则的统计概率。

如果把“->”看作一个运算符，PCFG可以写成如下形式。

形式：A -> α, P

约束：![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784696165-b6073e80-b9c9-4ad1-aec1-11a076c51dc5.jpeg)

约束的含义：![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784696867-12763c9f-aef7-4dad-89c3-7f8f7b796807.jpeg)

下面根据一个例子来看PCFG求解最优句法树的过程。有一个规则集，内容如下。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784697376-8019a66a-e87e-458f-bb2d-81ffafed1e2c.jpeg)

给定句子S:Astronomers saw stars with ears.

经过CFG推导，得到两棵句法树，如图6.20所示。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784698248-f262ee58-4b7a-4ea7-b07a-8e48f109f285.jpeg)

图6.20 CFG推导出的两棵子树

我们的目标是要找到哪一棵树才是最优的选择。

在计算之前，给出分析树概率的基本假设，内容如下。

❑ 位置不变性：子树的概率与其管辖的词在整个句子中所处的位置无关，即对于任意的k, P(Ak(k+c)->w)一样。

❑ 上下文无关性：子树的概率与子树管辖范围以外的词无关，即P(Akl->w|任何超出k～l范围的上下文)=P(Akl->w)。

❑ 祖先无关性：子树的概率与推导出该子树的祖先节点无关，即P(Akl->w|任何除A以外的祖先节点)=P(Akl->w)。

图6.21所示为三种基本假设下的计算过程。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784698980-bfd11e75-3b46-4a0b-a9ea-bd3ba21af49d.jpeg)

图6.21 三种基本假设下的计算过程

依据这三个假设，分别得到两棵子树的概率，内容如下。

P(t1)=S×NP×VP×V×NP×NP×PP×P×NP

=1.0×0.1×0.7×1.0×0.4×0.18×1.0×1.0×0.18=0.000 907 2

P(t2)=S×NP×VP×VP×V×NP×PP×P×NP

=1.0×0.1×0.3×0.7×1.0×0.18×1.0×1.0×0.18=0.000 680 4

可以很容易地根据概率值的大小来做出选择。

根据上述例子，我们很自然地想到关于PCFG算法的如下三个基本问题。

❑ 给定句子W=w1,w2, …,wn和PCFG G，如何快速计算得到P(W|G)？

❑ 给定句子W=w1,w2, …,wn和PCFG G，如何选择最佳的句法树？

❑ 给定句子W=w1,w2, …,wn和PCFG G，如何估计G的参数，使得P(W|G)最大？

### 6.3.2 内向算法和外向算法

1．内向算法

使用内向算法解决第一个问题。内向算法的基本思想是从给定字符串的底层向上逐次推导出句子的全部概率。使用动态规划的方法计算由非终结符A推导出的某个字符片段wi, wi+1, …, wj的概率αij(A)。语句W=w1, w2, …, wn的概率即为文法G(S)中S推导出的字符串的概率α1n(S)。

定义：内向变量αij(A)是由非终结符A推导出的语句W中子字符串wi, wi+1, …, wj的概率。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784699448-e3524ad4-9abb-416a-8895-961d97a592f8.jpeg)

计算αij(A)的递推公式得到：

（1）αii(A)=P(A→wi)。 （一步直接推导得出）

（2）![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784700412-5e7d0719-dda5-4ba3-acd2-b2b92961780e.jpeg)。 （需要经过中间步骤得出）

如图6.22所示，当i=j时，字串wi, wi+1, …, wj只有一个词wi，由A推导出wi的概率就是产生式A→wi的概率P(A→wi)；当i≠j时，也就是说，字符串wi, wi+1, …, wj中至少有两个词。根据约定，A要推导出该字符串，必须首先运用产生式A→BC。那么，可用B推导出前半部wi, wi+1, …, wk, C推导出后半部wk+1, wk+2, …, wj。由这一推导过程产生的概率为：![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784702599-d28252b2-b36a-4d64-825f-8d49c97f4746.jpeg)。考虑到B、C和k取值的任意性，应计算各种情况下概率的总和。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784704155-1a2f65ba-6aa4-4a07-bbaf-46c9f7e33025.jpeg)

图6.22 递推公式示意图

内向算法描述如下。

（1）输入：文法G(S)，语句W=w1, w2, …, wn。

（2）输出：![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784704771-4e11b71d-1658-4e73-8402-7cbdaec16b20.jpeg)。

步骤1，初始化：αij(A) =P(A->wi)

A∈VN,1≤i≤j≤n

步骤2，归纳计算：![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784705622-fde133d1-428b-4b02-bc09-afcbceb66087.jpeg)

步骤3，终结：![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784706831-ee4622f9-80bc-4e4b-9c14-13b003e55bd0.jpeg)

2．外向算法

使用外向算法解决第一个问题。外向算法的基本思想是从给定字符串的顶层向下逐次推导出句子的概率。

定义：外向变量βij(A)是由文法初始符号S推导出语句W=w1, w2, …, wn的过程中，到达扩展符号串w1, …, wi-1, A, wj+1, wn的概率(A=w1, …, wj)。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784707606-80253ea5-2a8f-476e-9673-1559caa113a8.jpeg)

计算βij(A)的递推公式得到：

（1）β1n(A)= δ(A, S)。 （初始化）

（2）![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784708118-a0165690-d2d2-4fa8-af3e-c05339c220e2.jpeg)（推导树是一棵二叉树）。

图6.23所示为“（2）式”中左右两式的含义示意图。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784708800-0473822f-06d1-446b-8612-31446e384cb9.jpeg)

图6.23 “（2）式”中左右两式的含义示意图

图6.23的说明如下。

（1）当i=1, j=n时，即wi, wi+1, …, wj是整个语句时，由于语法中不可能有规则S->A，因此，S推导出W的过程中，如果A≠S，那么A推导出W的概率为0（β1n(A)）。如果A=S，那么β1n(A)就是由初始符S推导出W的概率。因此，β1n(A)=1。

（2）当i≠1或j≠n时，如果在S推导出W的过程中，出现了字符串w1, …, wi-1, A, wj+1, …, wn，则该推导过程必定使用了规则B->AC或B->CA。如果运用了规则B->AC，则该推导可以分解为如下内容。

❑ 由S推导出w1, …, wi-1, B, wk+1, …, wn，其概率为βik(B)。

❑ 运用产生式B->AC扩展非终结符B，其概率为P(B->AC)。

❑ 由非终结符C推导出wj+1, …, wk，其概率为αj+1, k(C)。

外向算法描述如下。

（1）输入：文法G(S)，语句W=w1, w2, …, wn。

（2）输出：βij(A), A∈N , 1=i=j=n。

步骤1，初始化：β1n(A) =δ(A, S), A∈N。

步骤2，归纳计算：j从n-1到0, i从1到n-j，重复计算如下。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784709264-4cb3f078-b812-4683-b18f-2a7db365d881.jpeg)

### 6.3.3 Viterbi算法

第二个问题是如何选择最佳的句法树，其解决方法使用前文讲过的Viterbi算法。在短语结构文法中，Viterbi的定义如下。

Viterbi变量γij(A)是由非终结符A推导出的语句W中字符串wi, wi+1, …, wj的最大概率。变量ψi, j用于记忆字符串W=w1, w2, …, wn的Viterbi语法分析结果。

Viterbi算法描述如下。

输入：文法G(S)，语句W=w1, w2, …, wn。

输出：γ1n(S)。

步骤1，初始化：γij(A) = P(A→wi), A∈VN,1≤i≤j≤n。

步骤2，归纳计算：j=1, …, n, i=1, …, n-j，重复如下计算。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784710267-11cc7b27-c210-40c2-8693-80aed0652bd6.jpeg)

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784710947-2edc26cf-7bd6-46c9-876e-d8431831ee9c.jpeg)

步骤3，终结：![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784711439-90bcaae3-5cff-4f46-aefe-3e0743a96c39.jpeg)。

### 6.3.4 参数估计

如果有大量已标注语法结构的训练语料，则可通过语料直接计算每个语法规则的使用次数。已知训练语料中的语法结构，记录每个语法规则的使用次数，用最大似然估计计算PCFG的参数，即

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784712047-56851619-b21c-42ee-95c2-d91d208ce970.jpeg)

一般来讲，树库的标注需要投入较大的人力，时间和资金投入都比较大。而且，树库也不可能覆盖所有的词汇信息，在NLP中，使用有限的资源预估目标的概率分布，这种情况是常态。所谓参数估计，就是对未知现象出现概率的一种计算方法。PCFG中常用的方法是EM（Expectation Maximization）算法。

EM算法描述：初始时随机地给这些参数赋值，得到语法G0，依据G0和训练语料，得到语法规则使用次数的期望值，以期望次数运用于最大似然估计，得到语法参数的新的估计，由此得到新的语法G1，由G1再次得到语法规则的使用次数的期望值，然后又可以重新估计语法参数。循环这个过程，语法参数将收敛于最大似然估计值。

PCFG中使用的方法称为内向、外向算法：给定CFG G和训练数据W=w1, w2, …, wn，语法规则A->BC的使用次数的期望值如下。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784712883-0e66f41f-1504-4f30-898f-aef72cd3da68.jpeg) （1）

解释：给定了语句，PCFG G中产生式A->BC被用于产生的使用次数的期望值为：在所有可能的1≤i≤k≤j≤n的情况下，w1, w2, …, wn的语法分析结构中，wi, …, wk由B导出，wk+1, …, wj由C导出，wi, …, wj由A导出的概率总和。

类似的，语法规则A -> a的使用次数的期望值如下。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784714620-d5364852-82f8-467d-9718-7591dc8020fb.jpeg) （2）

G的参数可由如下公式重新估计。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784715936-8ac84280-22ff-4589-87b2-bad714b8ffd5.jpeg) （3）

其中，μ要么为终结符号，要么为两个非终结符号串，即A->μ为乔姆斯基语法范式要求的两种形式。

内向、外向算法的步骤如下。

步骤1，初始化：随机地给P(A->μ)赋值，使得![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784716796-59300b30-c18c-4f8d-985f-729141ed5ea7.jpeg)，由此得到语法G0。令i=0。

步骤2, EM步骤如下。

① 由Gi根据公式（1）和公式（2），计算期望值C(A->BC)和C(A->a)。

② M-步骤：用E-步骤所得的期望值，根据公式（3）重新估计P(A->μ)，得到语法Gi+1。

步骤3，循环计算i=i+1，重复EM步骤，直至P(A->μ)收敛。

### 6.3.5 Stanford的PCFG算法训练

在Stanford PCFGParser的LexicalizedParser类中，有一个Main方法。它定义了训练外部语料的一些指令和参数，用户可以从命令行访问指向。该Main方法可以用于构建和序列化树库的数据，数据来源可以是文件或URL，一气呵成地训练和测试一个树库的解析结果（主要用于解析器质量测试）。

从一个树库的目录中训练一个解析器，并将训练结果存储为序列化语法文件，需要执行如下指令。

​        java -mx1500m edu.stanford.nlp.parser.lexparser.LexicalizedParser [-v] -train

​      trainFilesPath [fileRange] -saveToSerializedFile serializedGrammarFilename

主要参数如下。

❑ [-v]：这里省略了一些参数细节。限于篇幅的关系，仅给出必需的参数。-tLPP：选定训练树库的语种类，中文选择：edu.stanford.nlp.parser.lexparser. ChineseTreebankParserParams。

❑ trainFilesPath：输入的训练文件目录。

❑ [fileRange]：训练文件范围为0～1 000个文件。

❑ serializedGrammarFilename：输出序列化文件名。

从一个树库的目录中训练一个解析器（但不存储为序列化文件），并提供测试结果报告，需要执行如下指令。

​        java -mx1500m edu.stanford.nlp.parser.lexparser.LexicalizedParser [-v] -train

​    trainFilesPath [fileRange] -testTreebank testFilePath [fileRange]

❑ testFilePath：测试文件路径。

❑ [fileRange]：测试文件范围。

在训练过程中，如果序列化文件（serializedGrammarFilename）的路径以“.gz”结尾，那么写入时就执行GZip压缩。如果序列化文件路径是一个URL，以http://开头，则解析器就从这个URL中读取。[fileRange]指定了一个数字值，而这个值必须包含在文件名中（这非常适用于目前大多数的树库）。它的格式诸如“200-2199”或“1-300”,“500-725”, “9000”或者“1”（如果所有的树都在一个文件中，可以省略此参数或只给一个伪参数，如0）。如果解析的文件名是“ - ”，那么解析器从标准输入解析。

解析器除提供序列化的模型文件之外，还可以提供一个模型的文本版本，以提供用户审阅，需要执行如下指令。

​        java edu.stanford.nlp.parser.lexparser.LexicalizedParser [-v] -train trainFilesPath

​    [fileRange] [-saveToSerializedFile grammarPath] [-saveToTextFile grammarPath]

❑ saveToTextFile：文本格式的模型文件路径。

关于Stanford Parser的更详细的内容可以在如下路径中找到。

http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/parser/lexparser/LexicalizedP arser.html#main-java.lang.String:A-。

最后，用python代码来实现Stanford Parser的训练过程，回顾一下第1章，实现了一个名为StanfordParser的Python类，现在在这个类上做一些修改，修改后的代码如下。

​        class StanfordParser(StanfordCoreNLP):

​            def __init__(self, jarpath):

​                StanfordCoreNLP.__init__(self, jarpath)

​                self.modelpath = "" # 模型文件路径

​                self.classfier = "edu.stanford.nlp.parser.lexparser.LexicalizedParser"

​                self.opttype = ""

​            def __buildcmd(self): # 构建命令行

​                self.cmdline = 'java -mx500m -cp "'+self.jarpath+'" '+self.classfier+'

​    -outputFormat "'+self.opttype+'" '+self.modelpath+' '

​            def parse(self, sent): #解析句子

​                self.savefile(self.tempsrcpath, sent)

​                tagtxt = os.popen(self.cmdline+self.tempsrcpath, "r").read()  # 输出到

​                                                                                  \# 变量中

​                self.delfile(self.tempsrcpath)

​                return   tagtxt

​            def tagfile(self, sent, outpath): # 输出到文件

​                self.savefile(self.tempsrcpath, sent)

​                os.system(self.cmdline+self.tempsrcpath+' > '+outpath )

​                self.delfile(self.tempsrcpath)

​            def __buildtrain(self, trainpath, parsemodel): # 创建模型文件

​                self.trainline = 'java -mx2g -cp "'+self.jarpath+'" '+self.classfier +'

​    -tLPP edu.stanford.nlp.parser.lexparser.ChineseTreebankParserParams -train "'+trainpath

​    +'" -saveToSerializedFile "'+parsemodel+'"'

​            def __buildtraintxt(self, trainpath, parsemodel, txtmodel): # 创建模型文件

​                self.trainline = 'java -mx2g -cp "'+self.jarpath+'" '+self.classfier +'

​    -tLPP  edu.stanford.nlp.parser.lexparser.ChineseTreebankParserParams  -train  "'+

​    trainpath+'" -saveToSerializedFile "'+parsemodel+'" -saveToTextFile "'+txtmodel+'"'

​            def trainmodel(self, trainpath, parsemodel, txtmodel=""): # 训练模型

​                if txtmodel:

​                      self.__buildtraintxt(trainpath, parsemodel, txtmodel)

​                else:

​                      self.__buildtrain(trainpath, parsemodel)

​                os.system(self.trainline)

​                print "save model to ", parsemodel

在__init__(self, jarpath)构造方法的参数中，仅保留了jarpath这一个参数，去掉其他两个参数。在类的内部也做了相应的修改。之后，增加了两个训练方法：__buildtraintxt和__buildtrain。__buildtraintxt用以输出文本格式和压缩序列化格式的模型文件，__buildtrain仅用于输出序列化格式的压缩文件。

它们都由trainmodel这个方法调用。具体调用根据输入参数的不同自动判断。Trainmodel有如下三个参数。

❑ trainpath：训练文件路径。

❑ parsemodel：训练后的模型。

❑ txtmodel：训练后文本形式的模型。

外部调用代码如下。

​        \# -＊- coding: utf-8 -＊-

​        import sys, os

​        from stanford import ＊

​        \# 设置 UTF-8输出环境

​        reload(sys)

​        sys.setdefaultencoding('utf-8')

​        os.environ['JAVA_HOME'] = 'D:\\Java7\\jdk1.8.0_65\\bin\\java.exe' # 配置环境变量

​        root = "E:/nltk_data/stanford-corenlp/" # 库所在路径

​        trainpath = "trainfile/" # 宾州树库的样例文件chtb_0001.mrg（utf-8）

​        modelpath = "trainmodel.ser.gz"

​        txtmodelpath = "trainmodel.ser"

​        parser=StanfordParser(root)

​        result = parser.trainmodel(trainpath , modelpath, txtmodelpath)

​        \# result = parser.trainmodel(trainpath , modelpath)

​        print result

可以看到，训练后在根目录下生成两个模型：trainmodel.ser.gz和trainmodel.ser。训练阶段输出的部分日志如下。

​        LexicalizedParser invoked on Sun Jul 31 07:44:36 CST 2016 with arguments:

​          -tLPP  edu.stanford.nlp.parser.lexparser.ChineseTreebankParserParams  -train

​    trainfile/ -saveToSerializedFile trainmodel.ser.gz -saveToTextFile trainmodel.ser

​        Training a parser from treebank dir: trainfile/

​        Reading trees...done [read 11 trees]. Time elapsed: 0 ms

​        Options parameters:

​        …

​        Using ChineseTreebankParserParams chineseSplitDouHao=false chineseSplitPunct=true

​    chineseSplitPunctLR=false markVVsisterIP=true markVPadjunct=true chineseSplitVP=3

​    mergeNNVV=false  unaryIP=false  unaryCP=false  paRootDtr=false  markPsisterIP=true

​    markIPsisterVVorP=true  markADgrandchildOfIP=false  gpaAD=true  markIPsisterBA=true

​    markNPmodNP=true markNPconj=true markMultiNtag=false markIPsisDEC=true markIPconj=

​    false markIPadjsubj=false markPostverbalP=false markPostverbalPP=false baseNP=false

​    headFinder=levy discardFrags=false dominatesV=false

​        Binarizing trees...done. Time elapsed: 13 ms

​        Extracting PCFG...done. Time elapsed: 180 ms

​        Extracting Lexicon...ChineseUWM: treating unknown word as the average of their

​    equivalents by first-character identity. useUnicodeType: false

​        ChineseUWM: using Good-Turing smoothing for unknown words.

​        Total tokens: 416.0

​        Total WordTag types: 236

​        Total tag types: 35

​        Total word types: 227

​        done. Time elapsed: 151 ms

​        Compiling grammar...done Time elapsed: 1 ms

​        Extracting Dependencies...done. Time elapsed: 78 ms

​        Done training parser.

​        Writing parser in text grammar format to file trainmodel.ser........done.

​        Writing parser in serialized format to file trainmodel.ser.gz done.

​        Grammar  States   Tags Words    UnaryR   BinaryR  Taggings

​        Grammar  194 35  228 45  252 236

​        ParserPack is edu.stanford.nlp.parser.lexparser.ChineseTreebankParserParams

​        Lexicon is edu.stanford.nlp.parser.lexparser.ChineseLexicon

​        Options parameters:

​        …

​        save model to  trainmodel.ser.gz

最后，给出文本的模型文件片段，内容如下。

​      …

​      BEGIN LEXICON edu.stanford.nlp.parser.lexparser.ChineseUnknownWordModel

​      "NR" -> "上海" SEEN 4.0

​      "NR" -> ".＊." SEEN 19.0

​      "NR" -> "浦东" SEEN 10.0

​      "NN" -> ".＊." SEEN 128.0

​      "CC" -> "与" SEEN 1.0

​      "VV" -> "开发" SEEN 1.0

​      "CC" -> ".＊." SEEN 8.0

​      "NN" -> "开发" SEEN 4.0

​      "VV" -> ".＊." SEEN 48.0

​      ".$$." -> ".$." SEEN 11.0

​      "NN" -> "建设" SEEN 5.0

​      ".$$." -> ".＊." SEEN 11.0

​        "NN" -> "法制" SEEN 3.0

​        …
