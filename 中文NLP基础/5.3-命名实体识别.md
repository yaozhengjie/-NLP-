5.3 命名实体识别

第3章已简单探讨过命名实体的实现，当时是作为中文分词的一部分来介绍的。本节将全面、详细地介绍命名实体的概念、模块架构及对应的算法策略。

### 5.3.1 命名实体

命名实体识别（Named Entity Recognition, NER），又称为“专名识别”，它的主要任务是对于一篇待处理文本，识别出其中出现的人名（Person）、地名（Location）、组织机构名（Organization）、日期（Data）、时间（Time）、百分数（Percentage）、货币（Monetary Value）这7类命名实体。其中，人名、地名、组织机构名的识别是最难、也是最重要的三类。本节将主要讲述这三类命名实体的识别。从语言分析的全过程来看，命名实体识别属于中文分词中未登录词识别的范畴。在涉及未登录词识别的所有问题中，命名实体是未登录词中数量最多、识别难度最大、对分词效果影响最大的问题。命名实体识别技术是信息抽取、信息检索、机器翻译、问答系统等多种自然语言处理技术必不可少的组成部分。

1．中文人名的特征

中文人名一般由姓氏和名字两部分构成，其构成方式包括：单字姓+单字或双字名、复姓（如上官、欧阳等）+单字或双字名、双姓（父母姓氏）+单字或双字名、夫姓+父姓+单字或双字名（如陈方安生、范徐丽泰等）、单姓+三字名，等等。

虽然《中华姓氏大辞典》显示，中国古今各民族用汉字记录的单姓6 931个、双字姓4 329个、三字姓1 615个、四字姓569个、五字姓96个、六字姓22个、七字姓7个、八字姓3个、九字姓7个（如爨邯汕寺武穆云籍鞲）、十字姓1个（伙尔川扎木苏他尔只多），但姓名汉字的个数以2～4字占绝对多数，而当今仍然使用、活跃的中文姓氏大概只有1 000个。据统计，常用的姓氏分布也很不均匀且相对集中。“王、李、刘、张、陈”这5大姓就占了姓名样本数的29.1%，前18个姓占50.3%，前181个姓占90.3%，前586个姓占98.6%，其余姓氏仅占不到1.5%。常用姓氏文件可从http://www.threedweb. cn/thread-1585-1-1.html下载。

名字用字分布较姓氏用字要平缓、分散。共得到3 679个名字用字，频率最高的前17个字的覆盖率为10.5%，前80个字为30.3%，前207个字为50.3%，前1 122个字为90.4%。名字用字涉及范围很广。从所属的词类看，不仅有实词，还有各类虚词。如副词“常、太、必、非、更、也、级、又、皆”等，介词“以、向、从、于、把”，连词“而、虽、且、与”等。从感情色彩看，多使用褒义字和中性字，但也出现了一些贬义字或不太文雅的字，如“狼、恶、悲、暴、虫”等。

值得注意的是，某些汉字既可用作姓氏，又可用作名字用字。如“林、方、金、江、万、颜、童、柳”等。

2．中文地名的特征

较之人名相比，地名更像一个闭集。绝大部分地名都可以通过相关的资料覆盖，如《中国地名录》、《地理词典》、《地物名称》词典集（可从http://www.threedweb. cn/forum-73-1.html下载）。而且，地名结尾经常有地名特征词出现，如省、市、县、乡、村、山等。多个地名常通过一些连接词或者连接符号一起出现，如“/吉林省/四平市/梨树县/梨树镇/霍家店村”。这样的地名大多表示行政地区的地名，这些对地名识别来讲都是有利的信息。上述都是地名识别比较容易的地方。

同时，地名识别也有其困难之处。

第一，据统计，中文地名用词一方面比较自由、分散，地名录中共享汉字3 685个；另一方面，中文地名用词又有相对集中的覆盖能力。但有时候，地名特征词出现的情况比较复杂：既可以作为普通用词出现，而不是真正的、具体的地名，如小山村、大都市，又可以出现在地名其他位置或作为地名的前部词，如三门峡市、张家界市等。

第二，不像中文姓名那样，地名长度没有严格限制。在真实文本中，经常会有地名简称出现。如“峨嵋山”常写作“峨嵋”，因为在武侠作品中它还代表一个武功门派，所以识别中常产生歧义。地名还可能和专名发生冲突。例如，作为其他命名实体的一部分，如“大连市机械厂”；作为组织机构名的一部分，如“海宁市长安邮电局”，切分成“海宁市/长安/邮电局”。这里长安容易被误切分为一个地名的古称。

第三，地名用词的情况非常复杂。地名常常与介词、动词、方位词之类的指示词连用，这些指示词对地名识别能起到标志作用，但有些指示词也可以作为地名组成部分，如“上甘岭”、“来复乡”等。还有一些地名，每一个单字均为高频单字词，如“西/直/门”、“白/家/塔”。这些字如果连用则作为地名用词的可能性非常大，但如果单个出现或部分出现，则作为非地名成分出现的次数也很多。

第四，多字词可以在地名不同的位置出现，可以在地名首部出现，也可以在地名中部出现。同时，相对于单字词来讲，地名词典中的多字词统计的信息不充分，对多字词的判断也是地名识别中的一个难点。

上述情况都增加了地名识别难度。综上所述，地名识别同样需要解决交叉歧义、边界模糊等问题。

3．组织结构的特征

较之前两种命名实体的识别，组织结构的识别更加困难。上述各种难点不同程度地出现在组织机构名识别中。此外，组织机构名的识别还有自身的问题。首先，中文组织机构名词常常表示为两种结构：全称和简称。“联想”、“方正”都是机构的简称。有时，同一机构的全称有不同的简称。例如，“上海交通大学”可以简称为“上海交大”或者“交大”。再如，“人民代表大会”的简称为“人大”或“人代会”，而“人民大学”也简称为“人大”。大量的机构简称的出现，使得机构名的识别变得更加困难。

其次，中文机构名的长度更加不稳定。少的仅仅有两个字，如“宝钢”，长的几乎比一个句子还要长，例如“中国人民政治协商会议第八届全国委员会常务委员会”。这给中文的命名实体边界的确定造成了很大的困难。

最后，组织机构名含有复杂的内部结构，通常是其他的命名实体。在这些命名实体中，地名占很大的比例，人名等也占相当一部分的比例。这些成分都制约了组织机构名的识别。

### 5.3.2 分词架构与专名词典

命名实体识别中遇到各种各样难以克服的困难，人们为此也想出了很多解决的办法。人们提出了各种不同的分词架构，期望通过分词架构的设计或者引入外部资源（专名词典）来提高命名实体识别精度或者减小命名实体识别对整体分词结果的干扰。

一个大型分词系统的简单架构如图5.7所示。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784671602-af18a4f7-91e0-455d-b604-e4b8a3fe25be.jpeg)

图5.7 一个大型分词系统的简单架构

领域分词系统的执行步骤如下。

（1）将输入文本先使用核心词典进行基础分词处理。

（2）对分词结果的文本再按所属领域计算文本所属的分类。

（3）根据文本所属的领域不同，执行使用领域专属的分词器，再对领域内的专名进行分词。

上述架构的特点，是将分词器针对不同领域做了优化，不同领域的文本可以使用本领域内专门的外部词典进行识别。专业化的细分势必带来更高的识别精度，这种策略针对领域专属的专名往往能够达到更好的分词效果。

其中，一种设计思路是将命名实体任务从中文分词的基础模块中分离出来，命名实体作为一个独立的模块位于基础分词步骤之后。在HITLtp 3.3（哈工大开源语言包https://github.com/HIT-SCIR）和Stanford NLP中都把命名实体识别作为一个单独的模块来处理，独立于分词流程之外。这样做的好处是命名实体模块可以具有更丰富的结构。比如，人名识别、地名识别和组织机构名识别都作为单独的子模块来处理。命名实体识别系统架构如图5.8所示。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784672355-be54cb1c-95aa-41ea-aea3-e9202ef7c08e.jpeg)

图5.8 命名实体识别系统架构

命名实体识别系统执行步骤如下。

（1）将输入文本先使用核心词典进行基础分词处理。

（2）再对分词结果进行命名实体识别。命名实体识别分为如下三个步骤。

① 使用专门的模型对分词结果进行人名识别。

② 使用专门的模型或词典对分词结果进行地名识别。

③ 使用专门的模型或词典对分词结果进行组织机构名识别。

（3）结果整合。将三种结果合并到基础分词的结果中作为最终的结果输出。常用的合并方式可以直接覆盖，好一些的使用最短路径算法。

上述架构的好处在于，对于不同类型的命名实体，可以使用专门的算法和专门的外部词典进行识别，专业化的算法和语料资源对专门的问题势必达到更高的精度。系统再将不同类型的识别结果通过一定的算法整合成最终的输出结果。这样所达到的分词结果往往最更好。

实践证明，标准的CRF算法即可解决大多数人名识别的问题；而地名的词汇更像一个闭集，丰富的外部词典对地名的识别具有很重要的作用。结合词典的CRF算法是CRF的一个变种，通常在地名识别上会达到更好的效果。对于组织机构识别国际上最通用的是semi-CRF算法，国内的层叠HMM算法也比较成功，对于专业化比较强的命名实体识别可以达到比较好的效果。

### 5.3.3 算法的策略——词典与统计相结合

前文中不同程度地提到了外部词典的引入。引入外部词典对命名实体识别有着很大的用处，而且外部词典的获得所需要的代价远远小于为相关领域标注分词语料所需要的代价。如果现有的分词方法能充分合理地利用外部词典，一方面可以提高句子中命名实体的准确率，另一方面还可以使系统具有良好的领域自适应性。特别在为特定领域分词时，只需加载该领域的专属词典，即可很好地解决该领域内专名的识别问题。当领域改变之后，原有的基础分词模型不需要再改变，只需改变领域词典即可，因此不需要针对不同领域重新去训练不同的基础模型。

本节以哈工大的中文分词系统作为案例，简要剖析词典与CRF算法结合如何实现领域自适应的中文分词。

第1章已经介绍过哈工大的Ltp 3.X的开源分词系统。该系统的源代码可以从https://github.com/HIT-SCIR/ltp下载。下面摘要其中的部分内容来看一下Ltp 3.2系统的相关实现。如果希望完全理解下文，建议参照如下两个资源帖子：在http://www. threedweb.cn/thread-1394-1-2.html中可以找到解析后的分词词典文件；在http://www. threedweb.cn/thread-1396-1-1.html中可以找到有关中文分词部分的源码解析文章。

下载和解压解析后的分词词典文件之后，可以看到词典文件一共分为如下4类文件。

❑ internal_lexicon.dat：内部词典文件。

❑ labels.dat：标签表。

❑ space0～14：特征函数表，一共15个文件。

❑ param：特征函数的权值表。

对上述解析出的数据表分别介绍如下。

1．标注、特征模板、语言模型

Ltp 3.2中文分词系统是基于CRF算法的中文分词系统，labels.dat指明其标注方法使用的是start/end表示法。

❑ B：当前字是一个词的开始。

❑ I：当前字在一个词内部。

❑ E：当前字是一个词的终结。

❑ S：当前字就是一个词，该词是独字词。

模板训练出的CRF语言模型保存在15个词典文件中，所取的文件名分别为：space0～space14。从space0到space14特征词典的词条序列号呈跨文件的递增形式，15个文件共计收录了1 618 355个语言模型特征。词典文件词条中还有几个断句标志符，它们分别为BOS = "_bos_"; EOS = "_eos_"; BOT = "_bot_"; EOT = "_eot_"。这4个标志符在词典装入模板时用于区分句首与句尾的边界。

通过源代码的分析，可以得到HIT-Ltp 3.2中文分词的特征模板（见表5.10）。

表5.10 HIT-Ltp 3.2中文分词的特征模板

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784674098-d4f8030c-f9bf-4b86-b615-1caf68f809a9.jpeg)

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784675151-98fb9aaf-9528-415d-af3e-a3f4c1bd3664.jpeg)

该算法使用的特征模板与CRF++大同小异，其中，c表示原子字符；ct表示原子字符类型（Chartype）。但也有特殊之处，在内部词典匹配结果的特征模板中，lex1～lex3表示在内部词典查询到词的位置标识。下面给出一个例子。

如果测试样本中包含“首都”这个词，而“首都”一词可以从内部词典中查到。在用CRF识别时，“首”字的内部词典特征为，{lex1}=2, {lex2}=0, {lex3}=0; “都”字的内部词典特征为，{lex1}=0, {lex2}=2, {lex3}=0。通过内部词典特征模板的选项决定了从词典查询到的词汇在分词结果中的优先地位。

最后，所有的矩阵概率值都被放到了Param表中。其中，_W[i]是概率值；_W_sum[i]是频率值。Param表的长度由如下两部分构成。

（1）1 618 351（词典总词条）×4（labels数）=6 473 404。表示space中的词属于每个labels标签的概率（频率）。也是CRF中一元特征函数矩阵的总元素数。

（2）4×4=16表示语料库中两两标签搭配的概率。也是CRF中二元特征函数矩阵的总元素数。

这两项相加后值为6 473 404+16=6 473 420，即Parameter维度的总行数。

2．内部词典

需要重点讲解的部分是，该算法提供了一本简易的内部词典，就是前面名为internal_lexicon.dat的文件，部分输出结果如图5.9所示。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784676237-55b9deee-8f9e-40fd-ba18-fa59a41cdb01.jpeg)

图5.9 部分输出结果

图5.9中，该词典一共收录了8 625个词，是一本简易的内部词典。从该词典中查到的词汇在分词阶段具有更高的优先级。

下面看一下词典的具体内容，以词条“比较”为例，输出的字符串为： 0x7f4d934eb970 ("比较", 1664811906, 1, 1, 3937)。其节点结构如表5.11所示。

表5.11 节点结构

| 0x7f4d934eb970 | Hash节点的地址                          |
| -------------- | --------------------------------------- |
| 比较           | 词条字符串                              |
| 1664811906     | 词条字符串的hash值                      |
| 1              | 占位符——原为词条序号                    |
| 1              | 词频                                    |
| 3937           | HashMap桶内下一词的词条序号，-1为无节点 |

在分词阶段优先选取内部词典中的词汇，再对剩下的字串进行分词。内部词典定义词频默认全部为1。系统还提供外部的扩展词典文件接口：External_lexicon，在实际应用中，可以根据需要导入专名词典文件。

3．分词流程

流程说明如下。

（1）首先通过原子切分，将字符串转换为字符列表，这个阶段与ICTCLAS的分词方法相似。

（2）与内部词典（或用户导入的外部词典）进行最大匹配，根据匹配结果给出成词的特征函数。

（3）将字符串与匹配到的词典词汇共同匹配特征模板，包括CRF的标准模板和内部词典匹配结果特征模板。

（4）根据匹配的模板，查询和计算模板的概率。

（5）建立词网，使用Viterbi算法解码。

（6）输出分词结果。

自适应分词步骤如图5.10所示。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784676939-2c2c7289-f5ba-4148-b719-bd78295903ce.jpeg)

图5.10 自适应分词步骤

其中，第3～5步为标准的CRF解码过程。下面对加粗黑框中的节点展开说明。

详细给出如下几个具体的环节的执行过程。

1）最大匹配词典

方法主体逻辑并不复杂，就是将输入句子字符串中的每个字与其后的n个字试组成词(n=1, …,5)，然后与内部词典和外部词典匹配，并找出最大的匹配项。

例句：欧洲东部的罗马尼亚，首都是布加勒斯特，也是一座世界性的城市。

Ltp系统默认的内部词典的词数并不多，仅有8625个词。这个步骤仅对输入的句子进行初步的切分。但是，中文中常有多字词内含更小单位词的情况。例如，“世界性”这个词中内含词“世界”; “罗马尼亚”中内含词“罗马”，等等。这就不仅需要记录最大匹配的词，还要记录内含词的下标。LTP分词为了提高效率，下标的计算使用了位运算。这种方式有助于提高代码的运行效率。现将位运算的结果总结出可读性较强的匹配规则，便于读者进一步理解。

不同长度词的下标列表如表5.12所示。

表5.12 不同长度词的下标列表

| 词长 | 多字词的词首字下标                           | 算法公式                                                     |
| ---- | -------------------------------------------- | ------------------------------------------------------------ |
| 1    | n=0                                          | 独字词为0                                                    |
| 2    | n=2; n+1=32                                  | 首字下标为最大匹配的词长尾字下标为32                         |
| 3    | n=3; n+1=768; n+2=48                         | 首字下标为最大匹配的词长中间词下标为768尾字下标为32+16       |
| 4    | n=4; n+1=1 024; n+2=1024; n+3=64             | 首字下标为最大匹配的词长中间词下标为1 024尾字下标为32+16+16  |
| 5    | n=5; n+1=1 280; n+2=1 280; n+3=1 280; n+4=80 | 首字下标为最大匹配的词长中间词下标为1280尾字下标为32+16+16+16 |

根据表5.12，内含词情况举例如下。

首都：这是一个二字词。它的索引序列如下。

| 首   | 都   |
| ---- | ---- |
| 2    | 32   |

南斯拉夫：这是一个四字专有名词，是国家的名称。其中任何两个字都不为词。它的索引序列如下。

| 南   | 斯    | 拉    | 夫   |
| ---- | ----- | ----- | ---- |
| 4    | 1 024 | 1 024 | 64   |

世界性：这是一个三字词。但查询到“世界”为二字词。它的索引序列如下。

| 世   | 界   | 性   |
| ---- | ---- | ---- |
| 2    | 32   | 0    |
| 3    | 768  | 48   |
| 3    | 800  | 48   |

罗马尼亚：这是一个四字专有名词，也是国家的名称。但查询到“罗马”为二字词。它的索引序列如下。

| 罗   | 马    | 尼    | 亚   |
| ---- | ----- | ----- | ---- |
| 2    | 32    | 0     | 0    |
| 4    | 1 024 | 1 024 | 64   |
| 4    | 1 056 | 1 024 | 64   |

其他情况依此类推。

当下标计算完成之后，这个值会转换为内部词典的特征值。以“罗马尼亚”为例，转换结果如下。

“罗”字的内部词典特征，{lex1}=4, {lex2}=0, {lex3}=0。

“马”字的内部词典特征，{lex1}=0, {lex2}=0, {lex3}=4。

“尼”字的内部词典特征，{lex1}=0, {lex2}=0, {lex3}=4。

“亚”字的内部词典特征，{lex1}=0, {lex2}=4, {lex3}=0。

2）生成特征模板及计算概率

特征模板的生成与概率计算如表5.13所示。

表5.13 特征模板的生成与概率计算

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784677436-d4199f9d-0606-4add-b193-0ba5193cd3f2.jpeg)

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784678753-1fbb74cd-a54b-447e-ab0a-46aab0c1634c.jpeg)

表5.12中，需要注意的是：key:17=2 dict id:12 indx:6473348对应着查询到的概率。系统通过调整这个概率值来使最终的解码算法将内部词典查询到的结果从所有候选词中分离出来，作为最终的分词结果。

受篇幅所限，有关细节的概率计算等相关内容，建议读者查询Ltp 3.2中文分词源码解析文件，该文件位于http://www.threedweb.cn/thread-1396-1-1.html中，并通过调试代码得到结果进行验证。这里不再赘述。

### 5.3.4 算法的策略——层叠式架构

NLP中常用的层叠式架构包括：层叠式HMM和层叠式CRF。其中，层叠式HMM主要用于解决组织机构名识别问题。本节主要介绍层叠式HMM的算法思想和实现代码。

基于层叠式隐马尔科夫模型（Cascaded HMM, Cascaded Hidden Markov Model）的方法也是由张华平博士等提出的。其算法思想是：首先在词语粗切分的结果集上，采用底层隐马尔科夫模型识别出普通无嵌套的人名、地名和机构名等，然后依次采取高层隐马尔科夫模型识别出嵌套了人名、地名的复杂地名和组织机构名。中国科学院计算技术研究所研制的汉语词法分析系统ICTCLAS采用的就是基于层叠式隐马尔科夫模型的命名实体识别，该系统在第一届中文分词大赛中名列前茅。

在层叠式HMM的基础上，他们又提出了基于角色标注的命名实体识别方法。命名实体识别中最难的部分当属实体机构名，这是因为机构名的组成成分十分复杂，可以是人名、地名、序数词、企业字号甚至上级机构名。为了充分利用机构名构成上的特点，根据每个字词在机构名构成中的不同作用，把它们分成各个不同的角色。经过对角色集选取的反复试验，对不同的命名实体制订出不同角色表，然后再根据角色表来进行识别。

回顾第2章的HanLP中文分词，为了保证分词算法的完整性，简要介绍过人名识别的算法部分，HanLP的命名实体识别部分包括：人名识别、地名识别和组织机构名识别，从整体上就是层叠式HMM的具体实现。下面给出整个命名实体识别的总流程代码，该部分代码位于HanLP 1.28的com.hankcs.hanlp.seg.NShort类中。

​                  ……

​        if (config.ner)  // 命名实体识别

​                   {

​                      wordNetOptimum.addAll(vertexList);

​                      int preSize = wordNetOptimum.size();

​                      if (config.nameRecognize) //中国人名识别  {

​                          PersonRecognition.Recognition(vertexList, wordNetOptimum, wordNetAll);

​                      }

​                      if (config.translatedNameRecognize) //译名识别  {

​                          TranslatedPersonRecognition.Recognition(vertexList, wordNetOptimum,

​    wordNetAll);

​                      }

​                      if (config.japaneseNameRecognize) // 日本人名识别   {

​                          JapanesePersonRecognition.Recognition(vertexList, wordNetOptimum,

​    wordNetAll);

​                      }

​                      if (config.placeRecognize) //地名识别   {

​                          PlaceRecognition.Recognition(vertexList, wordNetOptimum, wordNetAll);

​                      }

​                      if (config.organizationRecognize) // 组织机构名识别   {

​                          vertexList = Dijkstra.compute(GenerateBiGraph(wordNetOptimum));

​                          wordNetOptimum.addAll(vertexList);

​                          OrganizationRecognition.Recognition(vertexList,  wordNetOptimum,

​    wordNetAll);

​                      }

​                      if (! NERexists && preSize ! = wordNetOptimum.size())

​                      {

​                          NERexists = true;

​                      }

​                   }

​        ……

本节以组织机构识别实现为例，给出命名实体识别的算法流程。该资源主要来源于HanLP 1.28项目的源码，限于篇幅原因，具体实现的部分我们做了简省。

1．定义角色词典

现给出组织机构部分的角色表（见表5.14）。该表位于com.hankcs.hanlp.corpus.tag包内，是一个枚举类型，名为enum NT。

表5.14 HanLP角色表（来自enum NT）

| 角色 | 意义               | 例子                     |
| ---- | ------------------ | ------------------------ |
| A    | 上文               | [参与]亚太经合组织的活动 |
| B    | 下文               | 中央电视台[报道]         |
| X    | 连接词             | 北京电视台[和]天津电视台 |
| C    | 特征词的一般性前缀 | 北京[电影]学院           |
| F    | 特征词的译名性前缀 | 美国[摩托罗拉]公司       |
| G    | 特征词的地名性前缀 | 交通银行[北京]分行       |
| H    | 特征词的机构名前缀 | [中共中央]顾问委员会     |
| I    | 特征词的特殊性前缀 | [中央]电视台             |
| J    | 特征词的简称性前缀 | [巴]政府                 |
| K    | 整个机构           | [麦当劳]                 |
| L    | 方位词             | —                        |
| M    | 数词               | 公交集团[五]分公司       |
| P    | 单字碎片           | —                        |
| W    | 符号               | —                        |
| D    | 机构名的特征词     | 国务院侨务[办公室]       |
| S    | 句子的开头         | —                        |
| Z    | 非机构名成分       | —                        |

生成角色词典。

下面给出从熟语料生成角色词典的过程，熟语料选自2014年的人民日报。以如下句子为例。

参与/v [北京/ns电影/n学院/nis]/nt和/cc [美国/nsf辛普森/nr公司/nis]/nt的/ude1活动/vn , /w由/p [交通/n银行/nis北京/ns分行/n]/nt与/cc麦当劳/nt赞助/v , /w [巴/b政府/nis]/nt和/cc [中共中央/nt顾问/nnt委员会/nis]/nt指导/vn, /w [中央/n电视台/nis]/nt报道/v

经过自动角色转换程序进行转化，部分转换步骤如下。

1）添加句首、句尾

[始##始/S，参与/v, [北京/ns电影/n学院/nis]/nt，和/cc, [美国/nsf辛普森/nr公司/nis]/nt，的/ude1，活动/vn, , /w，由/p, [交通/n银行/nis北京/ns分行/n]/nt，与/cc，麦当劳/nt，赞助/v, , /w, [巴/b政府/nis]/nt，和/cc, [中共中央/nt顾问/nnt委员会/nis]/nt，指导/vn, , /w, [中央/n电视台/nis]/nt，报道/v，末##末/Z]

2）标注上文

[始##始/S，参与/A, [北京/ns电影/n学院/nis]/nt，和/A, [美国/nsf辛普森/nr公司/nis]/nt，的/ude1，活动/vn, , /w，由/A, [交通/n银行/nis北京/ns分行/n]/nt，与/A，麦当劳/nt，赞助/v, , /A, [巴/b政府/nis]/nt，和/A, [中共中央/nt顾问/nnt委员会/nis]/nt，指导/vn, , /A, [中央/n电视台/nis]/nt，报道/v，末##末/Z]

3）标注下文

[始##始/S，参与/A, [北京/ns电影/n学院/nis]/nt，和/B, [美国/nsf辛普森/nr公司/nis]/nt，的/B，活动/vn, , /w，由/A, [交通/n银行/nis北京/ns分行/n]/nt，与/B，麦当劳/nt,赞助/B, , /A, [巴/b政府/nis]/nt，和/B, [中共中央/nt顾问/nnt委员会/nis]/nt，指导/B, ,/A, [中央/n电视台/nis]/nt，报道/B，末##末/Z]

4）标注中间

[始##始/S，参与/A, [北京/ns电影/n学院/nis]/nt，和/X, [美国/nsf辛普森/nr公司/nis]/nt，的/B，活动/vn, , /w，由/A, [交通/n银行/nis北京/ns分行/n]/nt，与/X，麦当劳/nt，赞助/B, , /A, [巴/b政府/nis]/nt，和/X, [中共中央/nt顾问/nnt委员会/nis]/nt，指导/B, ,/A, [中央/n电视台/nis]/nt，报道/B，末##末/Z]

5）处理整个句子

[始##始/S，参与/A，未##地/G，电影/C，学院/D，和/X，未##地/G，未##人/F，公司/D，的/B，活动/Z, , /Z，由/A，交通/C，银行/D，未##地/G，分行/D，与/X，麦当劳/K，赞助/B, , /A，巴/J，政府/D，和/X，未##团/K，顾问/C，委员会/D，指导/B, , /A，中央/C，电视台/D，报道/B，末##末/Z]

注：所有标注内容均以黑体斜体字列出。该实例参考了《层叠HMM-Viterbi角色标注模型下的机构名识别》一文。

在对所有熟语料句子执行自动标注后，即可统计每一个非Z词语的各角色词频，最后得到一个角色词典，如图5.11所示。该词典位于Hanlp-1.28\data\dictionary\organization目录下的nt.txt文件中。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784679385-8032c30f-be28-4b4a-b80b-5a5865330bc1.jpeg)

图5.11 nt.txt角色词典示例

2．统计转移矩阵

转移矩阵是指从一个角色标签转移到另一个角色的频次，利用它和角色词频可以计算出HMM中的初始概率、转移概率和发射概率，进而完成求解。2015年人民日报训练转移矩阵如表5.15所示。

表5.15 2015年人民日报训练转移矩阵

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784680452-531b83f2-3c0e-4637-9195-e6681337a576.jpeg)

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784680968-f653099a-4313-4b6d-b602-f15ae6f14c8b.jpeg)

如表5.14所示，该矩阵的文本文件位于Hanlp-1.28\data\dictionary\organization目录下的nt.tr.txt文件中。

3．识别算法

HanLP的命名实体识别代码位于com.hankcs.hanlp.seg.NShort类中，如下节选的代码为组织机构识别部分。

​                    if (config.organizationRecognize)

​                     {

​                        vertexList = Dijkstra.compute(GenerateBiGraph(wordNetOptimum));

​                        wordNetOptimum.addAll(vertexList);

​                        OrganizationRecognition.Recognition(vertexList,  wordNetOptimum,

​    wordNetAll);

​                     }

因篇幅所限，我们就不给出算法的源码解析过程。如下是《层叠HMM-Viterbi角色标注模型下的机构名识别》一文中的一个例子，对原文内容我们做了修改，便于读者领略整个算法的过程。

济南杨铭宇餐饮管理有限公司是由杨先生创办的餐饮企业

在组织机构名识别前，会得出如下输出。

[济南/ns，杨铭宇/nr，餐饮/n，管理/vn，有限公司/nis，是/vshi，由/p，杨先生/nr，创办/v，的/ude1，餐饮/n，企业/n]

上例中该公司的各个成分被拆散，无法构成完整的机构名。根据分词结果进行组织机构名的角色标注。

组织机构名角色观察如下。

[S 1162194][济南G 83472 B 1200 A 470 D 84 X 4][杨铭宇F 4309 B 769 A 266 D 254 X 6] [餐饮C 58 B 12][管理C 706 B 70 A 5][有限公司D 2861 A 1 B 1][是A 2340 B 353 X 20 P 2] [由A 1579 B 16 X 11][杨先生F 4309 B 769 A 266 D 254 X 6][创办A 20 B 5 ][的B 7092 A 4185 X 20][餐饮C 58 B 12][企业C 86 A 42 B 11 X 6][B 710]

组织机构名角色标注如下。

[ /S，济南/G，杨铭宇/F，餐饮/C，管理/C，有限公司/D，是/B，由/A，杨先生/F，创办/A，的/B，餐饮/C，企业/C , /B]

模式匹配如下。

系统提供了各类组织机构名称的角色模式串库。当上文识别出角色标注串与组织机构的模式串匹配时，就认为识别出了一个组织机构。这些模式串由组织机构的角色表中的每个角色构成，保存在com.hankcs.hanlp.dictionary.nt下的OrganizationDictionary类中。这些模式共有3 661个，数量众多，无法在这里全部列出。我们已经将其整理为一个txt模式文件，有兴趣的读者可以从http://www.threedweb.cn/thread-1590-1-1.html下载。

HanLP使用Aho-Corasick算法来进行模式匹配，Aho-Corasic是一个基于确定有限状态自动机快速模式匹配算法，算法内容详见http://www.hankcs.com/program/algorithm/implementation-and-analysis-of-aho-corasick-algorithm-in-java.html。因与本节主要内容关系不大，有兴趣的读者可以参看原文。匹配之后模式串如下。

​        CCCD

​        PPD

​        PPDCD

​        PPFCCD

​        PPFCD

​        ……

经过细分，识别出机构名：济南杨铭宇餐饮管理有限公司GFCCD。例句的命名实体输出如图5.12所示。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784681969-a00eb5f9-af37-45e1-b1a6-617354ae59a1.jpeg)

图5.12 例句的命名实体输出

需要说明的是，这仅仅是层叠式隐马尔科夫模型的最后一层。整个组织机构的识别经历了人名识别（中国人名、日本人名、其他译名）、地名识别最终完成到组织机构名称的识别。这个漫长的流程得出的最终结果如下。

[济南杨铭宇餐饮管理有限公司/nt，是/vshi，由/p，杨先生/nr，创办/v，的/ude1，餐饮/n，企业/n]
