5.2 语义组块标注

法国的著名语言学家Steven Abney最早提出了一个完整的组块（Chunk）描述体系，并给出了组块的定义。他把组块定义为句内的一个非递归的核心成分。这种成分包含核心成分的前置修饰成分，而不包含后置附属结构。同时，Abney还提出了组块解析的策略，通过引进句法块（Chunk）概念，他将句法分析问题分为如下三个阶段。

❑ 块识别：利用块识别器快速识别出句子中所有的块。

❑ 块内结构分析：对每个块内部的成分赋予合适的句法结构。

❑ 块间关系分析：利用块连接器（Attacker）将各个不同的块组合成完整的句法结构树。

这样，一方面由于对不同的子问题的准确功能定位，可以独立地选用不同的语言模型和搜索策略加以分析处理；另一方面，通过在块层次上进行自底向上的块间关系分析和自顶向下的块内结构分析，可以大大提高整体分析效率，达到降低句子分析难度的目的。

语义组块的另一个用处在于浅层语法分析，即将语义角色标注的工作建立在浅层语法分析之上，不再使用句法解析树，而是利用分析出来的语法组块直接进行语义角色标注，希望利用相对更准确的组块分析结果提升语义角色标注准确率。

最后，最常用的一个领域是，语义组块可用于知识库的实体关系抽取，我们将在第7章详细讨论语义组块在实体关系抽取中的应用。

目前，大规模的中文熟语料库（含分词、词性标注加工的中文语料库）已经具有很大规模。这为构建更高层次的语言资源建立了坚实的基础。本文所介绍的组块规范和来源均获取自美国宾州大学的中文树库。

### 5.2.1 语义组块的种类

我们对组块种类的获取来自CTB对汉语句法短语类型的分类。CTB将汉语句子中的短语分为如表5.4所示的类型。

表5.4 CTB汉语短语类别表

| 标注 | 英文说明                                      | 中文说明                       |
| ---- | --------------------------------------------- | ------------------------------ |
| ADJP | Adjective phrase                              | 形容词短语                     |
| ADVP | Adverbial phrase headed by AD (adverb)        | 由副词开头的副词短语、状语     |
| CLP  | Classifier phrase                             | 量词短语                       |
| CP   | Clause headed by C (complementizer)           | 由补语引导的补语从句、关系从句 |
| DNP  | Phrase formed by“XP+DEG”                      | XP+DEG结构构成的短语           |
| DP   | Determiner phrase                             | 限定词短语                     |
| DVP  | Phrase formed by“XP+DEV”                      | XP+DEV结构构成的短语           |
| FRAG | fragment                                      | 片段                           |
| IP   | Simple clause headed by I（INFL或其曲折成分） | 简单句                         |
| LCP  | Phrase formed by“XP+LC”                       | 处所词为中心语的短语           |
| LST  | List marker                                   | 用于解释说明性的列表标记短语   |
| NP   | Noun phrase                                   | 名词短语                       |
| PP   | Preposition phrase                            | 介词短语                       |
| PRN  | Parenthetical                                 | 插入语                         |
| QP   | Quantifier phrase                             | 数词短语                       |
| UCP  | unidentical coordination phrase               | 非一致性并列短语               |
| VP   | Verb phrase                                   | 动词短语                       |

如表5.4所示，并非所有的短语类型都能作为语义组块。其中，IP、CP为简单句和从句语法块，该组块本质上是一个完整的句子，这不符合Abney对组块的定义，因此将其删减。几乎所有的CLP都是QP的一个子集，将其与QP合并。FRAG是由若干词汇构成的一个集合，其不属于句子的范畴，不能作为组块来分析。LST是对文本中列表说明的一种标注，不属于句内的成分，也不作为组块分析。UCP是一种逻辑上并列的句子成分的标注，而不是具有语义组块的一种范畴，将其删除。

剩下的短语类型包括：ADJP、ADVP、DNP、DP、DVP、LCP、NP、PP、PRN、QP和VP这几种。5.2.2节开始对这些组块逐一详细说明。

### 5.2.2 细说NP

宾州树库中的名词短语是指中心词为名词所构成的短语，其语法功能相当于名词性成分，一般可以在句子中充当主语、宾语、定语等。

NP是NLP组块分析中最为复杂的一种结构。从语法的角度来讲，该结构具有两种含义：一种是指按句法成分构成的短语，如组块在句子中充当主语、宾语等，这种NP可以增加辅助标签NP-Sbj、NP-Obj；另一种是指知识库中的实体和属性，这种组块称为baseNP，本节主要给出的baseNP的构成。

（1）名—名复合词。连续的词性标注为NN词串构成的复合词被括号括起来作为NP。对于一个NN1 NN2…NNi序列，尽管通常说来最后一个NN总是中心语，但并非前面所有的NN都会直接修饰最后的NN。前面任意数量的NN都可能构成一个短语去修饰最后一个NN。因为很难确定谁修饰谁，所以整个序列保留扁平化，组成最低层次的NP。这种NP既可以修饰其他短语，也可以被其他短语修饰。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784655175-56070f7d-eb19-480f-866b-115f8accda29.jpeg)

（2）词级并列结构。该名词复合结构由两部分构成，左侧为并列结构的名词短语，右侧为一个名词或名—名复合词。当它们修饰另一个名词时，就被当作短语层级的修饰语。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784655790-16833657-1495-4fbd-b25f-0ba01408ca90.jpeg)

（3）由NR（专有名词）加上一个或多个NN所组成的专有名词NR+NN组成的专有名词。它包括如下两种情况。

① 组织或公司名称。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784656350-7dcdc309-7808-4d6a-9963-3b5345a38aca.jpeg)

此外，如果NR和NN没有构成专有名词，其内部结构也要进行句法分析。如果组织名称包含NR和NN外的其他成分，它们的内部结构用括号进行句法分析。

| (NP-PN (NP-PN (NR深圳)) | (NP (NP-PN (NR深) | (NP (NP-PN (NR国民党) |
| ----------------------- | ----------------- | --------------------- |
| (NP (ADJP (JJ高速))     | (NR港))           | (NN政府))             |
| (NP (NN公路)))          | (NP (NN经济)      | (NP (PU “)            |
| (NP (NN股份))           | (NN合作)          | (NN陪都)              |
| (ADJP (JJ有限))         | (NN前景)))        | (PU ”)))              |
| (NP (NN公司)))          |                   |                       |

在这种情况下，NR投射为一个NP，该NP修饰由NN组成的NP。

② 姓名+称谓。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784656844-ae0ca3c9-58e0-41e3-8709-097f83c20da4.jpeg)

注意，这与“职位+名称”不同，后者两个词是同位结构，比较如下。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784657465-286c314e-c372-4d6b-9d64-9a7b7c4493a7.jpeg)

（4）日期与地点。

① 构成日期的一连串NT被保留扁平化结构。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784658031-09fb16e3-03c6-45e8-b42b-05210d603c92.jpeg)

② 构成地点名词的一连串NR也标为NP，其内部结构也做扁平化处理。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784658555-6231e184-86e4-4216-aa7f-b140b4462e3c.jpeg)

### 5.2.3 细说VP

所谓动词短语就是以动词为中心，与其修饰、限定或并列成分共同构成的一种语义组块，除中心动词表达的行为之外，其修饰和限定成分更明确和具体化动作的语义。在句法上，它的作用与一个单独的动词差不多，在实际分析中，要把动词短语当成一个整体。这与传统句法分析的VP=VP+NP的结构有显著的不同。这里给出的VP相当于baseVP。

1．复合动词短语

复合动词是指动词与动词的复合，构成一个动词的序列。两个动词之间构成动结式机构、动趋式结构、修饰语+中心语结构、并列结构等。由于在中文里缺乏区分复合词和短语的明确标准，因此采用如下确定为复合动词的工作标准。如果一个动词序列：（1）共享相同的论元结构。（2）共享相同的体标记。（3）共享修饰语。（4）不属于明确的提升或控制结构。此处不讨论汉语中词和短语的区别问题。

如下是动词复合词的分类，并给出例子说明如何用括号方法进行句法分析。复合动词是标注员之间容易出现不一致的地方。

（1）复合动词搭配（Coordinated Verb Compounds）。两个动词之间有相同的次范畴框架，即在所出现的语境中共享论元。如果这些动词带有宾语，其标注方式可以看作如下标注方式的简略形式。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784659132-c4126664-1cef-42a5-b35e-2efbf016f366.jpeg)

更多例子：VP。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784659933-7d74f15f-ecb5-4fc9-a609-8e11e162330a.jpeg)

（2）动结式和动趋式复合词（VRD）。通常这类复合动词由两个成分构成，其中第二个成分表示第一个成分的方向结果。举例如下。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784660445-b515777f-dba6-497a-b74a-819ea9c7f82c.jpeg)

（3）“修饰语+中心语”式复合动词（VSB）。这类复合动词中，第一个成分应为不及物动词，而且两个成分之间没有附加语或体标记。举例如下。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784661026-76b0f39b-ddaf-49f1-b952-f03f24b4d704.jpeg)

（4）“VV+VC”式复合动词（VCP）。

例如，VP。

(VP (VCP (VV看作)

(VC是)))

2．动词（复合动词）+体标记/得

在宾州树库中，体标记（如“了、着、过”）不和前面的动词括在一起，动词和体标记放在同一层级。“得”的处理方式与此类似（这一点实在有点让人不理解）。举例如下。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784661541-0de158f6-bf27-4456-8d66-fc6bdb619ea8.jpeg)

如果前面是一个复合动词，该复合动词也与体标记放在同一层级。

(VCD (VV 培养) (VV 造就)) (AS 了)

(VRD (VA 紧张) (VV 起来)) (SP 了)

(VRD (VV 移交) (VV 给)) (AS 了)

3．A不A, A一A，以及变种“V不V”、“V得V”

“A不A”作为一个词层级的语类，标为VNV。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784662069-9ffcd2d7-9a23-4cbc-a02e-606a82d8fe51.jpeg)

4．前面三种类型相互组合，构成更为复杂的“复合词”

如下给出了可以组合的类型。A(i, j)表示如果前面类型为i，之后可以搭配类型j。“? ”表示结果是否可以接受有疑问。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784662596-433fcffc-869c-4540-bad5-38e47030f5a8.jpeg)

*: VCD + VCD同VCD。

**: V-ASP + V-ASP仅适用于V-过-了。

举例如下。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784663116-102f2552-ad42-40c8-879f-bb8ddaab2d5f.jpeg)

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784663585-c10adefd-b853-46e0-9243-f1cc39628961.jpeg)

5．有连接词的并列结构

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784664177-03650ae8-0339-4c4d-bea8-b6bfef2d37aa.jpeg)

举例如下。

(VV 继承) (CC 和) (VV 发扬)

(VV 解释) (CC 并) (VV 回答)

(VV 丰富) (CC 和) (VV 完善)

(VV 拓展) (CC 与) (VV 变革)

### 5.2.4 其他语义块

（1）QP：由数量词构成的短语结构。

| (QP (CD 30多) | (QP (CD 300多) | (QP (CD 5)    |
| ------------- | -------------- | ------------- |
| (CLP (M 名))) | (CLP (M 只)))  | (CLP (M 间))) |

（2）DP：限定词短语，一般用于修饰NP或限定QP，可以作为复合NP的子结构。

| (NP (DP (DT 任何)) | (NP (DP (DT 全体)) | (NP (DP (DT 这) |
| ------------------ | ------------------ | --------------- |
| (NP (NN 人)))      | (NP (NN 外交)      | (QP (CD 五)     |
|                    | (NN 人员)))        | (CLP (M 个))))  |
|                    |                    | (NP (NN 学生))) |

（3）ADJP：形容词短语，由JJ投射得到，其所修饰的名词中心语总是要先投射成一个NP。

| (NP (ADJP (JJ 若干)) | (NP (ADJP (JJ 大型)) | (NP (ADJP (ADVP (AD 不)) |
| -------------------- | -------------------- | ------------------------ |
| (NP (NN 规定)))      | (NP (NN 会议)))      | (ADJP (JJ 完全)))        |
|                      |                      | (NP (NN 统计)))          |

注意，在第一个例子中，JJ+NN组合被另一个NP修饰，而在最后一个例子中，ADVP修饰JJ, ADJP再修饰NN。

（4）DNP：由多种类型的短语加上(DEG的)构成。它们总是出现在NP的上下文中。(DEG的)除表示它前面的短语为NP的修饰语之外，没有其他作用。DNP常被看作一种复合的NP结构。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784664685-3248227d-235f-495b-8170-6d23e9ff39ec.jpeg)

（5）ADVP：副词短语，常用作动词的修饰语。

(IP (NP-PN-SBJ (NR 西门子)))

(VP (ADVP (AD 将)))

(ADVP (AD 努力))

(VP (VV 参与)

(NP-OBJ (DNP (NP-PN (NR 中国))

(DEG 的))

(NP (NP-PN (NR 三峡))

(NP (NN 工程)))

(NP (NN 建设))))))

（6）PP：介词短语。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784665040-31366534-7c21-4a94-9b77-2fdcb740a3ea.jpeg)

（7）LCP：处所词为中心语的短语，如其父短语是一个PP，一般作为PP的子结构。独立使用时，可以看作一个不完整的PP。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784665626-3bedee29-5c5d-4820-99c0-79d748f1e052.jpeg)

### 5.2.5 语义块的抽取

从本节开始，详细介绍语义组块自动识别的算法。关于语义组块识别的最常用方法是条件随机场（Conditional Random Fields, CRF）。它也是NLP序列标注中最有代表性的算法。在小规模的中文分词、词性标注、未登录词（命名实体）识别和语义组块，以及后面的语义角色标注中都有很广泛的应用。因为本节以算法介绍和应用为主，有关特征工程方面的问题暂不涉及，所以本节不讨论算法的性能问题。

使用CRF来进行语义块的识别，使用的语料一般为Penn TreeBank的CTB树库语料。前面介绍过，该语料为宾州大学开发的中文树库，标注方法为短语结构法。如果读者没有该语料，可以通过安装NLTK下载该语料，有关NLTK的安装问题，第1章相关内容已经详细说明了。

使用CRF来识别语义组块，需要通过如下三个阶段来完成。

❑ 将Penn TreeBank树库中的语料从树状结构变为序列结构。

❑ 使用CRF算法对制作好的语料进行训练，生成模型。

❑ 使用训练的结果，测试组块标注。

下面逐一给出每个过程所需要的执行步骤及相应的实现代码。将Penn TreeBank树库中的语料从树状结构变为序列结构。

（1）宾州树库的源文件格式样例与转换。

( (IP-HLN (NP-SBJ (NP-PN (NR上海)

(NR浦东))

(NP (NN开发)

(CC与)

(NN法制)

(NN建设)))

(VP (VV同步))) )

如果不对组块的定义有特殊要求，则推荐使用国外专家编写的基于Perl的小程序：ChunkLinkCTB。用户可以从https://github.com/rainarch/ChunkLinkCTB（或http://www. threedweb.cn/thread-1588-1-1.html）下载。程序需要运行在Perl环境下，如果是Windows用户则需要安装perl程序包（下载地址：http://strawberryperl.com/）。

在命令行中输入执行指令，内容如下。

​        perl chunklinkctb.pl -fhHct 源文件．mrg > 目标文件．chunk

截图如下。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784665871-525b810c-0f29-49c8-af4c-ba90f0c28ff2.jpeg)

（2）转换后的序列标注文件样例。

序列标注转换样例表如表5.5所示。

表5.5 序列标注转换样例表

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784666151-7f0d4990-a0ab-42fe-a6b1-c4638649bcf0.jpeg)

表5.5中的前三列主要是为了索引原始树库的索引记录，在序列标注阶段都没用。后三列需要根据CRF算法训练文件的要求改变一下顺序，改变后的结果如图5.2所示（因为实现程序比较简单，略）。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784666808-a812f163-2649-4ba6-a77d-dab99146940e.jpeg)

图5.2 CRF用序列标注训练语料样例表

注意，图5.2最终生成的训练样本为一个纯文本文件，字符集编码应为UTF-8格式。两列之间用空格（Space）键或Tab键（\t）隔开，行尾换行符为“\n”，句子与句子之间用空行相隔。在训练文件的末尾，最好不要留有多余的空行。

（3）序列标签说明。

ChunkLinkCTB得到的序列标签共分为两部分，用“-”分割，前面部分称为IOB表示法（也称作BIO表示法），后面部分标签对应所有的语义组块名称。

IOB表示法常用于序列标注的各种算法中。“B”代表当前词是一个组块的开始，“I”代表当前词在一个组块中，“O”代表当前词不在任意一个组块中。ChunkLinkCTB默认使用此方法进行标注。如果读者希望达到更高的精度，我们还推荐使用IOB的一个变种——start/end表示法，在中文分词中最常使用的就是这种方法。Start/end的优势是可对序列表达得更为细致，它共有5种符号：B、I、E、O和S。这5种符号表示的意义如下。

❑ B：当前词是一个组块的开始。

❑ I：当前词在一个组块内部。

❑ E：当前词是一个组块的终结。

❑ O：当前词不在任意一个组块中。

❑ S：当前词是一个组块，该组块只有一个词。

上例如转换为start/end表示法则为表5.6的形式，两种标注的转换并不复杂，有兴趣的读者可以尝试自己实现一下。

表5.6 IOB和start/end表示法对照样例表

| 词汇（Word） | 词性标签（Pos） | 序列标签（IOB） | 序列标签（start/end） |
| ------------ | --------------- | --------------- | --------------------- |
| 上海         | NR              | B-NP            | B-NP                  |
| 浦东         | NR              | I-NP            | E-NP                  |
| 开发         | NN              | B-NP            | B-NP                  |
| 与           | CC              | I-NP            | I-NP                  |
| 法制         | NN              | I-NP            | I-NP                  |
| 建设         | NN              | I-NP            | E-NP                  |
| 同步         | VV              | B-VP            | S-VP                  |

### 5.2.6 CRF的使用

下面使用CRF算法对制作好的语料进行训练，生成模型。

1．CRF的工具包介绍

CRF算法比较复杂，对训练精度和速度要求都很高，实验级别的学习和应用不建议使用基于脚本语言的工具包。目前，C/C++的实现框架很多。其中，最著名的有如下两个包。

❑ CRF++开源工具包，项目地址：https://taku910.github.io/crfpp/。该工具包用C++语言编写，算法实现了LBFSG参数估计办法，以及多线程的训练过程。它不仅训练速度快、精度高，而且代码简洁易懂，便于学习。它是开发最早、使用最广泛的软件包之一。本书使用的就是该软件包。

❑ CRFsuite，项目地址：http://www.chokkan.org/software/crfsuite/。它也实现了LBFSG参数估计办法。该工具包的优势是代码用C语言写成，源代码跨平台，兼容Linux和Windows，并与外部脚本程序结合得很好。例如，CRFsuite提供了Python的扩展，可以通过Python直接调用算法，非常适合那些不需要了解细节的外部应用。

2．CRF++的项目安装和介绍

打开CRF++项目的主页（https://taku910.github.io/crfpp/#download）, CRF++下载界面如图5.3所示。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784667164-a3060f31-7b08-4d47-8d40-9bdca4e056f7.jpeg)

图5.3 CRF++下载界面

CRF++源码下载界面如图5.4所示，目前最新版为0.58版。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784667479-efbb987c-98c7-48be-a0e6-b47200429da7.jpeg)

图5.4 CRF++源码下载界面

Windows用户可以下载编译后的动态链接库版本。因为涉及源码的调试，我们使用Linux环境下的CRF++源码，如图5.5所示。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784667885-3bb92325-be8f-4953-b3a1-124c39d3fe77.jpeg)

图5.5 CRF++源码界面

CRF++的编译和安装都很简单，不需要安装特别的库，仅需要知道最基本的Linux命令即可使用。

​        \#  ./configure

​        \#  make

​        \#  make install

​        \#  ldconfig

3．使用CRF算法对制作好的语料进行训练，生成模型

1）训练指令与可选参数

训练指令示例如下。

​        ../crf_learn -[可选参数] template train.data model

可选参数表如表5.7所示。

表5.7 可选参数表

| 参数                        | 说明                                           |
| --------------------------- | ---------------------------------------------- |
| -f, --freq=INT              | 使用属性的出现次数不少于INT（默认为1）         |
| -m, --maxiter=INT           | 设置INT为LBFGS的最大迭代次数（默认10k）        |
| -c, --cost=FLOAT            | 设置FLOAT为代价参数，过大会过度拟合（默认1.0） |
| -e, --eta=FLOAT             | 设置终止标准FLOAT（默认0.000 1）               |
| -C, --convert               | 将文本模式转为二进制模式                       |
| -t, --textmodel             | 为调试建立文本模型文件                         |
| -a, --algorithm=(CRF\|MIRA) | 选择训练算法，默认为CRF-L2                     |
| -p, --thread=INT            | 线程数（默认1），利用多个CPU减少训练时间       |
| -H, --shrinking-size=INT    | 设置INT为最适宜的迭代变量次数（默认20）        |
| -v, --version               | 显示版本号并退出                               |
| -h, --help                  | 显示帮助并退出                                 |

训练过程的时间、迭代次数等信息会输出到控制台上，如果想保存这些信息，可以将这些标准输出流输出到文件上，命令格式如下。

​        % crf_learn template_file train_file model_file >> train_info_file

所有参数都可以根据需要调整，其中主要的有如下4个参数。

​        -a CRF-L2 or CRF-L1

-a是规范化算法选择。默认是CRF-L2。一般来说L2算法效果要比L1算法稍微好一点，虽然L1算法中非零特征的数值要比L2中的小很多。

​        -c float

-c设置CRF的超参数（Hyper-Parameter）。c的数值越大，CRF拟合训练数据的程度越高。这个参数可以调整过拟合和欠拟合之间的平衡度，可以通过交叉验证等方法寻找较优值。

​        -f NUM

-f设置特征的cut-off threshold。CRF++使用训练数据中至少NUM次出现的特征。默认值为1。当使用CRF++到大规模数据时，只出现一次的特征可能会有几百万个，这个选项就会在这样的情况下起到作用。

​        -p NUM

-p NUM是线程数量，如果电脑有多核CPU，那么可以通过多线程提升训练速度。

2）示例和运行脚本

在CRF++的example目录中（路径：CRF++-0.58/example/）有几个示例项目，分别为：basenp、chunking、japaneseNE、seg。如果希望使用执行中文分词，可以参照seg下的示例。我们的任务是组块标注，参照basenp和chunking两个案例的特征文件和执行脚本都可以。两个项目的特征模板差异不大，basenp只多了一个特征。同样在example下新建一个目录：chunktest，把刚才生成的训练样本放到这个目录下，训练样本可以从http://www.threedweb.cn/thread-1589-1-1.html下载。然后从chunking目录下复制得到执行的shell脚本和模板文件，文件名分别为：exec.sh、template。

在控制台终端中执行如下指令即可进行训练。

​        % ../../crf_learn  template chtb_utf8.chunk chunkmodel

如果需要查看生成后的模型文件的文本内容，加上-t参数即可。

​        % ../../crf_learn -t template chtb_utf8.chunk chunkmodel

系统会自动生成文本格式和二进制的两个模型文件。

3）特征模板及其说明

语义组块的识别一般使用Chunking模板文件格式，文件位置在源码根目录的example/chunking/ template。

​        \# Unigram

​        U00:%x[-2,0]

​        U01:%x[-1,0]

​        U02:%x[0,0]

​        U03:%x[1,0]

​        U04:%x[2,0]

​        U05:%x[-1,0]/%x[0,0]

​        U06:%x[0,0]/%x[1,0]

​        U10:%x[-2,1]

​        U11:%x[-1,1]

​        U12:%x[0,1]

​        U13:%x[1,1]

​        U14:%x[2,1]

​        U15:%x[-2,1]/%x[-1,1]

​        U16:%x[-1,1]/%x[0,1]

​        U17:%x[0,1]/%x[1,1]

​        U18:%x[1,1]/%x[2,1]

​        U20:%x[-2,1]/%x[-1,1]/%x[0,1]

​        U21:%x[-1,1]/%x[0,1]/%x[1,1]

​        U22:%x[0,1]/%x[1,1]/%x[2,1]

​        \# Bigram

​        B

由上述可知，CRF++的模板类型有如下两种：Unigram Template，也称为一元特征模板；Bigram Template，也称为二元特征模板。下面给出两种模板的说明。

第一种是一元特征模板（Unigram Template）：模板的第一个字符是U，这是用于描述一元特征模板。当给出一个模板"U00:%x[-2,0]"时，CRF会自动生成一个特征函数集合(func1 ... funcN)，内容如下。

​        func1 = if (output = B and feature="U00:Tulsa")return 1 else return 0

​        func2 = if (output = I and feature="U00:Tulsa")return 1 else return 0

​        func3 = if (output = O and feature="U00:Tulsa")return 1 else return 0

​        ....

​        funcX1 = if (output = B and feature="U01:'s") return 1 else return 0

​        funcX2 = if (output = I and feature="U01:'s") return 1  else return 0

​        funcX3 = if (output = O and feature="U01:'s") return 1 else return 0

​        ...

一元模型生成的特征函数的个数总数为L×N。其中，L是类别标签数，N是根据给定的模板扩展出的一元特征数。它构成了一元特征函数矩阵，矩阵的行是模板扩展出的一元特征函数，列为类别标签。

已知如表5.8所示的输入数据。

表5.8 训练语料样本

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784668805-c3f1e761-0897-434e-b385-44d4969622e1.jpeg)

按照特征模板，组块的训练样本一共有三列，前两列为样本取值，最后一列为分类标签，每一行称为一个Token。模板参数也需要两列，其坐标[0,0]代表两个训练样本的两列，第一个表示词汇列，第二个表示词性列。数值0是当前位置，-1是当前词的前一个位置，1是当前词的后一个位置。模板及其对应特征说明如表5.9所示。

表5.9 模板及其对应特征说明

| 模板            | 对应特征 |
| --------------- | -------- |
| %x[0,0]         | 与       |
| %x[0,1]         | CC       |
| %x[-1,0]        | 开发     |
| %x[-2,1]        | NR       |
| %x[0,0]/%x[0,1] | 与/ CC   |
| ABC%x[0,1]123   | ABCCC123 |

第二种是二元特征模板（Bigram Template）：模板的第一个字符是B。使用这个模板，系统将自动产生当前输出标签与前一个输出标签的二元组合（Bigram）。产生的可区分的特征的总数是Lp×Lc×N。其中，Lp是前一个输出类别标签数；Lc是当前输出类别标签数（Lp=Lc）; N是这个模板产生的一元特征数。它构成了二元特征函数矩阵，构成矩阵的行和列都是类别标签，但行表示前一个类别标签（Lp），列表示当前的类别标签（Lc）。

当类别数很大时，这种类型会产生许多互不相同的特征，特征数的激增会导致训练和测试的效率都很低下。该类模板因为训练效率较低，一般很少使用。在特征模板中只使用“B”，表示二元特征矩阵是仅由当前的前一个输出Lp标签和当前输出标签Lc构成。

4）训练输出日志

​        CRF++: Yet Another CRF Tool Kit

​        Copyright (C) 2005-2013 Taku Kudo, All rights reserved.

​        reading training data:

​        Done!0.01 s

​        Number of sentences: 11

​        Number of features:  52624

​        Number of thread(s): 2

​        Freq:              1

​        eta:               0.00010

​        C:                 10.00000

​        shrinking size:     20

​        iter=0 terr=0.99506 serr=1.00000 act=52624 obj=1122.89843 diff=1.00000

​        iter=1 terr=0.32593 serr=1.00000 act=52624 obj=881.06755 diff=0.21536

​        iter=2 terr=0.22716 serr=1.00000 act=52624 obj=247.73335 diff=0.71883

​        …

​        iter=23 terr=0.00000 serr=0.00000 act=52624 obj=18.64905 diff=0.00017

​        iter=24 terr=0.00000 serr=0.00000 act=52624 obj=18.64876 diff=0.00002

​        iter=25 terr=0.00000 serr=0.00000 act=52624 obj=18.64738 diff=0.00007

​        iter=26 terr=0.00000 serr=0.00000 act=52624 obj=18.64715 diff=0.00001

​        Done!0.70 s

说明如下。

❑ iter：迭代次数。

❑ terr：标签（tag）错误率。

❑ serr：句子（sentence）错误率。

❑ obj：当前对象值，该值收敛于一个固定的值则停止迭代。

❑ diff：与上次对象值的相对差异。

5）文本模型文件说明

最后，给出训练完成后，生成文本形式的模型文件说明，内容如下。

❑ 文件头。

| version: 100   | 左侧为样本输出的文件头。                  |
| -------------- | ----------------------------------------- |
| cost-factor: 1 | version：模型的版本。                     |
| maxid: 52224   | cost-factor通过-c参数指定，这里是默认值。 |
| xsize: 2       | maxid:CRF矩阵的元素总数量。               |
|                | xsize是特征维数，也就是训练语料列数-1。   |

❑ 标签和模板。

接下来的两块为训练样本中使用的标签和模板，模板前文已经讲过，这里就不重复了。仅给出统计出的全部标签：B-ADJP、B-ADVP、B-DNP、B-DP、B-DVP、B-LCP、B-NP、B-PP、B-QP、B-VP、I-ADVP、I-DP、I-NP、I-QP、I-VP和O。从训练集中得到的标签数一共为16个。

❑ 特征函数和权值。

截取的部分特征函数列表，内容如下。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784669427-240b5807-96fb-404d-9f4a-ff07aa6c933f.jpeg)

➢ “_B-1”表示句子第一个单词前面的一个单词，_B+1表示末尾后面的一个单词，依次类推。

➢ “U00：上海”：冒号前为一元特征编码，冒号后为一元特征对应的词汇。通过模板和训练语料学习出的全部一元特征函数一共有3 249个，即3 249行。

➢ 一元特征前面的序号，如“U00：上海”前的序号为592。因为标签总数为16个，592除以16得到结果为37，表示“U00：上海”位于一元特征函数矩阵的第37行。

➢ 一元特征函数矩阵的总元素数为：3 249×16=51 984

截取的部分矩阵权值列表，内容如下。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784670438-826c1b21-dc8c-41e5-b648-70053d6c09cd.jpeg)

➢ 计算得到的矩阵中每个元素对应权值。矩阵包括一元特征函数矩阵和二元特征函数矩阵。

➢ 由于对矩阵中的每个元素做了顺序存储，所以两个矩阵都表示为列的形式。

➢ 它包括一元特征函数矩阵和二元特征函数矩阵的全部元素值。

二元特征矩阵的总元素数为：16×15=240。

全部元素共有52 224个，也就是maxid的值。

4．测试训练结果

执行命令行如下。

​        % crf_test -m chunkmodel test

测试后的训练样本如图5.6所示。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784671112-986d40f9-e345-4b06-859e-39086d4a1084.jpeg)

图5.6 测试后的训练样本

读者可以根据训练文件，自己编写一个测试文件来验证执行结果。这里不再演示。
