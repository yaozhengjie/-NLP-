9.4 深度学习框架与应用

目前，越来越多的深度学习开源框架在网络上发布出来。下面给出如下几个有代表性的框架帮助读者学习。

❑ Caffe。该项目源自加州伯克利分校的贾扬清，Caffe由C++写成，由于其高效的运行效率，发布之后很快被广泛应用，包括Pinterest这样的Web大户。目前Caffe主要用于卷积神经网络的学习，在机器视觉领域仍旧是应用最广泛的深度学习框架。

❑ Torch。Torch诞生已经有十年之久，是易于使用且高效的计算框架，由于Facebook开源了大量Torch的深度学习模块和扩展，使Torch很快广泛地发展起来，成为受欢迎的深度学习计算框架之一。另外一个特殊之处是采用了不怎么流行的编程语言Lua。

❑ Tensorflow。2015年11月9日，Google官方称，Google Research宣布开源了第二代机器学习系统TensorFlow，它也是用C++语言开发的，前端脚本使用Python，并针对先前的DistBelief的短板有了各方面的加强。与前面的框架不同，TensorFlow不仅实现了传统的深度学习算法，而且不断提出和发布新的、有价值的算法。现在，Tensorflow越来越成为深度学习发展的前沿。

### 9.4.1 Keras框架介绍

1．Keras介绍

Keras（https://keras.io/）是一个基于Theano或TensorFlow作为后端的深度学习框架，它的设计参考了Torch，用Python语言编写，是一个高度模块化的神经网络库。因此，在结构上是极度简化、便于设计的深度学习第三方库。基于Python整合多种后端开发，充分发挥了GPU和CPU运算能力。其开发目的是利用快速原型的方法设计基于深度学习的各种实验。适合前期的网络原型设计、支持卷积网络和递归网络及两者整合的结果、支持任意的连接范式、能够在GPU和CPU上无缝连接运行。

Keras的源代码可以从https://github.com/fchollet/keras下载。源代码结构简单，便于阅读，感兴趣的读者可以从源码学习各种深度学习网络的实现细节。

指导原则如下。

❑ 模块化。模型被理解为一个序列或独立的图等完全可配置的模块，并使用尽可能少的限制被插到一起。特别是神经网络层、代价函数、优化方法、初始化方案，激活函数，正规化的方案都是独立的模块，它们可以结合起来构造新的模型。

❑ 极简主义。每个模块的设置尽量简短。每一段代码在第一次读应该是透明的。没有黑魔法，不会伤害迭代速度和创新能力。

❑ 易扩展性。新的模块添加起来极其简单（如新类和函数），并且现有模块已经提供了充足的例子。能够轻松地创建新的模块提升总体性能，使得Keras适用于最先进的研究任务。

❑ 基于Python的编码。没有以声明形式提供的独立的模型配置文件。模型使用Python代码来描述，其结构紧凑、易于调试，并允许扩展。

2．Keras的安装与结构

由于Keras不是独立的深度学习（DL）算法包，其安装必须依赖于后端系统的安装，特别是对GPU的支持，因此其安装并非那么简单。在Windows系统下，需要事先安装Theano开发包，并使Theano支持GPU运算；在Linux系统下，可以选择Theano和TensorFlow两种后端，当然每种后端都要支持GPU运算。

本章重点讲解Windows下的Keras安装。关于Theano的安装建议读者参考Theano官方网站的安装教程，网址是http://deeplearning.net/software/theano/install.html。网页内包含Windows下的安装介绍，更多的细节建议参考《机器学习算法原理与编程实践》中第10章中的更多细节。

这里需要说明的是，目前CUDA 7.5最高仅支持Visual Studio 2013，如果你使用的是更高的版本，必须降级（Downgrade）到Visual Studio 2013或更低的版本上。

Keras提供了一套完整的深度学习网络的实现。该实现主要包括一套完整的参数和函数API。Keras提供了一套完整的教程文档，从文档中我们了解到Keras主要包含如下几部分内容。

Models（模型）。这是Keras最主要的模块。目前比较流行的模型是序列化模型，文档内对序列化模型给出了讲解，用户都是从操作模型来实现网络架构的设计的，但在操作这一层之前，需要对网络的各个细节有充分的了解。后面定义了各种基本组件，都通过模型层将它们组合起来。

Layers（层）。模块是由不同类型的神经网络层构成的，Layers模块包含Core、Convolutional、Recurrent、Advanced_activations、Normalization、Embeddings这几种。对于NLP而言，Core和Recurrent循环层是我们关注的重点。在Core中包含如下一系列层的种类。

❑ Flatten层。CNN的全连接层之前需要把二维特征图Flatten成为一维的。

❑ Reshape。CNN输入时将一维的向量变成二维的，相当于Flattern的逆操作。

❑ Dense。就是网络中的隐含层（英文Dense是稠密的意思）。

❑ Convolutional。Convolutional层基本就是Theano的Convolution2D的封装。

Optimizers（优化算法）。Optimizers包含了一些优化的方法，比如最基本的随机梯度下降SGD。另外，还有Adagrad、Adadelta、RMSprop、Adam，一些新的方法以后也会被不断添加进来。

Objectives（目标函数）。这是神经网络的目标函数模块，Keras提供了Mean_squared_error、Mean_absolute_error、Squared_hinge、Hinge、Binary_crossentropy、Categorical_crossentropy这几种目标函数。

Activations（激活函数）。Keras提供了Linear、Sigmoid、Hard_sigmoid、Tanh、Softplus、Relu、Softplus，另外，Softmax也放在Activations模块中（笔者觉得放在Layers模块中更合理些）。此外，像LeakyReLU和PReLU这种比较新的激活函数，Keras也提供。

Initializations（初始化方法）。这是估计参数初始化模块，在添加Layer的时候调用Init进行初始化。Keras提供了Uniform、Lecun_uniform、Normal、Orthogonal、Zero、Glorot_normal、He_normal这几种。

Preprocessing（预处理）。这是预处理模块，包括序列数据的处理、文本数据的处理和图像数据的处理等。

有关上述模块的细节，后面会提供一个实例进行讲解。

3．使用Keras设计网络架构

使用Keras框架来实现深度学习的快速原型，原因有很多。其中一个主要原因是Keras在教程文档中提供了多种网络框架以供用户选择，用户可以以现有的框架为基础，根据自己的专门用途设计出新的网络原型。而构建各种网络原形的代码量很少、设计速度很快，便于将专有的网络框架快速应用于自身NLP任务实验中。两种常用的网络架构如下。

（1）堆叠的LSTM网络架构。

有状态循环模型（LSTM）将前一批样本生成的内部状态（记忆）保存，在下一批重用。它能处理更长的序列，并降低计算复杂性。

在图9.25所示的模型中，堆叠了3个LSTM层，使模型有能力进行更高层的时序表达。前两个LSTMs返回了全部的输出序列，但最后一个仅仅返回了其最后一步的输出序列。这样，丢弃了时间的维度（这就将输入序列转换为一个独立的向量）。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784829787-4e9eeb53-0088-44c5-9da4-29a157c06a26.jpeg)

图9.25 堆叠的神经网络架构

（2）LSTM层的合并网络架构。

如图9.26所示，在该模型中，两个输入序列由两个独立的LSTM模块被编码成向量。然后这两个向量串联起来，一个全连接的网络位于串联表达的顶层。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784832391-29eb09bd-ca46-4b0f-a726-a30aaa2c8bbb.jpeg)

图9.26 合并网络架构

### 9.4.2 Keras序列标注

下面以一个中文分词的例子来展示Keras序列标注的过程。

1．必须导入模块包和语料库文件

​        \# -＊- coding: utf-8 -＊-

​        import os, sys

​        import numpy as np

​        from numpy import ＊

​        import nltk

​        import codecs

​        import pandas as pd

​        from nltk.probability import FreqDist

​        from gensim.models import word2vec

​        from  cPickle import load, dump

​        from keras.preprocessing import sequence

​        from keras.optimizers import SGD, RMSprop, Adagrad

​        from keras.utils import np_utils

​        from keras.models import Sequential, Graph

​        from keras.layers.core import Dense, Dropout, Activation, TimeDistributedDense

​        from keras.layers.embeddings import Embedding

​        from keras.layers.recurrent import LSTM, GRU, SimpleRNN

​        from keras.layers.core import Reshape, Flatten , Dropout

​        from keras.regularizers import l1, l2

​        from keras.layers.convolutional import Convolution2D, MaxPooling2D, MaxPooling1D

​        from sklearn.cross_validation import train_test_split

上述为全部需要导入的包，将这些包放到一个称为seqlib.py的文件中，执行文件在执行过程中根据需要自行引入。受篇幅所限，这里对执行程序的需要包不另作说明，均加以忽略。

下文中的必要函数都和神经网络模型类位于seqlib.py中。

训练语料库使用的是在第7章中介绍过的微软研究院的中文分词语料库：msr.utf8.txt。

2．使用分词语料库生成词向量

（1）必要的函数：seqlib.py。

​        def load_file(input_file):  # 读单个文本

​            input_data = codecs.open(input_file, 'r', 'utf-8')

​            input_text = input_data.read()

​            return input_text

​        \# 我们使用了gensim的word2vec库

​        def trainW2V(corpus, epochs=20, num_features = 100, sg=1, \   # word2vec建模

​                min_word_count = 1, num_workers = 4, \

​                context = 4, sample = 1e-5, negative = 5):

​            w2v  =  word2vec.Word2Vec(workers  =  num_workers, sample  =  sample,  size  =

​    num_features, min_count=min_word_count, window = context)

​            np.random.shuffle(corpus)

​            w2v.build_vocab(corpus)

​            for epoch in range(epochs):

​                print('epoch' + str(epoch))

​                np.random.shuffle(corpus)

​                w2v.train(corpus)

​                w2v.alpha ＊= 0.9

​                w2v.min_alpha = w2v.alpha

​            print("word2vec DONE.")

​            return w2v

（2）执行程序：corpus2vector.py。

​      \# -＊- coding: utf-8 -＊-

​      from seqlib import ＊

​      reload(sys) # 设置 UTF-8输出环境

​      sys.setdefaultencoding('utf-8')

​      corpuspath = "msr.utf8.txt"

​      input_text = load_file(corpuspath)

​      \# word2vec 是一个二维数组

​      txtwv = [line.split() for line in input_text.split('\n') if line ! = '']

​      \# word2vec

​      w2v = trainW2V(txtwv)

​      w2v.save("wordvector.bin")

生成词向量二进制文件：wordvector.bin。

3．语料预处理

（1）必要的函数：seqlib.py。

​      def freq_func(input_txt):      # nltk  输入文本，输出词频表

​          corpus = nltk.Text(input_txt)

​          fdist = FreqDist(corpus)

​        w = fdist.keys()

​        v = fdist.values()

​        freqdf = pd.DataFrame({'word':w, 'freq':v})

​        freqdf.sort('freq', ascending =False, inplace=True)

​        freqdf['idx'] = np.arange(len(v))

​        return freqdf

​    def initweightlist(w2v, idx2word, word2idx):    # 初始化权重

​        init_weight_wv = []

​        for i in range(len(idx2word)):

​            init_weight_wv.append(w2v[idx2word[i]])

​        \# 定义’U’为未登录新字， 'P’为两头padding用途，并增加两个相应的向量表示

​        char_num = len(init_weight_wv)

​        idx2word[char_num] = u'U'

​        word2idx[u'U'] = char_num

​        idx2word[char_num+1] = u'P'

​        word2idx[u'P'] = char_num+1

​        init_weight_wv.append(np.random.randn(100, ))

​        init_weight_wv.append(np.zeros(100, ))

​        return init_weight_wv , idx2word, word2idx

​    def character_tagging(input_file, output_file): # 加入标注标签：SBME

​        input_data = codecs.open(input_file, 'r', 'utf-8')

​        output_data = codecs.open(output_file, 'w', 'utf-8')

​        for line in input_data.readlines():

​            word_list = line.strip().split()

​            for word in word_list:

​                  if len(word) == 1:

​                      output_data.write(word + "/S ")

​                  else:

​                      output_data.write(word[0] + "/B ")

​                      for w in word[1:len(word)-1]:

​                          output_data.write(w + "/M ")

​                      output_data.write(word[len(word)-1] + "/E ")

​            output_data.write("\n")

​        input_data.close()

​        output_data.close()

​    def featContext(sentence, word2idx = '', context = 7):

​        predict_word_num = []

​        for w in sentence:         # 文本中的字如果在词典中则转为数字，如果不在则设置为’U

​            if w in word2idx:

​                  predict_word_num.append(word2idx[w])

​              else:

​                  predict_word_num.append(word2idx[u'U'])

​        num = len(predict_word_num)  # 首尾padding

​        pad = int((context-1)＊0.5)

​        for i in range(pad):

​              predict_word_num.insert(0, word2idx[u'P'] )

​              predict_word_num.append(word2idx[u'P'] )

​        train_x = []

​        for i in range(num):

​              train_x.append(predict_word_num[i:i+context])

​        return train_x

（2）执行程序：preprocess.py。

​    from seqlib import ＊

​    reload(sys) # 设置 UTF-8输出环境

​    sys.setdefaultencoding('utf-8')

​    corpuspath = "msr.utf8.txt"

​    input_text = load_file(corpuspath)

​    \# 计算词频

​    txtnltk = [w for w in input_text.split()]   # 为计算词频准备的文本格式

​    freqdf = freq_func(txtnltk) # 计算词频表

​    \#  建立两个映射词典

​    word2idx = dict((c, i) for c, i in zip(freqdf.word, freqdf.idx))

​    idx2word = dict((i, c) for c, i in zip(freqdf.word, freqdf.idx))

​    w2v = word2vec.Word2Vec.load("wordvector.bin")

​    \# 初始化向量

​    init_weight_wv, idx2word, word2idx = initweightlist(w2v, idx2word, word2idx)

​    dump(word2idx, open('word2idx.pickle', 'wb'))

​    dump(idx2word, open('idx2word.pickle', 'wb'))

​    dump(init_weight_wv, open('init_weight_wv.pickle', 'wb'))

​    \# 读取数据，将格式进行转换为带4种标签 S B M E

​    output_file = 'msr.tagging.utf8'

​    character_tagging(corpuspath, output_file)

​    \# 分离word 和 label

​        with open(output_file) as f:

​            lines = f.readlines()

​            train_line = [[w[0] for w in line.decode('utf-8').split()] for line in lines]

​            train_label = [w[2] for line in lines for w in line.decode('utf-8').split()]

​        \# 将所有训练文本转成数字list

​        train_word_num = []

​        for line in train_line:

​            train_word_num.extend(featContext(line, word2idx))

​        \# 持久化

​        dump(train_word_num, open('train_word_num.pickle', 'wb'))

​        dump(train_label, open('train_label.pickle', 'wb'))

执行结果如下。

词与索引映射文件：word2idx.pickle、idx2word.pickle。

初始化权重向量：init_weight_wv.pickle。

使用BMES标签标注后的分词语料：msr.tagging.utf8。

使用Word2Vec转换后的词向量和索引训练文件：train_word_num.pickle、train_label.pickle

4．训练语料

（1）必要的类：seqlib.py。

​        class Lstm_Net(object): # lstm神经网络

​            def __init__(self):

​                self.init_weight = []

​                self.batch_size = 128

​                self.word_dim = 100

​                self.maxlen = 7

​                self.hidden_units = 100

​                self.nb_classes = 0

​            def buildnet(self):

​                self.maxfeatures = self.init_weight[0].shape[0] # 词典大小

​                self.model = Sequential()

​                print 'stacking  LSTM...' # 使用了堆叠的LSTM架构

​                self.model.add(Embedding(self.maxfeatures, self.word_dim, input_length=

​    self.maxlen))

​                self.model.add(LSTM(output_dim=self.hidden_units, return_sequences =True))

​                self.model.add(LSTM(output_dim=self.hidden_units, return_sequences =False))

​                self.model.add(Dropout(0.5))

​                self.model.add(Dense(self.nb_classes))

​                self.model.add(Activation('softmax'))

​                self.model.compile(loss='categorical_crossentropy', optimizer='adam')

​            def train(self, modelname):

​                result = self.model.fit(self.train_X, self.Y_train, batch_size=self.

​    batch_size, nb_epoch=20, validation_data = (self.test_X, self.Y_test), show_accuracy=

​    True)

​                self.model.save_weights(modelname)

​            def splitset(self, train_word_num, train_label, train_size=0.9, random_state=1):

​                self.train_X, self.test_X, train_y, test_y = train_test_split(train_

​    word_num, train_label, train_size=0.9, random_state=1)

​                self.Y_train = np_utils.to_categorical(train_y, self.nb_classes)

​                self.Y_test = np_utils.to_categorical(test_y, self.nb_classes)

​            def  predict_num(self, input_num, input_txt,  label_dict='',  num_dict=''):

​    \#根据输入得到标注推断

​                input_num = np.array(input_num)

​                predict_prob = self.model.predict_proba(input_num, verbose=False)

​                predict_lable = self.model.predict_classes(input_num, verbose=False)

​                for i , lable in enumerate(predict_lable[:-1]):

​                    if i==0: # 如果是首字 ，不可为E, M

​                          predict_prob[i, label_dict[u'E']] = 0

​                          predict_prob[i, label_dict[u'M']] = 0

​                    if lable == label_dict[u'B']: # 前字为B，后字不可为B, S

​                          predict_prob[i+1, label_dict[u'B']] = 0

​                          predict_prob[i+1, label_dict[u'S']] = 0

​                    if lable == label_dict[u'E']: # 前字为E，后字不可为M, E

​                          predict_prob[i+1, label_dict[u'M']] = 0

​                          predict_prob[i+1, label_dict[u'E']] = 0

​                    if lable == label_dict[u'M']: # 前字为M，后字不可为B, S

​                          predict_prob[i+1, label_dict[u'B']] = 0

​                          predict_prob[i+1, label_dict[u'S']] = 0

​                    if lable == label_dict[u'S']: # 前字为S，后字不可为M, E

​                          predict_prob[i+1, label_dict[u'M']] = 0

​                          predict_prob[i+1, label_dict[u'E']] = 0

​                    predict_lable[i+1] = predict_prob[i+1].argmax()

​                predict_lable_new = [num_dict[x]  for x in predict_lable]

​                result =  [w+'/' +l  for w, l in zip(input_txt, predict_lable_new)]

​              return ' '.join(result) + '\n'

​          def getweights(self, wfname):

​              return self.model.load_weights(wfname)

（2）执行训练：segment_lstm.py。

​    from seqlib import ＊

​    reload(sys) # 设置 UTF-8输出环境

​    sys.setdefaultencoding('utf-8')

​    train_word_num = load(open('train_word_num.pickle', 'rb'))

​    train_label = load(open('train_label.pickle', 'rb'))

​    nb_classes = len(np.unique(train_label))

​    \# 初始字向量格式准备

​    init_weight_wv = load(open('init_weight_wv.pickle', 'rb'))

​    \# 建立两个词典

​    label_dict = dict(zip(np.unique(train_label), range(4)))

​    num_dict = {n:l  for l, n  in label_dict.iteritems()}

​    \# 将目标变量转为数字

​    train_label = [ label_dict[y] for y in train_label ]

​    \# print shape(train_label)

​    train_word_num = np.array(train_word_num)

​    \# print shape(train_word_num)

​    \# stacking LSTM

​    modelname = 'my_model_weights.h5'

​    net = Lstm_Net()

​    net.init_weight = [np.array(init_weight_wv)]

​    net.nb_classes = nb_classes

​    net.splitset(train_word_num, train_label)

​    print "Train..."

​    net.buildnet()

​    net.train(modelname)

训练结果如下。

​    my_model_weights.h5

5．执行分词

​    from seqlib import ＊

​        reload(sys) # 设置 UTF-8输出环境

​        sys.setdefaultencoding('utf-8')

​        word2idx = load(open('word2idx.pickle', 'rb'))

​        train_word_num = load(open('train_word_num.pickle', 'rb'))

​        train_label = load(open('train_label.pickle', 'rb'))

​        nb_classes = len(np.unique(train_label))

​        init_weight_wv = load(open('init_weight_wv.pickle', 'rb'))

​        \# 建立两个词典

​        label_dict = dict(zip(np.unique(train_label), range(4)))

​        num_dict = {n:l  for l, n  in label_dict.iteritems()}

​        temp_txt = u’罗马尼亚的首都是布加勒斯特。'

​        temp_txt = list(temp_txt)

​        temp_num = featContext(temp_txt, word2idx = word2idx)

​        net = Lstm_Net()

​        net.init_weight = [np.array(init_weight_wv)]

​        net.nb_classes = nb_classes

​        net.buildnet()

​        net.getweights('my_model_weights.h5')

​        temp = net.predict_num(temp_num, temp_txt, label_dict=label_dict, num_dict=num_dict)

​        print(temp)

执行结果如下。

​        罗/B 马/M 尼/M 亚/E 的/S 首/B 都/E 是/S 布/B 加/M 勒/M 斯/M 特/E 。/S

执行结果为标注后的文本，文本的标注规则符合BMES规则，分词后的结果如下。

​        罗马尼亚| 的 |首都| 是 |布加勒斯特| 。

### 9.4.3 依存句法的算法原理

第6章讨论了两种主要的句法解析方法：一种是基于乔姆斯基的短语结构句法，另一种就是依存句法，但仅详细讲解了短语结构句法的算法原理。由于最新的依存句法分析用到了深度学习的思想，因此，本节对依存句法的算法原理做出详细的分析。

本节使用基于深度学习的神经网络解析器来学习这个模型。首先给出神经网络模型及其构成组件，9.4.4节给出语料处理和训练过程的细节。

1．深度学习模型

图9.27展示了基于深度学习的依存句法解析的神经网络结构，首先通过词向量表达，将每个词转换为d维向量，![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784832875-90162aa4-0953-4c45-aa8a-9ffbc4f3c89d.jpeg)。全部词嵌入的向量矩阵为![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784833807-16995fa4-e124-45d9-b09c-7fd1313a47b8.jpeg)。其中，Nw是词典的大小。然后，模型还映射了一个词性标签和一个弧标签到一个d维的向量空间，其中，![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784834514-7fd32256-bbbb-4d9a-aff6-4ab391bd37bd.jpeg)表示第i个词性标签和第j个弧标签。相应的，词性的嵌入矩阵为![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784835751-29940ea1-22e7-4089-93bb-defc87be13cc.jpeg)；弧标签的嵌入矩阵为![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784836562-59c71856-c5a3-4ab3-b479-97fab35b6230.jpeg)，这里Nt是词性标签的数量，Nl是弧标签的数量。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784838560-5aef579f-e736-4d04-80ac-959492380b22.jpeg)

图9.27 依存解析的神经网络架构

网络模型是一个具有一个输入层、一个隐含层和一个输出层的LSTM神经网络，输入层由Sw、St和Sl三部分共同构成，符号nw、nt、nl表示每种输入类型的长度。这样，加入![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784839767-3ede26d5-ac7d-4906-af32-98cf0047da7d.jpeg)]到输入层，其中，Sw={w1, …, wnw}，与之相似，词性标签特征xt和弧标注特征xl也被加入到层中。

在输入层到隐含层的映射阶段，隐含层是一个d维节点，激活函数是cube函数。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784841304-82e6ec0e-d5fc-4846-ab18-9bf6601b6073.jpeg)

其中，![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784843191-08bfef45-5ce7-4413-9f94-3c8c5adad392.jpeg)是权重，![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784843914-63dac1f4-d2b8-44b3-9fed-0c2026af1d63.jpeg)是偏置项。

在隐含层顶部使用softmax函数进行分类。该函数用于生成多类的概率![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784844311-08d2de0e-d4d1-4ef0-bd69-23054d675910.jpeg)![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784845297-e535cdf1-89b9-49a2-baa5-9e99adbb1553.jpeg)。

2．特征组合

1）基于转移的识别过程

深度学习依存解析器在神经网络中嵌入了词性标记和弧标签。虽然词性标注p={NN,NNP, NNS, DT, JJ, …}和弧标签L={amod, tmod, nsubj, csubj, dobj, …}都是离散的集合，但它们如同词汇一样都展示出很大的语义相似性。例如，NN（单数名词）与NNS（复数名词）比DT（限定词）要更接近，或者amod（形容词修饰成分）与num（数量词修饰成分）应该比nsubj（名词性主语）更接近。我们希望这些词性和弧的语义特征对获取最终的输出是有效的。

句法解析的识别过程，称为基于转移（Transition-Based）的依存句法识别，基本上都采用了弧标准体系（Arc-Standard System），这是最流行的一种转换系统。在弧标准体系中使用了一个特征组合c = (s, b, A)。它由一个栈s、一个缓冲区b和一套依存弧A构成。一个句子（w1, …, wn）在转换前的起始结构被表示为s=[ROOT], b=[w1, …, wn], A=∅。如果缓冲区b为空，栈仅包括单一的节点ROOT，而解析树被Ac给出，则特征组合c表示终结符号。

用si(i=1, 2, …)表示栈中第i个元素，bi(i = 1, 2, …)表示缓冲中第i个元素，弧标准体系定义了如下三种类型的转换。

❑ LEFT-ARC(l)（左边弧）。加入一条带有l的弧s1→s2，从栈中移走s2。前提：|s|≥2。

❑ RIGHT-ARC(l)（左边弧）。加入一条带有l的弧s2→s1，从栈中移走s1。前提：|s|≥2。

❑ SHIFT（转换）。从缓冲移动b1到栈。前提：|b|≥1。

在解析器标注过程中，转换次数共有|T|=2Nl+1次。其中，Nl是大量的互不相同的弧标签。表9.5展示了基于转移算法的特征模板。

表9.5 依存树的转换特征

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784846957-b7dd1d36-f778-4c6c-8b7b-1e0529b65c24.jpeg)

表9.5展示了一个用于分析弧lc1(si)和rc1(si)的特征模板，lc1(si)和rc1(si)表示si的最左和最右的孩子节点，w表示词，t表示词性标签。

下面给出一个例子，显示出从初始的特征组合到转换终止的过程，如图9.28所示。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784847399-263da278-ec35-4db9-861a-5249d83986df.jpeg)

图9.28 基于转移的依存解析的例子

图9.28左图所示为依存树的最终结果，图9.28右图所示为中间状态，弧标准系统的转换序列。

2）Sw、St、Sl的选择

深度学习依存解析的特征模板是一组基于“栈/缓冲区（Stack / Buffer）”位置的元素，包括词汇、词性和弧标签的信息。这些信息对预测是非常有用的。解析器用Sw、St、Sl三个符号来分别表示词汇、词性和弧标签的信息。在网络模型中，依存解析器使用了庞大的标签集：Sw的nw中包含了18个元素。

（1）在栈和缓冲区上的前三个词：s1、s2、s3、b1、b2、b3。

（2）在栈顶前两个词的第一个和第二个最右侧/最左侧的孩子节点：lc1(si), rc1(si), lc2(si), rc2(si), i=1,2。在栈顶前两个词的最右侧/最左侧的孩子节点的最右侧/最左侧的孩子节点：lc1(lc1(si) ), rc1(rc1(si) ), i = 1,2。

（3）与之相一致，词性标签也是18个，弧标签使用了12个（需要排除栈/缓冲区的6个词）。

该解析器的一个优势在于，可以很容易地添加一组丰富标签元素，而无须手工编写更多的特征函数（指示函数）。

例如，上述给定St={lc1(s2)﹒t, s2﹒t, rc1(s2)﹒t, s1﹒t}的特征组合中，从语料中按顺序抽取出PRP、VBZ、NULL、JJ。这里使用一个特殊的标签NULL代表不存在的元素。

3）Cube激活函数

神经网络中使用的不同激活函数如图9.29所示。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784848667-298888d4-9157-4fd8-9c58-02ec99ffd78d.jpeg)

图9.29 神经网络中使用的不同激活函数

深度学习的依存解析模型中引入一个新的激活函数：Cube g(x)=x3，以代替常用的Tanh和Sigmoid函数。直觉上，使用g(x)=x3，能为像xi、xj、xk这样的三个不同元素的连乘积形式建立更好的模型。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784850234-2dc2d892-0b81-4abe-a23e-5a9d76119ef2.jpeg)

在此模型中，xi、xj、xk是具有不同维度的三个嵌入结构，三个元素的相互作用是依存分析的非常想要得到的性质。实验结果也验证了Cube激活函数的成功。然而，这种激活函数的表现力仍然处于理论上的调查阶段。

3．损失函数与训练

通过依存树库的语料集中生成训练样本![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784851629-a66a573d-4c6b-4173-a5c3-ff719c74e64e.jpeg)。其中，ci是一个特征组合，ti∈T是转换次数。最终的训练目标是最小化交叉熵损失加上一个L2-正则项。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784853957-43e1cc8e-6225-4547-9d7e-5d98ed05a75f.jpeg)

这里θ代表了所有参数![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784855902-d7023337-3097-4953-852b-84ee704d9568.jpeg)。一个细微变化是，算法只在实际可行的转换间计算了softmax的概率。

算法使用预训练的词向量来初始化参数Ew，词向量的维度为50维，该词向量来自中文维基百科的语料，使用（-0.01,0.01）随机数来初始化Et和El。

在训练阶段，训练误差的导数将被反向传播到嵌入向量中，算法使用了最小批次AdaGrad（mini-batched AdaGrad）进行优化，丢包率设置为0.5。

在训练中，还使用了一个开发集进行验证，设置上述参数的网络取得了最优的得分，因此上述参数被定为最终的结果。表9.6给出了算法的最终训练结果。

表9.6 CTB语料上的解析精度

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784857608-01ce9516-6252-4cdb-9b29-d9564057120d.jpeg)

### 9.4.4 Stanford依存解析的训练过程

在充分了解依存句法解析器的原理之后，本节继续通过一个实例来介绍Stanford依存句法解析器的训练过程。通过这个过程，读者可以使用自己的数据训练一个新的解析器，数据使用CoNLL-X版本的数据格式（有关此种数据格式的细节，第6章已经介绍过）。

不幸的是，这个过程并不简单，整个训练过程占用大量的CPU资源，耗时很长，而且需要读者已经具有支持Stanford标注规范的汉语树库资源。我们使用的是宾州短语结构树库（CTB 8.0）。整个树库已经被加载到数据库中。读者可以从自己的数据库中导出该树库。导出的文件格式如图9.30所示。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784859531-ce7445c8-197a-4943-93e8-01e07f54921d.jpeg)

图9.30 导出的文件格式

为了便于后续的程序处理，树库中短语间的换行符都被删掉了，整个句法树变为单独的一行。这样，一行就代表了一个完整的句子。在下面的训练过程中，假设读者已经拥有了上述结构的宾州短语结构树库。这样，整个训练过程大致分为如下三个阶段。

1．将树库转换为CoNLL-X版本的形式，并划分出训练集和开发集两部分

1）将短语结构树库转换为CoNLL-X版本形式

回顾一下在第6章训练短语结构树库的过程。我们曾经使用过名为StanfordParser的Python类。现在，在这个类中加入一个新的方法，代码如下。

​        …

​        def penn2conlldep(self, treebank, conllfile):

​                self.trainline ='java -mx1g -cp "'+ self.jarpath+'" edu.stanford.nlp.

​    trees.international.pennchinese.ChineseGrammaticalStructure -basic -keepPunct -conllx

​    -treeFile "'+treebank+'" > "'+conllfile+'"'

​                os.system(self.trainline)

​                print "save model to ", conllfile

该函数的功能是将短语结构树库转换为依存树库结构。执行代码如下。

​        \# -＊- coding: utf-8 -＊-

​        import sys, os

​        from stanford import ＊

​        from framework import ＊

​        reload(sys)  # 设置 UTF-8输出环境

​        sys.setdefaultencoding('utf-8')

​        os.environ['JAVA_HOME'] = 'D:/Java/jdk1.8.0_91/bin/java.exe' # 配置环境变量

​        treebank = "treebank/ctb8_all_in_one.txt" # 宾州树库文件的文本形式——导出自数据库

​        root = "E:/nltk_data/stanford-corenlp/" # 安装库

​        parser=StanfordParser(root)

​        treeconll = "treebank/ctb8_all_dep.conll"

​        parser.penn2conlldep(treebank, treeconll)

执行结果如图9.31所示。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784861753-26eef1a5-f512-4ad3-a585-089b6a615db6.jpeg)

图9.31 执行结果

在处理过程中可能会出现几处错误，正常情况下，应不超过10处，程序提示可以自己修正，这里忽略不计。现在，我们有能力把整个树库转换为一个完整的CoNLL格式的文本，但我们的工作还未最终完成。

2）现在要划分出训练集和开发集两部分

首先编写一个按比例随机抽取样本的函数，代码如下。

​    \# 分割训练集和开发集

​    def splitcorpus(rawlist, ratio):

​        rawlen = len(rawlist)

​        devlist = random.sample(rawlist, int(rawlen＊ratio))

​        trainlist = [tree for tree in rawlist if tree not in devlist]

​        return trainlist, devlist

再次执行短语结构到依存树库的转换，代码如下。

​        …

​        os.environ['JAVA_HOME'] = 'D:/Java/jdk1.8.0_91/bin/java.exe' # 配置环境变量

​        treebank = "treebank/ctb8_all_in_one.txt"

​        treelist = readfile(treebank).splitlines()

​        trainlist, devlist = splitcorpus(treelist,0.1)

​        trainpath="treebank/train.txt"; devpath="treebank/dev.txt"

​        savefile(trainpath, "\n".join(trainlist))

​        savefile(devpath, "\n".join(devlist))

​        root = "E:/nltk_data/stanford-corenlp/" # 安装库

​        parser=StanfordParser(root)

​        train_conll = "treebank/train.conll"

​        parser.penn2conlldep(trainpath, train_conll)

​        dev_conll = "treebank/dev.conll"

​        parser.penn2conlldep(devpath, dev_conll)

这样，整个树库按照90%和10%的比例被划分为如下两大部分：训练集——train.conll和开发集——dev.conll。

2．将树库中的词汇训练成Word2Vec的词向量

有了前面几节Word2Vec的基础，这一步就变得比较简单了。将完整版（未分割）依存树库文件直接训练成词向量。代码如下。

​            \# -＊- coding: utf-8 -＊-

​            import sys, os

​            from stanford import ＊

​            from framework import ＊

​            from gensim.models import word2vec

​            \# 设置 UTF-8输出环境

​            reload(sys)

​            sys.setdefaultencoding('utf-8')

​            trainpath = "treebank/ctb8_all_dep.conll"

​            sentlist = []; wordlist = []

​            for tokens in readfile(trainpath).splitlines():

​                tokenlist = tokens.split("\t")

​                if len(tokenlist) > 1:

​                    wordlist.append(tokenlist[1])

​                else:

​                    sentlist.append(wordlist)

​                    wordlist = []

​            epochs = 20

​            w2v = word2vec.Word2Vec(workers = 8, sample = 1e-5, size = 50, min_count=1, window = 5)

​            w2v.build_vocab(sentlist)

​            for epoch in range(epochs):

​                print('epoch' + str(epoch))

​                w2v.train(sentlist)

​                w2v.alpha ＊= 0.9

​                w2v.min_alpha = w2v.alpha

​            \# w2v.save('treebank/ctb8vector.bin') # 存储为二进制格式

​            w2v.save_word2vec_format('treebank/vectors.bin.txt', fvocab=None, binary=False)

​            print("word2vec DONE.")

在训练过程中应注意如下内容。

❑ Stanford神经网络句法训练器仅接收文本格式的词向量文件，因此输出的词向量文件必须为文本格式，而不是二进制格式。

❑ 因为语料规模不大，建议词向量维度为50维，这也与依存解析器所需的词向量默认维度一致。

❑ 训练好的文件第一行是词汇总数、向量维度，而在训练依存解析器时，该行不能识别，必须删除。

训练结果如图9.32所示。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784862824-3743f3e4-7097-4485-84c9-e42b3e8f57af.jpeg)

图9.32 训练结果

该文件使用时需要删除第一行：70325 50，然后保存。

3．使用神经网络模型训练依存句法树

继续在StanfordParser类中加入训练神经网络模型的新方法，代码如下。

​            def

​    train_nndeptrain(self, trainpath, devpath, embedpath, embedgsize, modelpath): # 创建模型

​    文件

​                self.trainline = 'java -mx8g -cp "'+self.jarpath+'" edu.stanford.nlp.

​    parser.nndep.DependencyParser -tlp edu.stanford.nlp.trees.international.pennchinese.

​    ChineseTreebankLanguagePack '

​                self.trainline += '-trainFile "'+trainpath+'" '

​                self.trainline += '-devFile "'+devpath+'" '

​                self.trainline += '-trainingThreads     4 '

​                self.trainline += '-embedFile "'+embedpath+'" '

​                self.trainline += '-embeddingSize '+ embedgsize+' '  # 去掉文件txt第一行

​    的词汇和维度信息：70325 50

​                self.trainline += '-model "'+modelpath+'"'

​                print self.trainline

​                os.system(self.trainline)

​                print "save model to ", modelpath

限于篇幅，这里不一一给出每个参数的解释，建议在设置时，读者一定要参考Stanford网站上有关神经网络解析器的说明（网址：http://nlp.stanford.edu/software/ nndep.shtml），了解每个参数的意义，并根据自己的设备、数据来设置符合自己的参数。

最后，执行代码如下。

​        \# -＊- coding: utf-8 -＊-

​        import sys, os

​        from stanford import ＊

​        reload(sys)  # 设置 UTF-8输出环境

​        sys.setdefaultencoding('utf-8')

​        os.environ['JAVA_HOME'] = 'D:\\Java7\\jdk1.8.0_65\\bin\\java.exe'  # 配置环境变量

​        root = "E:/nltk_data/stanford-corenlp/"  # 安装库

​        trainpath = "treebank/train.conll"

​        devpath = "treebank/dev.conll"

​        embedpath = "treebank/vectors.bin.txt"

​        embedgsize = "50"

​        modelpath = "treebank/nndep.model.txt.gz"

​        parser=StanfordParser(root)

​        parser.train_nndeptrain(trainpath, devpath, embedpath, embedgsize, modelpath)

执行结果：

训练非常耗时，大约要跑三天时间，具体时间视设备配置情况而定，输出模型：nndep.model.txt.gz。
