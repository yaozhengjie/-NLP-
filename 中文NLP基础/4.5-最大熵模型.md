4.5 最大熵模型

隐马尔科夫模型（以下称HMM）将标注问题看作一个马尔科夫链，针对相邻标注的关系进行建模，其中每个标记都对应一个概率。HMM在概率图模型中属于产生式模型，需要依赖一个联合概率分布。为了得到这个联合概率分布，产生式模型需要枚举出所有可能的观察序列，这在NLP的实际运算过程中往往是很困难的。

另一方面，NLP的大量真实语料中，观测序列更多地是以一种多重的交互特征形式表现，观测序列之间广泛存在长程相关性。例如，在命名实体识别任务中，由于专名结构所具有的复杂性，利用简单的特征函数往往无法涵盖所有的特性（它无法使用多于一个标记的特征）。

此时，需要引入一个新的模型。由于该模型获得的是所有满足约束条件的模型中信息熵极大的标签，因此称为最大熵模型。最大熵模型有如下特点。

❑ 可以使用任意的复杂相关特征。

❑ 可以灵活地设置约束条件，通过约束条件的多少来调节模型对未知数据的适应度和对已知数据的拟合程度。

❑ 它还能自然地解决统计模型中参数平滑的问题。

### 4.5.1 从词性标注谈起

下面通过一个简单的例子来介绍最大熵概念。假设我们模拟一个汉语中词性标注的过程。汉语中很多词有多个标签，如“把”就有如下5个词性标签。

❑ 数词：个把月。

❑ 名词：车把、门把。

❑ 介词：把门关上。

❑ 量词：一把伞、一把锁。

❑ 动词：把门、把住。

我们为“把”的每个词性分配一个估计值p（词性），即系统选择模型p作为某个词性出现的概率。为了得到模型p，我们收集大量的语料样本。我们的目标有两个：一个是从样本中抽取一组决策过程的事实（规则）；另一个是基于这些事实构建这一词性标注模型。

为了简便，我们把“把”的5种词性写成集合的形式：{数词、名词、介词、量词、动词}。这样，就产生了模型的一个约束条件，内容如下。

p（介词）+p（动词）+p（量词）+p（名词）+p（数词）=1

上述等式代表了“把”字各种词性出现概率的统计公式，即不论p（词性）取任何值，其总和必须为1。

显然，p（词性）可以取无数个值，那么就有无数种满足这个条件的组合可供选择。假设一种极端的情况是p（介词）=1，这个模型总是预测为介词，另一个满足这一约束的模型是p（介词）=1/2和p（量词）=1/2。当然，这两个模型都有违常理：但是，我们仅知道“把”共有这5个词性，而它们的实际概率分布我们一无所知。在没有任何语料支持的情况下，我们只能使用熵最大的均匀分布（uniform），这就是所谓的最大熵原则，内容如下。

p（介词） = 1/5

p（动词） = 1/5

p（量词）= 1/5

p（名词） = 1/5

p（数词） = 1/5

这个模型将概率均匀分配给5个可能的词性。这毕竟不是我们满意的结果。为此，引入一定数量的语料样本。在样本中有30%的比例，“把”被标注为“介词”和“量词”。这样，可以使用这些经验的数据更新模型的约束条件，内容如下。

p（介词）+p（量词）=3/10

p（介词）+p（动词）+p（量词）+p（名词）+p（数词）=1

那么，如何确定剩下几个词性的概率呢？很简单，在没有其他知识的情况下，按照最大熵原则的合理模型p是均匀分布，也就是在满足约束的条件下，将概率尽可能均匀地分配。均匀分布示例如表4.9所示。

表4.9 均匀分布示例

| 已知的约束条件               | 未知的均匀分布（最大熵原则）               |
| ---------------------------- | ------------------------------------------ |
| p（介词）=3/20p（量词）=3/20 | p（动词）=7/30p（名词）=7/30p（数词）=7/30 |

继续引入语料样本，随着语料数量的增大，我们发现“把”作为介词和名词的概率显著增加，单独记录下这个统计结果，把它作为统计信息的第三个约束，内容如下。

p（介词）+p（量词）=3/10

p（介词）+p（动词）+p（量词）+p（名词）+p（数词）=1

p（量词）+p（名词）=1/2

我们可以再次寻找满足这些约束的均匀分配的模型p。当然，可以像表4.9一样很简单地构建一种新表。但这一次的结果没有那么明显。由于增加了问题的复杂度，现在出现了如下两个问题。首先，“均匀”究竟是什么意思呢？如何度量一个模型的均匀度（uniformity）？其次，有了这些问题的答案之后，如何找到满足一组约束且最均匀的模型？

最大熵的方法回答了上述两个问题。直观上讲，很简单，即根据已知的知识进行建模，而对未知的不做任何假设（model all that is known and assume nothing about that which is unknown）。换句话说，在给定一组事实（features+output）的条件下，选择符合所有事实，并在未知方面使用熵最大的均匀模型。这恰恰是上述例子中每一步选择模型p所采取的方法。

### 4.5.2 特征和约束

根据上例构造一个概率图模型，它产生一个输出y, y属于一个有穷集合Y。对于刚才词性标注的例子，该过程输出词汇“把”的词性，输出值y可以是集合{数词、名词、介词、量词、动词}中任何一个标签。在输出y时，该过程可能会受上下文信息的影响，我们把这个语境因素定义为x, x属于有穷的集合X。在词性标注的例子中，X是指句子中出现在“把”周围的所有词汇。而我们的任务是构造一个统计模型，该模型的任务是预测在给定上下文x的情况下，输出y的概率：p(y|x)。

为了得到这个模型，我们需要收集大量的语料样本：(x1, y1), (x2, y2), …, (xn, yn)。这里每一个样本都包含“把”的上下文词汇x，以及“把”对应的标签y。这需要大量的文本语料，有关语料的收集、整理和处理，在后面的章节将有详细的介绍，本章暂不涉及。

但不论你收集多少语料，它们都不是语言文本的总体（语言的生成具有动态性），而只是样本，换句话说不能用p来表示，于是引入一个新的经验分布函数![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784622406-e9fa8908-70a9-4ce6-a496-c29937be152b.jpeg)来表达训练样本：

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784622811-7b8c7b09-1db4-482d-9457-0b6825b31754.jpeg) 在样本中出现的次数

通常，对于一个特定的(xi, yi)对，它要么不出现在样本中，要么最多出现N次。

我们的目标是根据收集到的语料样本构建![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784623206-148e34f6-7624-4132-8992-be6ca1fc9b9e.jpeg)的概率模型。构成该概率模型的组件将是一组训练样本的统计值。在上例中，已经采用了几个统计数据：（1）p（介词）+p（量词）=3/10。（2）p（量词）+p（名词）=1/2，等等。这些统计模式都是上下文独立的，但也可以考虑上下文依赖信息x的模式。例如，在语料样本中，如果“一”出现在“把”之前，那么“把”属于“量词”的概率为9/10。

为了表示这个事件（event），即当“一”出现在“把”之前，“把”被标注为“量词”，引入了如下指示函数（取值为0、1两种情况）：

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784623468-cd53f641-f90c-4f75-898d-092efa0982cf.jpeg)

其中，特征f为关于经验分布![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784623730-b95f1ee9-2d15-42ef-b6cd-dd6773e627be.jpeg)的期望值。将这个期望值表示为：

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784624212-1368a55f-86e8-4918-a00f-536d6faa7c4b.jpeg) （4.5.1）

可以将任何样本的统计量表示成一个适当的二值指示函数的期望值，这个函数叫作特征函数（Feature Function）或简称为特征（Feature）。

概率模型的一个设计要点就是要找到符合要求的统计量。该统计量要能使模型通过拟合达到希望的结果。这个拟合过程通过约束模型p分配给相应特征函数的期望值来实现。特征f关于模型p(y|x)的期望值：

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784624992-6bb40254-b3f0-41a7-8f06-2a41c8159569.jpeg) （4.5.2）

这里，![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784625438-8c221a85-4f22-4358-bdb9-41b45281146d.jpeg)是来自语料样本中的x的经验分布。约束这一期望值和语料样本中f的期望值相同。

这里隐含一个假设，即样本的分布与总体的分布关系，根据大数定理，样本要足够大才能够满足如下要求：

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784625825-11e05d8f-71b9-4acf-bf1a-73970868d122.jpeg) （4.5.3）

组合上述的式（4.5.1）、式（4.5.2）和式（4.5.3），得到如下等式：

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784626268-d49e234f-4b91-40c6-ae89-90c9f6e8fa7a.jpeg)

式（4.5.3）称为约束等式（Constraint Equation）或者简称为约束（Constraint）。我们只关注那些满足式（4.5.3）的概率模型p(y|x)，而不考虑那些和训练样本中特征f频率不一致的模型。

最后，关于特征和约束，这里再强调两句：“特征”和“约束”在讨论最大熵时经常被混用，我们希望读者注意区分这两者的概念：特征（Feature）是(x, y)的二值函数；约束是一个等式，即模型的特征函数期望值等于训练样本中特征函数的期望值。

### 4.5.3 最大熵原理

假设有n个特征函数fi，它们的期望值决定了在建模过程中重要的统计数据。要想模型符合这些统计，也就是说，要想模型E属于P的子集C，即

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784626585-5d4f31eb-455c-4f65-9480-3ab1894eebc0.jpeg) （4.5.4）

图4.14所示为式（4.5.4）约束的几何解释。这里，P是所有可能的概率分布空间。如果不施加任何约束，如图4.14（a）所示，所有概率模型都是允许的。施加第一个线性约束C1后，模型被限制在C1定义的区域，如图4.14（b）所示。如果施加两个约束且第二个线性约束如图4.14（c）所示，则可以准确地确定p。另一种情形是，第二个线性约束与第一个不一致，例如，第一个约束可能需要的概率是1/3，第二个约束需要的概率是3/4，如图4.14（d）所示。在目前的设置中，线性约束是从语料数据集中抽取的，数据量庞大不可能手工构造，在众多的数据中不一致的可能性很小。进一步来说，在应用中的线性约束甚至不会接近唯一确定的p，就像图4.14（c）所示的那样。相反，集合C=C1∩C2∩…∩Cn中的模型是无穷的。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784626988-9ba58a43-18f5-4116-8fee-2b05da822de8.jpeg)

图4.14 最大熵模型

属于集合C的所有模型p中，最大熵的理念决定我们选择最均匀的分布。但现在，我们面临一个前面遗留的问题：什么是“均匀分布”？数学上，条件分布p(y|x)的均匀度被定义为条件熵：

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784627974-d085e74a-6ee4-44a4-901e-71d16c8022ff.jpeg) （4.5.5）

熵的下界是0，这时模型没有任何不确定性；熵的上界是log|Y|，即在所有可能（|Y|个）的y上均匀分布。有了这个定义，我们就可以提出最大熵原则。

当从允许的概率分布集合C中选择一个模型时，选择模型p*∈C，使得熵H(p)最大。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784628285-3a21585b-52a5-4522-ba57-588209972d2d.jpeg) （4.5.6）

这是之前所称的原始问题的展开。简单地讲，我们的目标是在p∈C的情况下，最大化H(p)。

该约束问题的完整形式如下：

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784628648-cb2ec043-e7dc-40f5-aaf0-670303c74aba.jpeg)

目标函数的约束条件如下：

（1）p(y|x)≥0，对于所有的x、y。

（2）![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784629062-022d8f20-35e8-4c0a-99e4-07977a4c49f2.jpeg)，对于所有的x。确保p是一个条件概率。

（3）![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784629585-e9e236e8-7736-49de-b2b8-76ff9fb94c8f.jpeg)。

其中，i∈{1,2, …, n}。换句话说，p∈C，满足有效约束C。

### 4.5.4 公式推导

为了解决优化问题，引入拉格朗日乘子：

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784630101-b3f7f363-c30e-42a3-80da-e725200bfadf.jpeg) （4.5.7）

实值参数γ和Λ={λ1, λ2, …, λn}对应施加在解上的n+1个约束。下面的策略可以求出p的最优解（p*）。

首先，将γ和Λ={λ1, λ2, …, λn}看成常量，寻找p最大化公式，即式（4.5.7）。这会产生以γ和Λ={λ1, λ2, …, λn}为参数的表示式p（参数没有解决）。接着，将该表达式代回式（4.5.7）中，这次求γ和Λ={λ1, λ2, …, λn}的最优解（ γ*和Λ*）。

按照这一方式，固定γ和Λ，计算在所有p∈P空间下ξ(p, Λ, γ)的无约束的最大值。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784630621-b2437060-9b2a-4b4e-b018-b0c32fb85ef6.jpeg)

令该式等于0，求解p(y|x)：

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784631222-bc12914c-28e3-49f7-9422-f4112b050815.jpeg) （4.5.8）

可以看出式（4.5.8）的第二个因子对应第二个约束：![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784632223-c95933e0-d964-4ccb-8e92-eb84d252d302.jpeg)。

将上式带入式（4.5.8）得到：

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784632690-b1272dd3-0d71-416a-b133-47c63798b047.jpeg) （4.5.9）

将式（4.5.9）带入式（4.5.8），得到：

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784633185-bfe1a52d-eb5c-4d11-ad60-1123fd10ad50.jpeg) （4.5.10）

因此得到：

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784633888-336ba652-f1ac-4137-a05c-85e3c0bc9e9b.jpeg) （4.5.11）

Z(x)是正则化因子。

### 4.5.5 对偶问题的极大似然估计

现在要求解最优值γ*和Λ*。显然，已经知道了γ*，还不知道Λ*。为此，引入对偶函数Ψ：

Ψ(Λ) ≡ ξ(p*, Λ, γ*)（4.5.12）

对偶优化问题如下：

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784634235-1e039b17-0921-4b99-b123-b7ffb92ace54.jpeg) （4.5.13）

因为p*和γ*是固定的，式（4.5.13）的右边只有自由变量Λ={λ1, λ2, …, λn}。

参数值等于Λ=Λ*的p*就是一开始约束优化问题的最优解。这是拉格朗日乘子理论的基本原理，通常叫作Kuhn-Tucker Theorem（KTT）。最后结论如下。

满足约束C最大熵模型具有式（4.5.13）的参数化形式，最优参数Λ*可以通过最小化对偶函数Ψ(Λ)求得。

补充说明如下。

Ψ(Λ)究竟是什么样呢？记住，我们要求ξ(p*, Λ, γ*)的最小值，这是拉格朗日乘子理论的基本原理。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784634745-63e51ae7-6ea4-4e43-8b28-d85415d3947d.jpeg)

在目前情况下，要说出最终的结果是很容易的：假设λ*是对偶问题的解，pλ*是原问题的解。那么，pλ*=p*。换句话说，最大熵模型的约束条件C有一个形式参数pλ*，其中，参数值λ*可以通过最大化对偶函数Ψ(λ)来确定。表4.10总结了我们已经建立的原问题—对偶问题的框架。

表4.10 最大熵的对偶问题

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784635419-8e7a9515-cbd7-4d92-8aa9-9927ba3b4ca3.jpeg)

下面使用经验分布![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784635792-dc8a2cfa-6e0e-4e5c-927e-c934338ac378.jpeg)的极大似然法的对数似然度![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784636035-cfa363cf-7cf6-4427-bdfe-31fffa2fda29.jpeg)，作为模型p的预测，定义为

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784636349-21ff2d2b-0436-4dba-a79b-bb812272a035.jpeg) （4.5.14）

推导过程：

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784636734-1a9b52ae-4663-496f-99b0-30fa18aec689.jpeg)

因此，有：max(Lp(p))+min(ψ(λ))

这里的p具有式（4.5.14）的形式，其结果进一步可以表述为：最大熵模型p*∈C是具有式（4.5.14）的形式，且最大化样本似然率的模型。最大熵模型与其极大似然估计的模型一致。

对偶函数Ψ(λ)其实就是指数模型pλ的对数似然度，即

Ψ (λ)+Lp(pλ)

按照这种解释，前面所述的结果可以改写为：

含有最大熵的模型p∈C就是参数族为Pλ(y|x)的模型，它最大化了与训练样本P～的似然度。

### 4.5.6 GIS实现

要计算λ，直接求解析解肯定是行不通的。对于最大熵模型对应的最优化问题，GIS、lbfgs、sgd等最优化算法都能解。相比之下，GIS大概是最好实现的。这里只介绍GIS算法。具体步骤如下：

（1）设置![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784637101-9936e2af-2aae-4038-8b0e-b512bebb6bd4.jpeg)等于任意值，如等于0。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784637453-38e648f5-74ca-4482-b194-710af4c732d4.jpeg)

（2）重复直到收敛。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784637796-5a682788-566a-4210-8df1-5507e9c981ea.jpeg)

这里，(t)是迭代下标，常数M定义为：![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784638012-9f3776a0-969e-41d9-bbb6-7184d644ad23.jpeg)

实践中M是训练样本中最大的特征个数。M再大些也没关系，但是它决定了收敛速度，还是取最小的好。实际上，GIS算法用第N次迭代的模型来估算每个特征在训练数据中的分布。如果超过了实际的，就把相应参数λ变小。否则，将它们变大。当训练样本的特征分布和模型的特征分布相同时，就求得了最优参数。

案例代码如下。

​        \# -＊- coding: utf-8 -＊-

​        import sys

​        import os

​        from collections import defaultdict

​        import math

​        \# 设置 UTF-8输出环境

​        reload(sys)

​        sys.setdefaultencoding('utf-8')

​        class MaxEnt(object):

​            def __init__(self):

​              self.feats = defaultdict(int)

​              self.trainset = []  # 训练集

​              self.labels = set() # 标签集

​            def load_data(self, file):

​              for line in open(file):

​                  fields = line.strip().split()

​                  if len(fields) < 2: continue # 特征数要大于两列

​                  label = fields[0] # 默认第一列是标签

​                  self.labels.add(label)

​                  for f in set(fields[1:]):

​                      self.feats[(label, f)] += 1 # (label, f) 元组是特征 # print label, f

​                  self.trainset.append(fields)

​            def _initparams(self): #初始化参数

​      self.size = len(self.trainset)

​      self.M = max([len(record)-1 for record in self.trainset]) # GIS 训练算法

​                                                                    \# 的M参数

​      self.ep_ = [0.0]＊len(self.feats)

​      for i, f in enumerate(self.feats):

​          self.ep_[i] = float(self.feats[f])/float(self.size) # 计算经验分布的

​                                                                \# 特征期望

​          self.feats[f] = i # 为每个特征函数分配id

​      self.w = [0.0]＊len(self.feats) # 初始化权重

​      self.lastw = self.w

  def probwgt(self, features, label): #计算每个特征权重的指数

​      wgt = 0.0

​      for f in features:

​          if (label, f) in self.feats:

​            wgt += self.w[self.feats[(label, f)]]

​      return math.exp(wgt)

  """

  calculate feature expectation on model distribution

  """

  def Ep(self): #特征函数

​      ep = [0.0]＊len(self.feats)

​      for record in self.trainset: #从训练集中迭代输出特征

​          features = record[1:]

​          prob = self.calprob(features) # 计算条件概率 p(y|x)

​          for f in features:

​            for w, l in prob:

​                if (l, f) in self.feats: # 来自训练数据的特征

​                    idx = self.feats[(l, f)] # 获取特征 id

​                    ep[idx] += w ＊ (1.0/self.size) # sum(1/N ＊ f(y, x)＊p(y|x)),

p(x) = 1/N

​      return ep

  def _convergence(self, lastw, w): #收敛—终止条件

​      for w1, w2 in zip(lastw, w):

​          if abs(w1-w2) >= 0.01: return False

​      return True

  def train(self, max_iter =1000): # 训练样本的主函数。默认迭代次数1000次

​      self._initparams() # 初始化参数

​      for i in range(max_iter):

​          print 'iter %d ...'%(i+1)

​          self.ep = self.Ep()  # 计算模型分布的特征期望

​          self.lastw = self.w[:]

​          for i, win in enumerate(self.w):

​              delta = 1.0/self.M ＊ math.log(self.ep_[i]/self.ep[i])

​              self.w[i] += delta # 更新 w

​          print self.w , self.feats

​          if self._convergence(self.lastw, self.w): # 判断算法是否收敛

​              break

​    def calprob(self, features): #计算条件概率

​      wgts = [(self.probwgt(features, l), l) for l in self.labels] #

​      Z = sum([ w for w, l in wgts]) # 归一化参数

​      prob = [ (w/Z, l) for w, l in wgts] # 概率向量

​      return prob

​    def predict(self, input): # 预测函数

​      features = input.strip().split()

​      prob = self.calprob(features)

​      prob.sort(reverse=True)

​      return prob

数据文件如下。

| 训练集                     | 整理后的特征：   |
| -------------------------- | ---------------- |
| Outdoor Sunny Happy        | Outdoor Sunny 5  |
| Outdoor Sunny Happy Dry    | Outdoor Happy 5  |
| Outdoor Sunny Happy Humid  | Outdoor Dry 2    |
| Outdoor Sunny Sad Dry      | Outdoor Humid 6  |
| Outdoor Sunny Sad Humid    | Outdoor Sad 4    |
| Outdoor Cloudy Happy Humid | Outdoor Cloudy 4 |
| Outdoor Cloudy Happy Humid | Indoor Rainy 4   |
| Outdoor Cloudy Sad Humid   | Indoor Humid 4   |
| Outdoor Cloudy Sad Humid   | Indoor Happy 2   |
| Indoor Rainy Happy Humid   | Indoor Dry 2     |
| Indoor Rainy Happy Dry     | Indoor Sad 4     |
| Indoor Rainy Sad Dry       | Indoor Cloudy 2  |
| Indoor Rainy Sad Humid     |                  |
| Indoor Cloudy Sad Humid    |                  |
| Indoor Cloudy Sad Humid    |                  |

测试代码部分如下。

​      \# -＊- coding: utf-8 -＊-

​      import sys

​      import os

​      import maxent

​      \# 设置 UTF-8输出环境

​      reload(sys)

​      sys.setdefaultencoding('utf-8')

​      model = maxent.MaxEnt()

​      model.load_data('data.txt') # 导入训练集

​      model.train() # 训练模型

​      print model.predict("Rainy Happy Dry") # 预测结果

训练结果如下。

​      iter 1 ...

​      [0.0, 0.23104906018664842, 0.0, -0.18653859597847416, 0.23104906018664842, 0.0,

  0.11889164797957746, -0.13515503603605475, 0.0, -0.07438118377140324, 0.060773852264651596,

  0.09589402415059367]

​      defaultdict(<type  'int'>,  {('Outdoor',  'Sad'):  0,  ('Outdoor',  'Sunny'):  1,

  ('Outdoor', 'Dry'): 2, ('Indoor', 'Happy'): 3, ('Indoor', 'Rainy'): 4, ('Indoor',

  'Dry'): 5, ('Outdoor', 'Happy'): 6, ('Indoor', 'Cloudy'): 7, ('Indoor', 'Sad'): 8,

  ('Indoor', 'Humid'): 9, ('Outdoor', 'Humid'): 10, ('Outdoor', 'Cloudy'): 11})

​      iter 2 ...

​      ...

​      iter 144 ...

​      [-0.3644512232512526, 3.7191730818297613, -0.05544928580348433, -1.3399214304769507,

  4.703394433851156,  0.058616584006608345,  0.6051416833043833,  -0.584974488792124,

  0.37383730234345736, 0.04635901193875606, -0.018818546603234308, 0.3283161400233158]

​      defaultdict(<type  'int'>,  {('Outdoor',  'Sad'):  0,  ('Outdoor',  'Sunny'):  1,

  ('Outdoor', 'Dry'): 2, ('Indoor', 'Happy'): 3, ('Indoor', 'Rainy'): 4, ('Indoor',

  'Dry'): 5, ('Outdoor', 'Happy'): 6, ('Indoor', 'Cloudy'): 7, ('Indoor', 'Sad'): 8,

  ('Indoor', 'Humid'): 9, ('Outdoor', 'Humid'): 10, ('Outdoor', 'Cloudy'): 11})

预测结果如下。

​    [(0.9464649414197988, 'Indoor'), (0.05353505858020124, 'Outdoor')]
