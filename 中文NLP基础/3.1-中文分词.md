3.1 中文分词

信息处理的目标是使用机器（主要指计算机）能够理解和产生自然语言。而自然语言理解和产生的前提是对语言能够做出全面的解析。汉语词汇是语言中能够独立运用的最小的语言单位，是语言中的原子结构。因此，对中文进行分词就显得至关重要。

前面讲过，汉字起源于象形字发展起来的表意符号（部分也表音，但不精确）。从古至今每个汉字之间都是相互独立的结构，在早期的文言文中，一个汉字表达一个完整的语义，一个汉字就是一个词，所以不存在分词的问题。随着白话文的发展，词汇从单音词走向了复音词，从独字为词发展为以二字词为主，独字词、多字词并存的阶段。以复音词占主体的现代白话文，却没有明确地给出划分词汇的标志。这与千百年来中文行文习惯上不实行分词连写有关。无论是文言文还是白话文，为了明确语义，仅在句子停顿处标以句读，这是古已有之的，但当时的标点符号不规范、不统一，你标你的，我标我的，各自为政，没有系统地发展起来。现代标点符号系统源于五四运动，之后几十年不断发展为白话文的统一书写规范，虽然它反映了现代汉语的进步，但这些符号主要都是标句符号，对于复音词之间的划分并未区分。

另一个重要方面，因为文化的巨变，从五四运动确立了白话文作为通用的书面语，距今时间还不到一百年。这近百年来，在语法、语义方面的研究也走了不少弯路。很多理论还不够成熟，独立的计算语言体系也未建立。基本上，中文的实际应用在传统与西方的双重影响下此消彼长地发展，还处于一个未成熟的阶段。

### 3.1.1 什么是词与分词规范

1996年，有人通过6个母语为汉语的被试者对同一篇文本进行手工分词，文本由100个句子组成，含4 372个字。被试者共6人，其中三位来自内地，三位来自我国台湾地区。人们因自身居住地区的不同、受教育程度等因素，对词汇的切分产生的差异，称为词语认同率。与人们的猜测有很大的不同，两个地区的人对词汇的认同产生了比较大的差异。其中，差异最大的仅为0.69，最小为0.89，平均认同率为0.76。（《中文分词十年回顾》）由此可见，即便是母语为汉语的使用者，对于划分词汇的标准也是有分歧的。

那么，什么是词，我们如何界定汉语词呢？古往今来，汉字虽然有5万多个，但常用的汉字大约仅有6 000个。即便如此，其中很多汉字在日常生活中较少用到。然而，这些有限的汉字足以维持词汇的长期更新，因为扩大中文词汇的方法是通过构造汉字的复合新词，而不是创造新的字符来完成的。这就造成了汉语中所谓的词和短语之间没有明确的界限。这可能也就是中国的一些语法学家认为，中文没有词语而只有汉字的原因，并创造了一个术语——“字短语”来代替传统的词汇。董振东就认为：“‘词或字符’的争论源于他们都急于给中国语言一个硬规范的共同基础。遗憾的是，中文不是那么明确的或硬的，它是软的。我们必须认识到其‘柔软度’”。

除构词法的原因之外，人们还因为自身的方言、受教育程度、亚文化等差异因素，对词汇的认识也不同。这些事实都成为制定统一的汉语分词标准的障碍。但是对于计算机系统而言，同一种语言应使用统一的规范来划分词汇，否则，很难想象在多重标准下的词汇系统能够计算出相同的语义结果。这是计算机系统所提供的规定性。因此，构建一套统一的分词规范就显得尤为重要。

随着NLP的大规模应用，计算语言学界逐渐统一了汉语词汇的标准。从最初的“结合紧密，使用稳定”到信息处理领域的《信息处理用现代汉语分词规范》的制定，都是确定汉语分词标准的一种尝试，该文关于汉语词的定义给出了如下说明。

汉语信息处理使用的、具有确定的语义或语法功能的基本单位。⋯⋯

——《信息处理用现代汉语分词规范》

从计算语言学的角度来看，如果把一个句子理解为一个特殊的可计算的逻辑表达式，那么句子中的一个词就是表达式中的一个可计算符号，有的表示为连接的符号，如连词“然后”、“而且”这样的虚词；有的表示为动作、状态（函数的签名），如“出现”、“思考”等这样的动词；有的表示为事物的概念，如“中国”、“泰山”等这样的名词。

本书涉及的分词规范有如下两大类：第一类包括《北大（中科院）词性标注》、《现代汉语语料库加工规范——词语切分与词性标注》、《北京大学现代汉语语料库基本加工规范》三篇文章，读者可从http://www.threedweb.cn/thread-1584-1-1.html、http://www. threedweb.cn/thread-437-1-2.html下载；第二类为《宾州树库中文分词规范》，读者可从http://www.threedweb.cn/thread-1478-1-1.html下载。

本节主要介绍中文分词中最常用的《北大（中科院）词性标注》（以下简称《北大规范》）的基本原则。

《信息处理用现代汉语分词规范》和传统的语法教育中将汉语的词类主要分为13种：名词、动词、代词、形容词、数词、量词、副词、介词、连词、助词、语气词、叹词和象声词。这与朱德熙先生提出的19种分类法有所不同。朱先生的分类法还包括：时间词、处所词、方位词、区别词、状态词等。除此之外，《北大规范》还加入了4个兼类谓词：副动词、名动词、副形词、名形词；最后还增加了前缀、后缀、成语、简称、习用语5种辅助词类。这样，《北大规范》就形成了40种词类。

这40种词类与《信息处理用现代汉语分词规范》所述的13种词类可以用表给出对照关系（见表3.1）。

表3.1 修改后的现代汉语词语分类体系对照表

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784536394-6b393381-6626-4469-9b42-ebade3f43dc6.jpeg)

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784538904-8cdb34d6-c1df-4ee0-89ac-0e042ea956fa.jpeg)

如表3.1所示，从分词的角度来看，这两种切分标准之间的差距在于，北大标准除考虑到词汇的语法特征之外，还兼顾了词的语义特征。但是从语义研究的角度来看，这些语义特征并不完备，不过一些更细节的词性可划归到其父类之中。

除此之外，表3.1中还有一个问题需要澄清，即“附加类别”中的各个子类。下面由简而繁逐个说明。

（1）成语和习用语。这是中文的特有词汇类别，但该种方式与句子解析和词汇切分无关，本书中基本遵循这样的原则：成语基本都作为一个完整的词进行切分；习用语有的仅包含一个词，就作为词来切分，有的是一个句子，就逐词进行切分。

（2）语素和前后缀。

❑ 语素。语素是构成词的最小单位，其粒度小于词汇。是否需要进行切分可根据用户需求，从语言处理的角度来看，语素级别的切分使用范围并不大。例如，“毛泽东”在《北大标准》中常切分为：“毛/泽东”。本书建议作为一个完整的词来对待。

❑ 前后缀。比较典型的前/后缀包括：初（初一）、阿（阿姨、阿爸）、老（老先生）、第（第一、第二）、儿（花儿）、们（男人们、同志们）。这要根据实际情况来做处理。其切分标准按照前/后缀的能产型和语义完整性两个标准来切分。例如，“初一”的“初”作为前缀，能产性较弱，不予切分；“们”的能产性比较强，应做切分，但也不绝对。“人们”的语义完整性更强，可不做切分。

有关更多的切分细节可参照前文给出的相应文档，并结合自身的需求，制定切分规则。

### 3.1.2 两种分词标准

由于语素对词汇的构成也产生影响，实际应用中，汉语分词也分为两个粒度。粗粒度分词：将词作为语言处理最小的基本单位进行切分。细粒度分词：不仅对词汇进行切分，也要对词汇内部的语素进行切分。

例如，原始串：浙江大学坐落在西湖旁边。

粗粒度：浙江大学/坐落/在/西湖/旁边。

细粒度：浙江/大学/坐落/在/西湖/旁边。

粗粒度将“浙江大学”看作一个完整的概念，对应一个完整的词汇，进行切分。而细粒度则不同，除将“浙江大学”完整切分出来之外，还要将构成“浙江大学”的各个语素切分出来：浙江/大学。

常见的例子还有很多，如“中华人民共和国”，粗粒度的分词就是“中华人民共和国”，细粒度的分词可能是“中华/人民/共和国”。一般细粒度切分的对象都为专有名词。因为专有名词常表现为几个一般名词的合成。

在实践中，粗粒度切分和细粒度切分都有其使用的范围。粗粒度切分主要用于自然语言处理的各种应用；而细粒度分词最常用的领域是搜索引擎。一种常用的方案是，在索引的时候使用细粒度的分词以保证召回，在查询的时候使用粗粒度的分词以保证精度。在本书中，如果未加特别的说明，则都为粗粒度分词。

### 3.1.3 歧义、机械分词、语言模型

现代汉语的复音词结构，使少量的字符（4 000多个）通过排列组合来表示大量的词汇（100万个以上），而中间又没有标点的分隔，最容易出现的问题是歧义问题。歧义问题在汉语中普遍存在，长久以来歧义切分问题一直是中文分词的核心问题之一。对此，梁南元等已经做过广泛和深入的研究。下面给出几种重要的歧义切分的研究成果。

定义7-1（交集型切分歧义）汉字串AJB称作交集型切分歧义，如果满足AJ、JB同时为词（A、J、B分别为汉字串），则此时汉字串J称作交集串。（梁南元1987）

例如，交集型切分歧义：“结合成”。

其中，A=“结”, J=“合”, B=“成”。

一种切分为：（a）结合 | 成；另一种切分为：（b）结 | 合成

这种情况在汉语文本中非常普遍，如“大学生”、“研究生物”、“从小学起”、“为人民工作”、“中国产品质量”、“部分居民生活水平”等。为了刻画交集型歧义字段的复杂结构，梁南元还定义了链长的概念。

定义7-3（组合型切分歧义）汉字串AB称作多义组合型切分歧义，如果满足A、B、AB同时为词。

例如，多义组合型切分歧义：“起身”。在如下两个例子中，“起身”有两种不同的切分：（a）他站 | 起 | 身 | 来。（b）他明天 | 起身 | 去北京。类似的，“将来”、“现在”、“才能”、“学生会”等，都是组合型切分歧义字段。

梁南元（1987a）曾经对一个含有48 092字的自然科学、社会科学样本进行了统计，结果交集型切分歧义有518个，多义组合型切分歧义有42个。据此推断，中文文本中切分歧义的出现频度约为1.2次/100字，交集型切分歧义与多义组合型切分歧义的出现比例约为12∶1。

有意思的是，据文献[刘挺等，1998a]的调查却显示了与梁南元截然相反的结果：汉语文本中交集型切分歧义与多义组合型切分歧义的出现比例约为1∶22。孙茂松认为，造成这种情形的原因在于，定义7-3有疏漏。因此，孙茂松等（2001）曾经猜测，加上一条上下文语境限制才真正反映了梁南元的本意。

定义7-3'（多义组合型切分歧义）汉字串AB称作多义组合型切分歧义，如果满足（1）A、B、AB同时为词；（2）文本中至少存在一个上下文语境c，在c的约束下，A、B在语法和语义上都成立。

——上文均来自《统计自然语言处理》—宗成庆著

针对上述问题，人们设计了早期的机械分词系统。机械分词系统都是基于最大匹配方法作为最基本的分词算法。该方法由苏联汉俄翻译学者提出，也称为MM（The Maximum Matching Method）方法。其基本思想是，假设自动分词词典中的最长词条所含汉字个数为I，则取被处理材料当前字符串序数中的1个字作为匹配字段，查找分词词典。若词典中有这样的一个I字词，则匹配成功，匹配字段作为一个完整的词被切分出来；如果词典中找不到这样的一个I字词，则匹配失败。匹配字段去掉最后一个汉字，剩下的字符作为新的匹配字段，回到上述步骤，重新匹配，如此进行下去；直至切分到成功为止。即完成一轮匹配，并切分出一个词，之后再按上述步骤进行下去，直到切分出所有词为止。

例如，现有短语“计算机科学和工程”，假设词典中最长词为7字词，于是先取“计算机科学和工”为匹配字段，来匹配词典，由于词典中没有该词，故匹配失败；去掉最后一个汉字成为“计算机科学和”作为新的匹配字段，重新匹配词典，同样匹配失败；取“计算机科学”作为新的字段来匹配词典，由于词典中有“计算机科学”一词，从而匹配成功，切分出的第一个词为“计算机科学”。以此类推，直至切分出第二、三……个词。

使用MM方法切分的精度并不高，很难达到实际应用的要求，随着语料的增大，误差也逐渐变大。之后人们又基于此方法提出了双向匹配法。该方法是从最大匹配方法发展而来的，分为正向最佳匹配法和逆向最佳匹配法。它们的基本原理都是相似的：将待分析的汉字串与机器词典中的词条进行最大匹配，若在词典中找到某个字符串，则匹配成功（识别出一个词）。所不同的是，两个算法的搜索方向相反。待处理的字符串中存在着交叉歧义，因此两种方法所得的结果必然不同。当然，基于最大匹配的搜索方法还存在着局限性，比如正向最大匹配，因为只能正向地找出最长的词，而不能找出所有的候选词条。因此，后来发展出了双向扫描法来更快速地检测出歧义产生的位置。

这类早期的分词器因为没有考虑到词汇上下文的相关性，分词的准确度都不高。基于正向最大匹配算法的分词器的准确度为78%；召回率为75%; F1值约为76%。后来改进的双向匹配算法的最高精度也在80%左右徘徊。显然这不能满足高精度文本处理的需求。

基于机械方法的分词器虽然没有得到广泛的应用，但是却揭示了一个重要的语言规律：一个词汇的出现与其上下文环境中出现的词汇序列存在着紧密的关系，如果算法不能反映和处理这种上下文依赖关系，则不能最终达到满意的分词结果。所谓上下文相关性是指，文本中第n个词的出现与其前后n-m到n+m个词有高度的相关性，而与这个范围之外的其他词的相关性较低。我们把[-m, m]范围也称为窗口范围。

考虑单侧的情况，文本中第i个词的出现与其前面i-n个词相关（0<n<i），而不考虑这个窗口之外的其他词的相关性。形式化如下，假设WlW2…Wn是长度为n的字串，则词Wi出现的似然度用方程表示为：

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784539541-9555c999-bee0-441d-8e9b-4f3bb78679f7.jpeg)

由上式可知，要计算得到词Wi的出现概率，必须要知道在Wi之前所有词的出现概率。这显然不可行，也没有意义。但是如果任意一个词Wi的出现概率只与它相邻的一个词有关，那么问题的解决即可得到极大的简化。这时的语言模型叫作二元模型，也称为一阶Markov链，即：

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784540438-1ceeb145-dfb7-422e-87c3-0def29a95177.jpeg)

当n=2时，统计二元概率的计算公式就为：

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784541418-5813cc44-baee-4e6f-af9e-211ffab50cfb.jpeg)

Count(.)表示一个特定词序列在整个语料库中出现的累计次数。

将语言模型应用到分词算法中，中文分词的水平将会得到显著的改善，ICTCLAS的中文分词算法就是这方面的最成功的案例，经测试该分词器的准确率为98%；召回率为98.50%; F1值约为98%。这一改善与之前的分词系统相比有一个显著的质变。该算法使高精度中文文本处理成为可能。

### 3.1.4 词汇的构成与未登录词

能够正确地消除切分中的歧义，使计算机处理词汇问题又前进了一大步。但是问题仍旧没有完全解决。所谓词汇，一般都具有三个重要的特性：稳固性、常用性和能产性。稳固性、常用性都比较容易理解。关键问题在于能产性。前面讲过有关汉语复音词的构词方法，与拼音文字不同，汉语的构词机制是一个动态的自组织认知系统，而不是一个静态的系统。随着外界新事物的产生，表达概念的新词汇也会层出不穷地涌现，这不是什么特别的技能，对于中国人而言是一种本能，几乎人人都具备。人们给小孩子起名字就是词汇的能产性生动的体现：一方面，孩子的名字要继承父辈的姓氏，另一方面又要传达大人对孩子未来最美好的期望，而又尽可能不与他人的名字重复。这个过程就是词汇能产性的表现。网络媒体、专业术语、组织机构的命名都是新词产生的重要来源。

在自然语言处理中，它们被统称为未登录词识别（Named Entity Recognition, NER）。在真实文本的切分中，未登录词总数的大约九成是专有名词，其余的为通用新词或专业术语。因此，未登录词识别就是包括中国人名、译名、日本人名、地理位置名称、组织机构名称等专有名词的识别。在自然语言处理研究中，人们通常将上述专有名词和数字、日期等词称为命名实体。由于命名实体识别不仅是汉语自动分词研究中的关键问题，也是诸如英语等其他语言处理中的重要问题，它的处理效果直接影响到信息抽取、信息检索和机器翻译、文摘自动生成等应用系统的性能。因此，近几年来专有名词的处理（包括识别、翻译等）已经成为自然语言处理研究中一个非常活跃的分支。

以往人们的处理方法常从构词学的角度来研究算法，因此研发了很复杂的基于构词编码的方法（下文介绍的HanLP中文分词的系统就是一种基于构词角度来进行命名实体识别的算法）。事实证明，这种做法对于狭窄的专门领域（人名中的中国人名、译名、日本人名）等的未登录词识别，能够获得较好的效果，但在处理大规模不同领域的未登录词问题上存在着很大的障碍，至少是不现实的。

笔者认为应从语义类的角度来重新考虑这个问题。不同语义类下的未登录词，在统计学规律上具有相似性。利用这一点，近些年命名实体识别方面新的算法层出不穷，已经证明，基于半监督的条件随机场（semi-CRF）算法，对于处理不同领域的专名识别具有较低的成本和较好的效果。
