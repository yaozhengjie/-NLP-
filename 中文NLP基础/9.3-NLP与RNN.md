9.3 NLP与RNN

前面提到关于NLP序列标注的几个基本任务，如分词、词性标注、未登录词识别、语义组块等。一个神经网络如果能够应用于NLP，则必须能够解决序列标注的问题。而循环神经网络（Recurrent Neural Networks, RNNs）通过使用反向传播和记忆的机制，能够处理任意长度的序列，在架构上比前馈神经网络更加符合生物神经网络的结构。因此，其正是为了解决这类问题应运而生的。

RNN及改进的LSTM等深度学习模型都是基于神经网络而发展起来的认知计算模型。从原理来看，它们都源于认知语言学中的“顺序象似性”原理：文字符号与其上下文构成了一个“像”，这个“像”可以被认为是符号与符号的组合——词汇，也可以被认为是词汇与词汇的句法关系——依存关系。算法的训练过程，是通过正向和反馈两个过程从训练语料中学习出识别这些“像”的能力，并记录下识别“像”的模型数据，当输入新的句子时，算法可以利用存储的模型数据识别出新输入中类似的“像”。

如图9.14所示，共分为5部分，从左到右，第一幅图是一个输入（单一标签）对应一个输出（单一标签），即“one to one”方式；第二幅图为一个输入对应多个输出，即“one to many”方式，这种网络架构广泛用于图片的对象识别领域，即输入的是一张图片，输出的是一个文本序列；第三幅图为多个输入对应一个输出，即“many to one”方式，这种网络架构广泛用于文本分类或视频片段分类的任务，输入的是视频或文本集合，输出的是类别标签；第四幅图是多个输入对应有间隔的多个输出，即第一种“many to many”方式，这种网络架构就是机器翻译系统，输入的是一种语言的文本，输出的是另一种语言的文本；第五幅图是多个输入严格对应多个输出，即第二种“many to many”方式，这种网络架构就是NLP中广泛使用的序列标注任务。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784805508-42bb8e75-6455-4204-9d48-46eb587711e6.jpeg)

图9.14 5种不同的RNN架构

在众多的深度学习网络中，循环神经网络由于能够接收序列输入，也能得到序列输出，在自然语言处理中取得了巨大成功，并得到广泛应用。

### 9.3.1 Simple-RNN

本节介绍的RNN是最简单的循环神经网络，称为Simple-RNN，它是后面LSTM的基础。从网络架构上，Simple-RNN与BP神经网络一脉相承，两个网络都有前馈层和反馈层，但Simple-RNN引入了基于时间（状态）的循环机制，因此而发展了BP网络。下面从整体上考察Simple-RNN的架构和训练运行。

图9.15所示为Simple-RNN的神经网络示意图。神经网络为A，通过读取某个时间（状态）的输入xt，然后输出一个值ht。循环可以使得信息从当前时间步传递到下一时间步。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784806052-4e60a507-a8e6-4b44-b0ff-4eceb3dab6f4.jpeg)

图9.15 Simple-RNN的神经网络示意图

这些循环使得RNN可以被看作同一网络在不同时间步的多次循环，每个神经元会把更新的结果传递给下一时间步。为了更清楚地说明，将这个循环展开，放大图9.15中的神经网络A，考察一下网络的细节。

如图9.16所示，网络某一时刻的输入xt，与之前介绍的BP神经网络的输入一样，xt是一个n维向量，不同的是，递归网络的输入将是一整个序列，也就是x=[x0, …, xt-1, xt, xt+1, …xT]，对于语言模型，每一个xt将代表一个词向量，一整个序列就代表一句话。ht代表时刻t的隐含状态，yt代表时刻t的输出。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784806554-33834722-089b-4c5c-b661-afb70165696d.jpeg)

图9.16 RNN神经元在不同时间步的传递

❑ 输入层到隐含层直接的权重由U表示，它将原始输入进行抽象作为隐含层的输入。

❑ 隐含层到隐含层的权重W，它是网络的记忆控制者，负责调度记忆。

❑ 隐含层到输出层的权重V，从隐含层学习到的表示将通过它再一次抽象，并作为最终输出。

将RNN展开之后，似乎一切都很明了，前向传播（Forward Propagation）依次按照时间的顺序计算一次即可，反向传播（Back Propagation）从最后一个时间将累积的残差传递回来即可，跟普通的BP神经网络训练并没有本质上的不同。由于加入了时间顺序，计算的方式有所不同，这称为BPTT（Back Propagation Through Time）算法。该算法的细节如下。

1．前向传播

首先在t=0的时刻，U、V、W都被随机初始化好，h0通常初始化为0，然后进行如下计算。

h1+f(U. x1+W.h0)

y1+g(V.h1)

其中，f(·)是隐含层激活函数，g(·)是输出层激活函数。一般，f(·)可以选择tanh、relu、logistic等；如果序列标注问题，则g(·)一般选择Softmax。h1为第一层的输出，y1为预测的结果。

这样，当在时间步t时，公式变为如下的形式。

ht+f(Uxt+Wht-1)

yt+g (Vht)

需要注意的是，循环神经网络是拥有记忆能力的，而这种能力是通过W记录了以往的输入状态，作为下次的辅助输入。隐藏状态可以理解为如下内容。

h=f （现有的输入+过去记忆）

其中，全局误差如下。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784807047-9cd238cc-1478-4a8b-ba93-649afbd75fb5.jpeg)

这里E是全局误差，ei是第i个时间步长的误差，y是输出层预测结果，d是实际结果。误差函数fe的选择可以是交叉熵（Cross Entropy），也可以是平方误差项等。

2．反向传播

这与之前讲解的BP神经网络用到的反向传播方法的思想是一致的，也就是利用输出层的误差e(Cost Function)，求解各个权重的梯度∇V、∇U、∇W，然后利用梯度下降法更新各个权重。

求解各个权重的递推公式如下。

V (t+1)+V(t)+α.▽V

U (t+1)+U(t)+α.▽U

W (t+1)+W(t)+α. ▽W

现在的问题就是如何求解各个权重的梯度。即我们的目标就是要求取：

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784807500-4b1906c2-2b52-4291-9aa0-9baa2c94ec18.jpeg)

求解的顺序分为如下两步，首先我们知道ht+ f(Uxt+Wht-1)，对于任何代价函数，直接求取每一时刻的![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784807769-fbb33c40-562e-452b-9ee0-e82c4e847eec.jpeg)，得到∇V，由于它不依赖之前的状态，可以直接求导获得。然后简单加和即可。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784808262-c3700cab-ee34-40df-b323-00e0df51f294.jpeg)

但是∇U、∇W依赖于之前的状态，不能直接求导，需要定义中间变量：

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784808832-1544bfb8-0872-4fa5-8c0c-671c2f7779fa.jpeg)

首先计算出输出层的δy，再向后传播至各层δh，依次类推，直至输入层：

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784809270-60b39260-6a48-405c-b686-aa62b57e8a53.jpeg)

这里．*表示点积，只要计算出δy，以及后面所有的δh，即可通过如下计算出∇U、∇W。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784809729-7b760288-3e8c-4084-8838-94ff9b54a8d9.jpeg)

为了便于读者进一步理解公式，这里给出Simple-RNN的部分源代码，内容如下。

​      \#3. 主程序--训练过程：

​      for j in xrange(maxiter):

​          \# 在实际应用中，可以从训练集中查询到一个样本： 生成形如[a] [b]--> [c]的样本：

​          a, a_int, b, b_int, c, c_int = gensample(dataset, largest_number)

​          \# 初始化一个空的二进制数组，用来存储神经网络的预测值

​          d = np.zeros_like(c)

​          overallError = 0 # 重置全局误差

​          layer_2_deltas = list(); # 记录layer 2的导数值

​          layer_1_values = list(); # layer 1的值。

​          layer_1_values.append(np.zeros(hidden_dim)) # 初始化时无值，存储一个全零的向量

​          \# 正向传播过程：逐个bit位(0,1)的遍历二进制数字。

​          for position in xrange(binary_dim):

​              indx = binary_dim - position - 1 # 数组索引7,6,5, ...,0

​              \# X 是样本集的记录，来自a[i]b[i]; y是样本集对应的标签，来自c[i]

​              X = np.array([[a[indx], b[indx]]])

​              y = np.array([[c[indx]]]).T

​              \# 隐含层 (input ～+ prev_hidden)

​              \# 1. np.dot(X, synapse_I)：从输入层传播到隐含层：输入层的数据＊（输入层—隐含层的

  权值）

​              \# 2. np.dot(layer_1_values[-1], synapse_h)：从上一次的隐含层[-1]到当前的隐含

  层：上一次的隐含层权值＊当前隐含层的权值

​              \# 3. sigmoid(input + prev_hidden)

​              layer_1 = sigmoid(np.dot(X, synapse_I) +np.dot(layer_1_values[-1], synapse_h))

​              \# 输出层 (new binary representation)

​              \# np.dot(layer_1, synapse_O)：它从隐含层传播到输出层，即输出一个预测值。

​              layer_2 = sigmoid(np.dot(layer_1, synapse_O))

​              \# 计算预测误差

​              layer_2_error = y - layer_2

​              layer_2_deltas.append((layer_2_error)＊dlogit(layer_2)) # 保留输出层每个

​    时刻的误差，用于反向传播

​                overallError += np.abs(layer_2_error[0]) # 计算二进制位的误差绝对值的总和，标量

​                d[indx] = np.round(layer_2[0][0]) # 存储预测的结果——显示使用

​                layer_1_values.append(copy.deepcopy(layer_1)) # 存储隐含层的权值，以便在下

​    次时间迭代中能使用

​            future_layer_1_delta = np.zeros(hidden_dim) # 初始化下一隐含层的误差

​            \# 反向传播：从最后一个时间点开始，反向一直到第一个： position索引0,1,2, ...,7

​            for position in xrange(binary_dim):

​                X = np.array([[a[position], b[position]]])

​                layer_1 = layer_1_values[-position-1] # 从列表中取出当前的隐含层。从最后一层

​    开始，-1, -2, -3

​                prev_layer_1=layer_1_values[-position-2]# 从列表中取出当前层的前一隐含层。

​                layer_2_delta = layer_2_deltas[-position-1] # 取出当前输出层的误差

​                \# 计算当前隐含层的误差：

​                \# future_layer_1_delta.dot(synapse_h.T): 下一隐含层误差＊隐含层权重

​                \# layer_2_delta.dot(synapse_O.T)：当前输出层误差＊输出层权重

​                \# dlogit(layer_1)：当前隐含层的导数

​                layer_1_delta = (future_layer_1_delta.dot(synapse_h.T) +layer_2_delta.

​    dot(synapse_O.T)) ＊dlogit(layer_1)

​                \# 反向更新权重： 更新顺序输出层-->隐含层-->输入层

​                \# np.atleast_2d：输入层reshape为2d的数组

​                synapse_O_update += np.atleast_2d(layer_1).T.dot(layer_2_delta)

​                synapse_h_update += np.atleast_2d(prev_layer_1).T.dot(layer_1_delta)

​                synapse_I_update += X.T.dot(layer_1_delta)

​                future_layer_1_delta = layer_1_delta # 下一隐含层的误差

​            \# 更新三个权值

​            synapse_I += synapse_I_update ＊ alpha

​            synapse_O += synapse_O_update ＊ alpha

​            synapse_h += synapse_h_update ＊ alpha

​            \# 所有权值更新项归零

​            synapse_I_update ＊= 0;    synapse_O_update ＊= 0;    synapse_h_update ＊= 0

​            \# 逐次打印输出

​            showresult(j, overallError, d, c, a_int, b_int)

相信读者通过公式和代码比对，能够理解源码的含义。由于篇幅限制，这里就不给出Simple-RNN的案例代码，Simple-RNN的Python实现源码参见网址：http://www. threedweb. cn/thread-1595-1-1.html，感兴趣的读者可从上述网址下载。

虽然Simple-RNN实现了循环网络的最初雏形，但是在实际应用中，使用得并不多。主要原因有如下两个。

❑ 因为我们的网络是根据输入而展开的，输入越长，展开的网络就越深，对于“深度”网络训练的困难最常见的是“梯度爆炸（Gradient Explode）”和“梯度消失（Gradient Vanish）”的问题。这在Yoshua Bengio的论文On the difficulty of training recurrent neural networks有详细的说明，这里不再说明。

❑ Simple-RNN善于基于先前的词来预测下一个词，但在一些更加复杂的场景中，例如，“我出生在法国……我能讲一口流利的法语。”“法国”和“法语”则需要更长时间的预测，而当上下文之间的间隔不断增大时，Simple-RNN会丧失学习到连接如此远的信息的能力。

### 9.3.2 LSTM原理

Long Short Term Memory Networks，即人们常称作“LSTM”的神经网络，它是RNN一个变种，专门用于解决Simple-RNN的上述两个问题。Hochreiter & Schmidhuber早在1997年就提出了LSTM网络，后来Alex Graves对其进行了改良和推广。在NLP的很多问题上，LSTM都取得了巨大的成功，并得到了广泛的使用。

LSTM通过对循环层的刻意设计来避免长期依赖和梯度消失等问题。长期信息的记忆在LSTM中是默认行为，而无须付出代价即可获得此能力。

从网络主体上来看，RNN和LSTM是相似的，都具有一种循环神经网络的链式形式。在标准的RNN中，这个循环节点只有一个非常简单的结构，如一个tanh层。LSTM的内部要复杂得多，在循环的阶段内部拥有更复杂的结构，即4个不同的层来控制信息的交互。

1．LSTM整体架构图与图例

为了更清晰地观察LSTM的内部结构，这里借用了colah's blog（http://colah.github.io/posts/2015-08-Understanding-LSTMs/）中相关的图片，如图9.17所示，并根据9.3.3节给出的程序代码，在参数上做了修改。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784810783-e9018d4c-ed44-4dba-9a6c-eb93f0ec099a.jpeg)

图9.17 LSTM中4个交互的层

首先给出一些必要的图例（见图9.18）。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784811572-f2080b06-a13d-4dcf-8667-5bf046617256.jpeg)

图9.18 LSTM神经网络的图例

在图9.18中，黄色的矩阵表示学习到的神经网络层；粉色的圆圈代表逐点（Pointwise）的运算，如“+”表示向量求和，“×”表示向量的乘积；每一条黑线传输着一个向量，从一个节点的输出到其他节点的输入，而合在一起的线表示向量的连接；分开的线表示内容被复制，然后分发到不同的位置。

如图9.19所示，LSTM中在图上方贯穿运行的水平线指示了隐含层中神经细胞（cell）的状态S，细胞状态类似于传送带，只与少量的线交互。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784812444-6dd8c64f-3334-43ca-84a7-abf8d455accb.jpeg)

图9.19 细胞的状态

数据直接在整个链上运行，信息在上面流传会很容易地保持不变。状态S的变化受到控制门的影响。

如图9.20所示，LSTM有通过精心设计的称作“门”的结构来去除或者增加信息到细胞状态的能力。门是一种让信息选择式通过的方法。其包含一个Sigmoid神经网络层和一个Pointwise乘法操作。LSTM拥有三个门，来保护和控制细胞状态。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784812840-cbc45918-5dec-460d-a604-f7220884d9fc.jpeg)

图9.20 控制门

前文讲过，Sigmoid层输出0到1之间的数值，描述每个部分有多少量可以通过。0代表“不许任何量通过”,1代表“允许任意量通过”。

2．逐步理解LSTM

如图9.21所示，首先，决定从细胞状态中丢弃什么信息。这个决策是通过一个称为“忘记门”的层来完成的。该门会读取ht-1和xt，使用sigmoid函数输出一个在0～1之间的数值，输出给在状态St-1中每个细胞的数值，1表示“完全保留”,0表示“完全舍弃”。

ft+σ(Wf.[ht-1,xt]+bf)

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784813449-b3ea7d69-cff6-40f2-9bea-be2ea8c2bcb2.jpeg)

图9.21 遗忘门

如图9.22所示，然后确定什么样的新信息被存放在细胞状态中。这里包含两部分：一部分是Sigmoid层，称为“输入门”，它决定我们将要更新什么值；另一部分是tanh层，创建一个新的候选值向量Gt，它会被加入到状态中。这样，就能用这两个信息产生对状态的更新。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784813901-b61fca6b-9edf-4f28-ac3b-c4ce0a2bed14.jpeg)

图9.22 输入与更新

it+σ(Wi.[ht-1,xt]+bi)

Gt +tanh(WG.[ht-1,xt]+bG)

如图9.23所示，现在是更新旧细胞状态的时间了，St-1更新为St。前面的步骤已经决定了将会做什么，现在就是实际去完成。把旧状态与ft相乘，丢弃掉我们确定需要丢弃的信息。接着加上it*Gt。这就是新的候选值，根据更新每个状态的程度进行变化。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784814952-9f4822c4-e629-4167-9e47-1e04f97d86e2.jpeg)

图9.23 更新细胞状态

在语言模型的例子中，这就是我们实际根据前面确定的目标，丢弃旧代词的类别信息并添加新的信息的地方。

St+ ft*St-1+it*Gt

如图9.24所示，最终，需要确定输出什么值。这个输出将会基于细胞状态，但也是一个过滤后的版本。首先，运行一个Sigmoid层来确定细胞状态的哪个部分将输出出去。接着，把细胞状态通过tanh进行处理（得到一个在-1～1之间的值）并将它和Sigmoid门的输出相乘，最终仅仅会输出我们确定输出的那部分。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784815809-c0af38fc-c00a-461e-9244-404f2b11b5a4.jpeg)

图9.24 输出信息

ot+σ(Wo[ht-1,xt]+bo)

ht +ot*tanh(St)

与RNN相同，都要最小化损失函数l(t)。下面用h(t)表示当前时刻的隐含层输出，y(t)表示当前时刻的输出标签，参考在后面的代码使用的是平方差损失函数，则损失函数被表示为：

l(t) + f (h(t), y(t))=||h(t)-y(t)||2

全局化的损失函数如下。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784816504-f203721e-851e-4438-8c72-1e91e3eac1d4.jpeg)

通过梯度法，实现损失函数最小化的参数估计。由于损失函数l(t)依赖于隐含层h(t)和输出层y(t)，根据链式法则，得到下式：

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784816976-fa58d2ef-0bc1-4b15-bd95-c65534ff8bc0.jpeg)

这里w是模型权重的标量，M是记忆单元的长度，i是隐含层的第i个记忆单元，hi(t)是一个标量，表示隐含层第i个记忆单元的输出。由于网络随时间前向传播，改变hi(t)将不会影响到先于时间t的损失。

引入一个变量L(t)，它表达了第t步开始到结束的损失，如果t=1，则计算全局误差。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784818427-c3788b6e-b694-46e1-ac8e-96d3f78104a2.jpeg)

上述函数变更如下。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784819655-f390e600-5007-41a5-9b68-f1910334c87a.jpeg)

求解这个式子的最优化结果：

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784820259-cbd66fc1-dbe7-4b61-a602-4b869eb9ea24.jpeg)

联立上述两个式子得到：

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784820779-d89efe57-faa9-4920-ba42-014b674933e6.jpeg)

上式的右侧第一项![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784821273-dc05a9ed-10a1-4cf8-9507-927fc08db21d.jpeg)来自简单的损失函数l(t)的时间t的导数。第二项![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784822214-99ac0d4f-5923-469e-9111-a59ef5b5233e.jpeg)的本质是一个循环项，它表明，计算当前节点的导数的信息时，需要下一节点的导数信息。这与RNN网络反向传播的过程相同，这里不再详细说明。

9.3.3节给出的是LSTM的一个案例代码，读者可以对照上述公式来运行LSTM的全过程。在执行之前，先对BPTT的过程进行说明。这个说明有助于读者更清晰地理解源代码的逻辑。

BPTT算法对应的公式为![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784822906-f2677d13-a266-4347-9394-d57d896c8d93.jpeg)。变量如下。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784825628-5263fb07-4a8f-423f-a8eb-b9750f7d278e.jpeg)

然后计算：

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784826449-58186c5f-0388-4dd8-8a29-965e09bbd64b.jpeg)

这些值将随时间反向传播，加入求导后的代码如下。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784827495-ca1d18b1-24ad-452b-a5c7-0d5c4f4e268b.jpeg)

关于剩下的内容，可参见9.3.3节的代码实现。

### 9.3.3 LSTM的Python实现

下面给出整个LSTM实现的Python版本。该版本仅用于学习，便于读者了解算法执行的每个细节。

1．LSTM类

​        \# -＊- coding: UTF-8 -＊-

​        import os, sys

​        import numpy as np

​        import random

​        import math

​        \# 网络激活函数

​        def sigmoid(x):

​            return 1. / (1 + np.exp(-x))

​        \# 创建一个范围在[a, b)的维度为args的随机矩阵

​        def rand_arr(a, b, ＊args):

​            np.random.seed(0)

​            return np.random.rand(＊args) ＊ (b - a) + a

​        \# 损失函数类：

​        class LossLayer:

​            @classmethod # 计算平方损失

​            def loss(self, pred, label):

​                return (pred[0] - label) ＊＊ 2

​            @classmethod # 实际输出和期望输出之间的差

​            def bottom_diff(self, pred, label):

​                diff = np.zeros_like(pred)

​                diff[0] = 2 ＊ (pred[0] - label)

​                return diff

​        \# lstm参数类

​        class LstmParam:

​            def __init__(self, mem_cell_ct, x_dim):

​                self.mem_cell_ct = mem_cell_ct

​                self.x_dim = x_dim

​        concat_len = x_dim + mem_cell_ct

​        \# 初始化各个门的权重矩阵

​        self.wg = rand_arr(-0.1, 0.1, mem_cell_ct, concat_len)

​        self.wi = rand_arr(-0.1, 0.1, mem_cell_ct, concat_len)

​        self.wf = rand_arr(-0.1, 0.1, mem_cell_ct, concat_len)

​        self.wo = rand_arr(-0.1, 0.1, mem_cell_ct, concat_len)

​        \# 初始化各个门的偏置矩阵

​        self.bg = rand_arr(-0.1, 0.1, mem_cell_ct)

​        self.bi = rand_arr(-0.1, 0.1, mem_cell_ct)

​        self.bf = rand_arr(-0.1, 0.1, mem_cell_ct)

​        self.bo = rand_arr(-0.1, 0.1, mem_cell_ct)

​        \# 损失函数的导数：权重、偏置

​        self.wg_diff = np.zeros((mem_cell_ct, concat_len))

​        self.wi_diff = np.zeros((mem_cell_ct, concat_len))

​        self.wf_diff = np.zeros((mem_cell_ct, concat_len))

​        self.wo_diff = np.zeros((mem_cell_ct, concat_len))

​        self.bg_diff = np.zeros(mem_cell_ct)

​        self.bi_diff = np.zeros(mem_cell_ct)

​        self.bf_diff = np.zeros(mem_cell_ct)

​        self.bo_diff = np.zeros(mem_cell_ct)

​    \# 更新参数

​    def apply_diff(self, lr = 1):

​        self.wg -= lr ＊ self.wg_diff

​        self.wi -= lr ＊ self.wi_diff

​        self.wf -= lr ＊ self.wf_diff

​        self.wo -= lr ＊ self.wo_diff

​        self.bg -= lr ＊ self.bg_diff

​        self.bi -= lr ＊ self.bi_diff

​        self.bf -= lr ＊ self.bf_diff

​        self.bo -= lr ＊ self.bo_diff

​        \# 重置权重和偏置的差异为0

​        self.wg_diff = np.zeros_like(self.wg)

​        self.wi_diff = np.zeros_like(self.wi)

​        self.wf_diff = np.zeros_like(self.wf)

​        self.wo_diff = np.zeros_like(self.wo)

​        self.bg_diff = np.zeros_like(self.bg)

​        self.bi_diff = np.zeros_like(self.bi)

​        self.bf_diff = np.zeros_like(self.bf)

​        self.bo_diff = np.zeros_like(self.bo)

\# lstm状态类

class LstmState:

​    def __init__(self, mem_cell_ct, x_dim):

​        self.g = np.zeros(mem_cell_ct) # 候选值向量

​        self.i = np.zeros(mem_cell_ct) # 输入门

​        self.f = np.zeros(mem_cell_ct) # 忘记门

​        self.o = np.zeros(mem_cell_ct) # 输出门

​        self.s = np.zeros(mem_cell_ct) # 内部状态

​        self.h = np.zeros(mem_cell_ct) # 实际输出

​        self.bottom_diff_h = np.zeros_like(self.h)

​        self.bottom_diff_s = np.zeros_like(self.s)

​        self.bottom_diff_x = np.zeros(x_dim)

class LstmNode:

​    def __init__(self, lstm_param, lstm_state):

​        self.state = lstm_state # store reference to parameters and to activations

​        self.param = lstm_param

​        self.x = None # 输入层（非循环层）节点

​        self.xc = None # 非循环(non-recurrent)层输入+ 循环(recurrent)层输入

​    \#

​    def bottom_data_is(self, x, s_prev = None, h_prev = None):

​        \# if this is the first lstm node in the network

​        if s_prev is None: s_prev = np.zeros_like(self.state.s)

​        if h_prev is None: h_prev = np.zeros_like(self.state.h)

​        \# 存储数据用于反向传播

​        self.s_prev = s_prev

​        self.h_prev = h_prev

​        xc = np.hstack((x,  h_prev)) # xc(t)=[x(t), h(t-1)]

​        self.state.g = np.tanh(np.dot(self.param.wg, xc) + self.param.bg)  # 候

选值向量

​        self.state.i = sigmoid(np.dot(self.param.wi, xc) + self.param.bi) # 输入门

​        self.state.f = sigmoid(np.dot(self.param.wf, xc) + self.param.bf) # 忘记门

​        self.state.o = sigmoid(np.dot(self.param.wo, xc) + self.param.bo) # 输出门

​        self.state.s = self.state.g ＊ self.state.i + s_prev ＊ self.state.f # 内部状态

​        self.state.h = self.state.s ＊ self.state.o # 实际输出

​        self.x = x

​        self.xc = xc

​    \# 注意top_diff_s沿固定误差传递

​    def top_diff_is(self, top_diff_h, top_diff_s):

​        ds = self.state.o ＊ top_diff_h + top_diff_s

​        do = self.state.s ＊ top_diff_h

​        di = self.state.g ＊ ds

​        dg = self.state.i ＊ ds

​            df = self.s_prev ＊ ds

​            \# diffs w.r.t. vector inside sigma / tanh function

​            di_input = (1. - self.state.i) ＊ self.state.i ＊ di

​            df_input = (1. - self.state.f) ＊ self.state.f ＊ df

​            do_input = (1. - self.state.o) ＊ self.state.o ＊ do

​            dg_input = (1. - self.state.g ＊＊ 2) ＊ dg

​            \# 输入层的误差

​            self.param.wi_diff += np.outer(di_input, self.xc)

​            self.param.wf_diff += np.outer(df_input, self.xc)

​            self.param.wo_diff += np.outer(do_input, self.xc)

​            self.param.wg_diff += np.outer(dg_input, self.xc)

​            self.param.bi_diff += di_input

​            self.param.bf_diff += df_input

​            self.param.bo_diff += do_input

​            self.param.bg_diff += dg_input

​            \# 计算底层（bottom）误差

​            dxc = np.zeros_like(self.xc)

​            dxc += np.dot(self.param.wi.T, di_input)

​            dxc += np.dot(self.param.wf.T, df_input)

​            dxc += np.dot(self.param.wo.T, do_input)

​            dxc += np.dot(self.param.wg.T, dg_input)

​            \# 存储底层（bottom）误差

​            self.state.bottom_diff_s = ds ＊ self.state.f

​            self.state.bottom_diff_x = dxc[:self.param.x_dim]

​            self.state.bottom_diff_h = dxc[self.param.x_dim:]

​    class LstmNetwork():

​        def __init__(self, lstm_param):

​            self.lstm_param = lstm_param

​            self.lstm_node_list = [] #状态序列

​            self.x_list = [] # 输入序列

​        def y_list_is(self, y_list, loss_layer): # 使用损失更新预测标签

​            assert len(y_list) == len(self.x_list)

​            idx = len(self.x_list) - 1

​            \# 计算损失：第一个节点仅得到来自目标标签的误差

​            loss = loss_layer.loss(self.lstm_node_list[idx].state.h, y_list[idx])

​            diff_h = loss_layer.bottom_diff(self.lstm_node_list[idx].state.h, y_list[idx])

​            \# here s is not affecting loss due to h(t+1), hence we set equal to zero

​            diff_s = np.zeros(self.lstm_param.mem_cell_ct)

​            self.lstm_node_list[idx].top_diff_is(diff_h, diff_s)

​            idx -= 1

​            while idx >= 0:

​                loss += loss_layer.loss(self.lstm_node_list[idx].state.h, y_list[idx])

\# 累计误差

​                \# 计算各种门的误差

​                diff_h = loss_layer.bottom_diff(self.lstm_node_list[idx].state.h,

y_list[idx]) #

​                diff_h += self.lstm_node_list[idx + 1].state.bottom_diff_h #

​                diff_s = self.lstm_node_list[idx + 1].state.bottom_diff_s #

​                self.lstm_node_list[idx].top_diff_is(diff_h, diff_s) #

​                idx -= 1

​            return loss

​        \# 清空输入层

​        def x_list_clear(self):

​            self.x_list = []

​        def x_list_add(self, x): #创建输入序列x和初始状态节点

​            self.x_list.append(x)

​            if len(self.x_list) > len(self.lstm_node_list):

​                lstm_state = LstmState(self.lstm_param.mem_cell_ct, self.lstm_par

am.

x_dim) # 初始状态节点

​                self.lstm_node_list.append(LstmNode(self.lstm_param, lstm_state))

​            \# 获取t-1个输入序列

​            idx = len(self.x_list) - 1

​            if idx == 0:

​                self.lstm_node_list[idx].bottom_data_is(x)

​            else: # 创建lstm各种门所需的t-1状态

​                s_prev = self.lstm_node_list[idx - 1].state.s

​                h_prev = self.lstm_node_list[idx - 1].state.h

​                self.lstm_node_list[idx].bottom_data_is(x, s_prev, h_prev)

2．执行函数

​      \# -＊- coding: UTF-8 -＊-

​      import os, sys

​      import numpy as np

​      from lstm import ＊

​    reload(sys) # 设置 UTF-8输出环境

​    sys.setdefaultencoding('utf-8')

​    np.random.seed(0)

​    def gensamples(x_dim):

​        ylabels = [-0.5,0.2,0.1, -0.5]

​        xinputs = [np.random.random(x_dim) for i in ylabels] # 对应输出矩阵的一系列随机数

​        return xinputs, ylabels

​    if __name__ == "__main__":

​            x_dim = 50        # 输出维度

​            maxiter = 100     # 最大迭代次数

​            \# input_val->X: 50维的随机数向量； y_list->y: 每个X

i

向量对应的一个y的输出值，

一个四列

​            \# Xi[0:50] -> yi

​            input_val_arr, y_list = gensamples(x_dim)

​            \# 初始化lstm各部分参数

​            mem_cell_ct = 100  # 存储单元维度

​            concat_len = x_dim + mem_cell_ct # 输入维度与存储单元维度之和

​            lstm_param = LstmParam(mem_cell_ct, x_dim) # 初始化 lstm神经网络的参数

​            lstm_net = LstmNetwork(lstm_param) # 创建lstm神经网络对象

​            \#主程序：

​            for cur_iter in xrange(maxiter):

​                  ypredlist = []

​                  for ind in xrange(len(y_list)):

​                      lstm_net.x_list_add(input_val_arr[ind])

​                      ypredlist.append((ind, lstm_net.lstm_node_list[ind].state.h[0]))

​                  loss = lstm_net.y_list_is(y_list, LossLayer) #  计算全局损失

​                  lstm_param.apply_diff(lr=0.1) #

​                  lstm_net.x_list_clear() #

​                  if (cur_iter+1)%10==0: # 输出迭代各个阶段的值

​                      print "cur iter: ", cur_iter

​                      print "y_pred:" , ypredlist

​                      print "loss: ", loss

3．输出结果

​      cur iter:  9

​      y_pred: [(0, -0.30918477256783872), (1, -0.14732219806187546), (2, -0.15495345472929759),

  (3, -0.29029294481375578)]

​      loss:  0.26602147336

​      cur iter:  19

​      y_pred: [(0, -0.38409063297396573), (1, -0.10630445803949244), (2, -0.12412162138802935),

  (3, -0.37845034415198858)]

​      loss:  0.17226222239

​      cur iter:  29

​     y_pred: [(0, -0.41986418632264572), (1, -0.077439243435045352), (2, -0.097247992104092351),

  (3, -0.42475339468242951)]

​      loss:  0.127963104432

​      cur iter:  39

​     y_pred: [(0, -0.44039365243178058), (1, -0.05940774053788752), (2, -0.078468153734407617),

  (3, -0.44766487445042785)]

​      loss:  0.105435139785

​      cur iter:  49

​     y_pred: [(0, -0.45370090557486359), (1, -0.047704273922078329), (2, -0.065475773582741523),

  (3, -0.46014229444177157)]

​      loss:  0.092471881799

​      cur iter:  59

​     y_pred: [(0, -0.46284093342326327), (1, -0.039649304074017955), (2, -0.056131579632202749),

  (3, -0.4677425990600011)]

​      loss:  0.0842301952459

​      cur iter:  69

​     y_pred: [(0, -0.46934208481704998), (1, -0.033817602589949217), (2, -0.049131574632131657),

  (3, -0.47283057482851498)]

​      loss:  0.0785889832607

​      cur iter:  79

​     y_pred: [(0, -0.47410142969798369), (1, -0.029421553566928434), (2, -0.043704000037573132),

  (3, -0.47648821545729736)]

​      loss:  0.0745086288239

​      cur iter:  89

​      y_pred: [(0, -0.47768068403126046), (1, -0.02599952437834465), (2, -0.039376054576753423),

  (3, -0.47925662987435835)]

​      loss:  0.0714299088781

​      cur iter:  99

​      y_pred: [(0, -0.48044164497776687), (1, -0.0232657206358283), (2, -0.035845123130771074),

  (3, -0.4814314917266011)]

​      loss:  0.0690287982401
