7.2 语法语料库

在构建NLP系统时，常要用到一套高精度的NLP算法库，以及一套高质量的、广为接受的基础语料资源。有关NLP的算法情况前面已经介绍过。下面重点谈谈基础语料的选择问题。

### 7.2.1 中文分词语料库

作为基础语料使用的中文分词库（含词性标注）在网上能找到的有很多。例如，LANCAST中文语料库、浙大汉语译文语料库、搜狗mini语料库等。最常用、最著名的中文分词语料库共有两个：一个是1998年人民日报标注语料库（版本1.0，下面简称PFR语料库）；另一个是微软亚洲研究院中文分词语料库（下面简称MSR语料库）。

1．PFR语料库

PFR语料库是在得到人民日报社新闻信息中心许可的条件下，以1998年人民日报语料为对象，由北京大学计算语言学研究所和富士通研究开发中心有限公司共同制作的标注语料库。该语料库对600多万字节的中文文章进行了分词及词性标注，其被作为原始数据应用于大量的研究和论文中。PFR词性标注规范遵循《北大规范》，有关词性标签的详细内容可以参见第3章的相关内容或《人民日报标注语料库（PFR）使用说明书》。

PFR语料库是纯文本文件，文件中每一行代表一个自然段或者一个标题，一篇文章有若干个自然段，因此在语料中一篇文章是由多行组成的。在一篇文章内的段落之间是不空行的。两篇文章之间会有一个空行，表示文章的分界线。同时，下一篇文章的"篇章号-段号"都会有所改变。

下面举例给出语料的样本。

19980101-01-001-001/m迈向/v充满/v希望/n的/u新/a世纪/n ——/w一九九八年/t新年/t讲话/n （/w附/v图片/n 1/m张/q ）/w

语料的开头是编号。比如"19980101-01-001-001"表示这一自然段是1998年1月1日的第01版的第001篇文章的第001自然段，用短横线隔开的4部分按照顺序是"年月日-版号-篇章号-段号"。标号也提供了词性标注，词性固定为"m（数词）"。

标号之后，是2个单字节空格，然后开始正文。正文部分按照规范已经切分成词，并且加上标注，标注的格式为"词语/词性"，即词语后面加单斜线，再紧跟词性标记。词与词之间用2个单字节空格隔开。每段最后的词，在标记之后也有2个单字节空格，保持格式一致。

我们对该语料做了一些精简，去掉了开头的编号，删除了段落间的空行。经过对全部PFR语料（1998年1月～6月，上半年）的统计，语料库中共有词汇172 120个（相同汉字但不同词性的词汇按词性划分为多个）。除去各种符号和数字、字母构成的词汇之外，完全由汉字构成的词汇共计155 747个。词频词典可从http://www.threedweb.cn/thread-1592-1-1.html下载分析。以CRF算法作为分词算法，PFR语料库作为基础语料进行分词达到的精度在94.7%左右。

人民日报语料1998年1月的语料资源可从http://www.threedweb.cn/thread- 1591-1-1. html免费下载。另有1998年上半年的收费版本，可从http://www.hyxr.com.cn/ cp/qikan/19.htm购买。值得一提的是，有人在Google上发布了一个基于CRF++的中文分词器，项目名为Nlpbamboo，已经将PFR的1998年1月的语料训练成CRF++的模型库。该库可以从http://www.threedweb.cn/thread-1319-1-1.html查询下载。

2．MSR语料库

MSR语料库是微软研究院开发的一套中文分词基础语料库。该语料库仅对词汇做了切分，而没有给出词性标注。MSR语料库包含两套不同用途的文本：一套用于标准的词汇切分；另一套用于命名实体识别。读者可从http://www.threedweb.cn/thread-1593-1-1.html下载。下面给出该语料库的样例：

“人们 常 说 生活 是 部 教科书，而 血 与 火 的 战争 更 是 不可多得 的教科书，她 确实 是 名副其实 的‘我 的 大学’。”

文档只有正文部分，每句都已经按照规范切分成词，词与词之间用空格分开，标点符号也用空格分开。与PFR语料库相同，句与句之间用换行符分隔。经过对MSR语料（分词部分）的统计，语料库中共有词汇68 946个。除去各种符号和数字、字母构成的词汇之外，完全由汉字构成的词汇共计68 444个。MSR语料库及其词频词典也可从http://www.threedweb.cn/thread-1593-1-1.html下载分析。该贴子还包含一个基于CRF模板标签的版本MSRA语料库，主要是为了满足不同分词算法的要求。同样以CRF算法作为分词算法，MSRA语料库作为基础语料进行分词达到的精度在97%以上。

3．词频统计的实现

最后，给出PFR语料库的词频统计实现的Python代码，内容如下。

​        \# -＊- coding: utf-8 -＊-

​        import sys, os

​        from framework import ＊

​        import nltk

​        import re

​        from cStringIO import StringIO

​        reload(sys)

​        sys.setdefaultencoding('utf8')

​        def fullfreq(fcontent, sumdict):  # 全部词汇

​            sent = " ".join(fcontent.splitlines()).strip().decode("utf8")

​            sTuple=[ nltk.tag.str2tuple(t) for t in sent.split(" ") ]

​            fredist=nltk.FreqDist(sTuple) #获取统计结果

​            print len(fredist)

​            for localkey in fredist:

​            if localkey in sumdict : #检查当前词频是否在词典中存在

​                  sumdict[localkey]=sumdict[localkey]+fredist[localkey]

​        \#如果存在，将词频累加，并更新词典值

​            elif str(localkey[1]).find("None")==-1 :

​                  sumdict[localkey]=fredist[localkey] #将当前词频添加到词典中

​        hanzi= re.compile(ur"[\u4e00-\u9fa5]+") # 切分汉字

​        def hanzfreq(fcontent, sumdict):  # 仅汉字词汇

​            sent = " ".join(fcontent.splitlines()).strip().decode("utf8")

​            sTuple=[ nltk.tag.str2tuple(t) for t in sent.split(" ") if hanzi.match(t) ]

​            fredist=nltk.FreqDist(sTuple) #获取统计结果

​            print len(fredist)

​            for localkey in fredist:

​            if localkey in sumdict : #检查当前词频是否在词典中存在

​                  sumdict[localkey]=sumdict[localkey]+fredist[localkey]

​        \#如果存在，将词频累加，并更新词典值

​            elif str(localkey[1]).find("None")==-1 :

​                  sumdict[localkey]=fredist[localkey] #将当前词频添加到词典中

​        sumdict={}                          # 统计结果

​        rootdir = " nltk_data/segments/"            # 根路径

​        segpath = "swresult1998.txt"            # 1998年上半年PFR语料库的文件名

​        segcorpus = readfile(rootdir+segpath)   # 读取语料为字符串格式

​        \# fullfreq(segcorpus, sumdict)      # 包含非汉字的词频统计

​        hanzfreq(segcorpus, sumdict)        # 仅有汉字的词频统计

​        sumlist= sorted(sumdict.items(), key=lambda x:x[1], reverse=True)

​        file_str = StringIO()

​        for key in sumlist:

​            file_str.write(str(key[0][0]));     file_str.write("\t")

​            file_str.write(str(key[0][1]));     file_str.write("\t")

​            file_str.write(str(key[1]));    file_str.write("\n")

​        savefile(rootdir+"freqdict.txt", file_str.getvalue()) #freqdict.txt

​        print "ok"

在分词语料的统计中还包括字频统计、词性统计等指标，这里就不一一列出，感兴趣的读者可以自己实现。

### 7.2.2 中文分词的测评

对一个中文分词系统的测评包括如下几个方面：分词正确率、切分速度、词典或语言模型的大小、功能完备性、易扩充性和可维护性6个方面。笔者认为后三个指标属于软件工程的范畴，而前三个才是考核分词系统性能的关键指标。

一般来讲，切分速度与具体的分词算法有关，目前无论使用CRF算法还是ICTCLAS的NShort最短路径算法，对于规模不大的分词任务，切分速度都能满足需求，也不属于性能的瓶颈。而如果执行超大规模的分词任务，较为常用的方法是使用现在流行的云计算平台（阿里云、新浪云、腾讯云等）。但从单机的性能上来看，ICTCLAS查询词典可以使用双数组Trie树来存储和查询词典，性能得到较大提升。

从生成语言模型的大小来看，ICTCLAS的二元关联词典远小于CRF算法生成的基于模板的语言模型。在实践中，特别是大规模、超大规模的语料分词，ICTCLAS是首选。CRF更多用于命名实体识别或领域自适应的专业分词系统。

在各项评测指标中，最核心的指标还是对切分精度的要求。传统上我们使用的切分精度为如下公式。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784724004-bd7b8678-30cb-4f24-ae08-bc8c074d652d.jpeg)

前文所说的精度，都是通过这个公式得到的结果。除此之外，按照机器学习的精度统计，我们也提供了基于召回率和准确率的计算方法。这种方法常用于命名实体识别的评测中。

（1）召回率（Recall Ratio）。指系统在实施某单项测试时，识别出该测试项的能力。设K为某单项测试项，召回率用公式表示为如下。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784724794-c6377db8-612d-4049-878b-324765a65ecb.jpeg)

（2）精确率（Precision Ratio）。指系统在实施某一单项测试项时，辨识出该项中正确成分的能力。设K为某单项测试项，精确率用公式表示为如下。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784725846-1c2fc72d-93d9-4077-b8a8-88fc2828256f.jpeg)

例如，从新闻语料库中随机抽取了1 000个包含英译名的句子作为测试样本。这1 000个句子共含6 107个中文字符、1 537个英语译名。系统运行后，辨识出“译名”2 376个，其中有1 507个真正译名，则该系统的召回率和精确率分别如下。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784726538-e00ed115-056e-4769-b366-637ce5011636.jpeg)

该测试方法现在常用于各项NLP的序列标注任务、句法分析任务等多个方面，是较为全面衡量算法精度的通用指标。

### 7.2.3 宾州大学CTB简介

前面已经较为详细地介绍过宾州CTB汉语树库，读者对树库应用应该有了初步的了解。本节的侧重点是介绍宾州树库在使用中管理方面的方法。

宾州树库的语料代码为LDC 2013T21，语料解压后，在data目录下有4个子目录，分别为：raw、segmented、postagged和bracketed。其中，raw表示原始文本，segmented是分好词的语料，postagged为经过词性标注的语料，只有bracketed为手工标注的树库。需要说明的是，这4个目录中的语料可能由于版本更新的关系，并不是完全一一对应的，有些句子在bracketed树库中是两棵树，在其他的目录中却是一句话，但这种情况并不常见。为了减少不必要的核对的工作量，在实践中，可以统一从bracketed中抽取其他三种语料格式。

全部树库都保存在bracketed目录中，以CTB 8.0为例，bracketed（树库）目录内共有3 007个文件，可以从ctb8.0-file-list.txt文件中找到全部文件列表。这些文件共有6种不同的文件后缀，分别为：BC、BN、DF、MZ、NW和WB。每种后缀表示其保存的句法树来自不同的来源。

❑ NW:Newswire: [0001-0325, 0400-0454, 0500-0540, 0600-0885, 0900-0931, 4000-4050]。

❑ MZ:Magazine articles: [0590-0596, 1001-1151]。

❑ BN:Broadcast news:[2000-3145, 4051-4111]。

❑ BC:Broadcast conversations: [4112-4197]。

❑ WB:Weblogs: [4198-4411]。

❑ DF:Discussion forums: [5000-5558]。

在CTB中，多数保存句法树的文本文件都有如下两大类标签：句法树专用“( )”标签，以及分隔不同句法树间的辅助标签，诸如“<>”。不同后缀的文件，句法树专用标签虽然相同，其保存句法树的辅助标签有细微差别。

在获得语料后，应对源文件进行最初的解析，目的是滤掉外部辅助标签，从不同格式的文件中提取出句法树来，然后保存到数据库中。实现代码如下。

（1）数据库类：需要安装python-MySQL库和python的MySQLclient库。

​        \#! /usr/bin/python2.7

​        \# -＊- coding:utf-8 -＊-

​        try:

​            import MySQLdb

​        except ImportError:

​            raise ImportError("[E]: MySQLdb module not found! ")

​        class CMySql(object):

​            def __init__(self, host, pwd, user, db, port=3306):

​                self.Option = {"host" : host, "password" : pwd, "username" : user,

​    "database" : db, "port":port}

​                self.__connection__()

​            def __del__(self):

​                if self.__conn:  self.close()

​            def __connection__(self):

​                try:

​                      self.__conn = MySQLdb.connect( host = self.Option["host"], user =

​    self.Option["username"],

​                            passwd = self.Option["password"], db = self.Option["database"],

​                    port = self.Option["port"], charset='utf8')

​            self.__dictcursor = MySQLdb.cursors.DictCursor

​        except Exception, e:

​            print e

​            raise Exception("[E] Cannot connect to %s" % self.Option["host"])

​    def execute(self, sqlstate):  # @todo: 增、删、改—不关闭数据库

​        self.cursor = self.__conn.cursor()

​        self.cursor.execute(sqlstate)

​        self.commit()  # 执行事物

​    def insert(self, sqlstate): # @todo: 增、删、改—不关闭数据库

​        self.cursor = self.__conn.cursor()

​        self.cursor.execute(sqlstate)

​        lastinsertid = int(self.__conn.insert_id())

​        self.commit()  # 执行事物

​        return lastinsertid

​    def query(self, sqlstate): # 查询全部记录

​        self.cursor = self.__conn.cursor(self.__dictcursor)

​        self.cursor.execute(sqlstate) #查询

​        return self.cursor.fetchall()

​    def querylist(self, sqlstate, size): # 查询多条记录

​        self.cursor = self.__conn.cursor()

​        self.cursor.execute(sqlstate) #查询

​        return self.cursor.fetchmany(size)

​    def queryone(self, sqlstate): #查询一条记录

​        self.cursor = self.__conn.cursor(self.__dictcursor)

​        self.cursor.execute(sqlstate) #查询

​        return self.cursor.fetchone()

​    def close(self):

​        self.__conn.close()

​    def commit(self):

​        try:

​            self.__conn.commit()

​        except:

​            self.__conn.rollback()

​            raise

（2）数据表及字段说明如下。

​    CREATE TABLE 'penn_treebank' (

​      'Id' int(11) NOT NULL AUTO_INCREMENT,

​      'filename' varchar(255) DEFAULT NULL COMMENT ’文件名’,

​      'seq' int(11) DEFAULT NULL,

​      'sentence' text COMMENT ’原始句子’,

​      'sentsegment' text COMMENT ’分词后句子’,

​      'sentPOS' text COMMENT ’词性标注后句子’,

​      'sentTree' text COMMENT ’依存树’,

​      'flatTree' text COMMENT ’展平的句法树’,

​      PRIMARY KEY ('Id'),

​      KEY 'sent' ('sentence'(10)),

​      KEY 'segment' ('sentsegment'(10)),

​      KEY 'file_name' ('filename'),

​      KEY 'file_seq' ('seq')

​    ) ENGINE=MyISAM AUTO_INCREMENT=71373 DEFAULT CHARSET=utf8 COMMENT=’树库主表’;

（3）执行代码如下。

​    \# -＊- coding: utf-8 -＊-

​    import sys, os

​    import re

​    import traceback

​    from framework import ＊

​    from treelib import ＊

​    from nltk.tree import Tree, ParentedTree

​    from mySQL import CMySql

​    from MySQLdb import ＊

​    reload(sys)

​    sys.setdefaultencoding('utf-8')

​    rootdir = "treebanks/ctb8.0/data/bracketed/"  #树库文件所在路径

​    DBconn =  CMySql("127.0.0.1", "root", "root", "treebanks",3306)

​    filelist = os.listdir(rootdir)

​    count = 0

​    for filename in filelist:

​        linelist = readfile(rootdir+filename).splitlines()

​        linelen = len(linelist)

​        treelist = []

​        begin = end = -1; indx=0

​        while indx < linelen: # 提取句法树字符串

​            if (begin == -1 ) and linelist[indx] and linelist[indx][0]=="(":

​                      begin = indx

​                  elif (begin ! = -1 ) and ( linelist[indx]==""  or linelist[indx][0]=="<"

​    or linelist[indx][0]=="(" or indx==(linelen-1) ):

​                      end = indx

​                      if indx==(linelen-1) and linelist[indx] and linelist[indx][0]! ="(" :

​                          treelist.append("\n".join(linelist[begin:]))

​                      else:

​                          treelist.append("\n".join(linelist[begin:end]))

​                      if linelist[indx] and linelist[indx][0]=="(" : indx -= 1

​                      begin = end = -1

​                  indx +=1

​            file_seq = 0;

​            for treestr in treelist :  #生成扁平树、句子字符串、句子分词、句子词性标注

​                  fieldlist =[]

​                  sentTree = escape_string(treestr)

​                  flatTree = escape_string(treestr.replace("\n", " ").replace("\t", " "))

​                  try:

​                      mytree = Tree.fromstring(treestr)

​                      wordlist = mytree.leaves()

​                      sentSegment = escape_string(" ".join(wordlist))

​                      sentence = escape_string("".join(wordlist))

​                    sentPOS = escape_string(" ".join([word_pos[0][0]+"/"+word_pos[0][1]

​    for word_pos in flatten_deeptree(mytree).pos()]))

​                  except Exception, te: # 异常处理

​                      print filename, flatTree

​                      print te

​                      sys.exit()

​                  file_seq +=1

​                  fieldlist.append(escape_string(filename));

​    fieldlist.append(str(file_seq));

​                  fieldlist.append(sentence); fieldlist.append(sentSegment); fieldlist.

​    append(sentPOS)

​                  fieldlist.append(sentTree); fieldlist.append(flatTree);

​                  insertsql   =   "INSERT   INTO   penn_treebank   VALUES('', '"+"', '".join

​    (fieldlist)+"')"  # 插入数据库的语句

​                  count +=1

​                  DBconn.insert(insertsql)

​        print count, "ok"

在解析过程中，不同版本的CTB可能会出现报出异常的情况，此时需要手工调整一下出错的文件。异常的情况不会很多，笔者在执行时遇到的错误大约有两处，手工修改之后即完成插入。

执行结果如图7.7所示。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784726924-1aca822b-50df-4215-b323-2d5fbae23dd2.jpeg)

图7.7 CTB数据库界面

CTB数据表字段说明如表7.2所示。

表7.2 CTB数据表字段说明

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784727680-75f97e29-06dc-42b6-a762-79a74d148a7c.jpeg)

全部语料插入数据库后，共生成71 372条记录。
