10.1 句子的语义和语法预处理

前面各个章节讲解了很多NLP的功能模块及实现的算法。每个模块都处理不同的问题，包括词法分析模块：中文分词、词性标注、命名实体识别、语义组块等；句法分析模块：短语结构、依存句法分析；认知计算模块：隐喻计算、Word2Vec词向量、语义角色标注。上述模块构成了NLP的句子语义解析的核心系统。

从本节开始介绍一些新的模块，包括：复句切分和指代消解。这些模块都不同程度地用于语义的预处理工作。

复句切分与关系识别，是将复句切分为单句的形式并标注出切分后的单句之间的相互依存关系。指代消解是确定文本中的名词和代词短语所指向的真实实体的过程，主要包含人称代词消解和名词短语消解。这两个模块都属于认知领域中对事件集合的压缩和省略的一种恢复，属于认知语言学中的完形范畴。

本节介绍的有些模块刚刚发展起来，其算法和语料资源都尚不成熟；有些则已经达到了实用的精度。本节的任务是将上述所有模块整合起来，形成一个完整的NLP语义解析系统。

### 10.1.1 长句切分和融合

一般来讲，汉语句子的长度越长，其结构就越复杂。汉语是一种表义的语言，汉语句子的组织和印欧语言有很大的不同。语言中常用的断句符（句号、感叹号等），不仅表示一个句子的结束，在书面语中更多地来表示一系列事件的结束。这些事件常有相同或相似的语境（Context），构成一个句群。重复使用的概念在句群中可以被压缩或省略，这在后面的指代消解中体现得很明显，这里就不重复了。而汉语中逗号这类非断句符，除表示小句和短语停顿，也用来分割句群中的单句，有助于语义的辨析。

这种情况对句法解析的影响非常大，汉语句子的长度越长，句法分析的准确率就越低。例如，CTB 5.0的句子平均长度是27个词，目前依存句法分析的最好水平为80%；中国台湾中央研究院的Sinica Treebank句子平均长度为6个词，而同样算法分析的准确率达到了90%。因此，正确地区别出那些非断句符的断句作用，对句法分析的精度具有巨大的提升作用。

为了解决长句切分的问题，很多科研机构都做了研究，其中比较著名的是哈工大社会计算与信息检索研究中心所做一系列实验，实验结果比较成功。实验分为如下两个阶段。

1．切分阶段

我们使用的标注规则，来自《基于统计方法的汉语依存句法分析研究》（马金山）一文中标注规则。文中将以逗号、冒号、分号、句号、问号和叹号结尾的字符串称为一个片段。在本文所使用的863词性标记集中，这些标点被统一标识为wp。根据片段的语法结构，本文制定了一个分类标准，将片段分为不同的类别，类别值通过片段末尾的标点进行标识。片段的类别共分为如下5种。

（1）分句。分句是语法结构完整的片段，分句之间只有语义上的联系，在句法结构上没有联系，标识的方法是将片段末尾标点的词性标注为wp1，如：

香港/ns中华/nz总商会/n今天/nt上午/nt举行/v会员/n元旦/nt团拜/n酒会/n, /wp1新华社/ni香港/ns分社/n副/b社长/n秦文俊/nh出席/v了/u酒会/n。/wp1

（2）无宾语结构——宾语是一个从句。片段的谓语是及物动词，但是谓语和宾语之间被标点间隔，将该结构末尾的标点标识为wp3，如：

我们/r在/p调研/n过程/n中/nd了解/v到/v , /wp3目前/nt我国/n企业/n集团/n的/u发展/n总体/n上/nd是/v比较/d顺利/a的/u , /wp1但/c存在/v一些/m值得/v注意/v的/u问题/n 。/wp2

（3）无主语结构——主语脱落的情况。片段的语法结构不完整，主语被省略或者位于前面的片段之中。将该结构末尾的标点标识为wp2，如：

按/p保护价/n敞开/d收购/v , /wp2充分/d掌握/v粮源/n , /wp2才/d能/v保护/v农民/n的/u利益/n , /wp2才/d能/v做到/v顺价/d销售/v , /wp2才/d不/d会/v被/p粮贩子/n牵/v着/u鼻子/n走/v 。/wp2

（4）短语。片段由一个句法成分构成，是一个短语或者一个词，片段中无谓语动词。通常是名词短语、介词短语或者连词。将该结构末尾的标点标识为wp4，如：

今日/nt清晨/nt , /wp4在/p嘹亮/a的/u国歌声/n中/nd , /wp4拉萨/ns隆重/d举行/v升/v国旗/n仪式/n 。/wp1

（5）停顿语。片段由两个或以上的句法成分构成，并且在句法结构和语义上均不完整，标点只起停顿的作用。将该结构末尾的标点标识为wp5，如：

双方/n应/v在/p此/r基础/n上/nd , /wp5妥善/a处理/v两/m国/n间/nd存在/v的/u问题/n 。/wp3

——《基于统计方法的汉语依存句法分析研究》

现代汉语的书面语中，虽然长句通过这样的标注规则被分割为多个片段，但每个片段的长度都不大。而Sinica Treebank中的句子就是经过类似标准分割之后的片段，所以长度很小。应该说上述切分标准还是比较合理的。在进行句法分析之前，第（1）、（2）种情况必须进行切分，切分出的结果为两个分句，或一主一从，它们之间的关系是句子与句子之间的关系。第（4）、（5）种情况，是短语关系而不是句子间的关系，所以在句法分析之前，不需要切分。对于第（3）种情况，如果指代消解能够识别出脱落的主语，那么需要切分；如果没有指代消解的过程，那么需要为这种特殊的句式单独训练样本，以提高依存解析的精度。

这样，该问题就转变为一个多分类的机器学习问题。大量的实验证明，长句切分为单句最有效的算法是支持向量机（SVM）分类器。支持向量机是一种基于统计学习理论的常用机器学习算法之一，具有较好的推广能力和非线性处理能力，尤其在处理高维数据时，有效地解决了“维数灾难”问题，在人脸检测、网页分类、手写体数字识别、图像检索等领域应用广泛。

有关支持向量算法的更多细节读者可以参考有关机器学习类的相关读物，这里推荐给大家一个著名的SVM框架——Libsvm，读者可从https://www.csie.ntu.edu.tw/～cjlin/libsvm/下载。该框架是台湾大学林智仁（Lin Chih-Jen）教授等开发设计的一个简单、易于使用和快速有效的SVM模式识别与回归的软件包，它不但提供了编译好的可在Windows系统执行的文件，还提供了源代码，方便改进、修改，以及在其他操作系统上应用。该软件对SVM所涉及的参数调节相对比较少，提供了很多的默认参数，利用这些默认参数可以解决很多问题；并提供了交互检验（Cross Validation）的功能。

针对多分类的要求，哈工大社会计算与信息检索研究中心还给出了每个类别的特征，这些特征取自于经过分词和词性标注的片段。分类器所使用的特征如表10.1所示：

表10.1 分类器所使用的特征

| 特征  | 描述                     |
| ----- | ------------------------ |
| len   | 片段长度是否大于4        |
| vg    | 是否含有一般动词         |
| vx    | 是否含有系动词           |
| vt    | 是否含有及物动词         |
| vi    | 是否含有不及物动词       |
| nd    | 末尾是否为方位名词       |
| de    | 是否含有“的”             |
| dei   | 是否含有“得”             |
| shi   | 是否含有“是”             |
| you   | 是否含有“有”             |
| zhe   | 是否含有“着、了、过”     |
| vg_n  | 是否含有动宾搭配         |
| p_nd  | 是否有介词和方位词的组合 |
| tag1  | 第一个词的词性           |
| tag_1 | 最后一个词的词性         |
| punct | 片段末尾的标点           |

训练数据使用的是1998年上半年的人民日报树库，该树库共有212 527个词、1万个句子，每个句子的平均长度为21.3个词。实验以前8 000个句子作为训练数据，8 000～9 000个句子作为开发集，最后1 000个句子作为测试集。表10.2所示为5种片段在语料中的分布情况。

表10.2 5种片段在语料中的分布情况

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784863657-24f910f6-2b5c-42a5-8a7a-d1aaac47b8e7.jpeg)

分类器使用的是LibSVM，测试结果如表10.3所示。

表10.3 片段类型的识别结果

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784865113-fcfa9af7-d276-4e75-8605-93029fa560b8.jpeg)

表10.3中，wp5的片段类型识别准确率较低，主要是因为该片段在句子中的比例较少，特征数据不足，导致分类器在识别的时候产生困难。由于不对wp5进行切分，统计意义不大。

之后，根据片段识别的结果，将句子进行分割。分割的位置选择为wp1、wp2、wp3、wp4。但依照本文的观点，是否对wp4也进行切分，还有待商榷，暂且依照文献中的切分规则进行下面步骤。

2．合并阶段

由于切分之后的片段在语法上是独立的，都可作为句法分析的基本单元，即只有一个核心词与外部发生联系。也就是说，各个片段只有一个核心节点，那么只要分析出各个片段的核心节点之间的依存关系，即可将句子合并，这样就能够得到完整的依存句法树。

需要说明的是，片段之间的关系与片段内部的关系不同，片段内部的依存关系主要是词与词之间的关系，片段间的依存关系则主要是短语（wp4）或句子（wp1、wp2、wp3）之间的关系。片段之间的依存关系类型如表10.4所示。

表10.4 片段之间的依存关系类型

| 关系类型 | 描述     |
| -------- | -------- |
| IC       | 独立分句 |
| SBV      | 主谓结构 |
| VOB      | 动宾结构 |
| VV       | 连动结构 |
| ADV      | 状中结构 |
| CNJ      | 关联结构 |

在标注阶段，每个片段都被分配一种专门的标签。这样该问题又转化为一个多元分类问题，仍然使用SVM分类器进行识别。与切分阶段不同的是，此时每个片段都经过了句法分析，已经获得内部的依存关系，可以使用句法信息（节点和弧）作为特征。从片段中抽取的特征如表10.5所示。

表10.5 片段间依存关系识别的上下文特征

| 特征     | 描述                       |
| -------- | -------------------------- |
| len      | 片段长度是否大于4          |
| lchild   | 根节点与最左侧孩子的关系   |
| rchild   | 根节点与最右侧孩子的关系   |
| shi      | 根节点是否为“是”           |
| you      | 根节点是否为“有”           |
| root     | 根节点的词性               |
| subj     | 片段中是否含有主语         |
| obj      | 片段中是否含有宾语         |
| punct    | 片段末尾标点               |
| position | 根节点是否在片段首部或尾部 |

表10.5中，每个特征分别从两个片段中抽取，每个关系类型对应的特征向量共有20个。获得片段之间的依存关系之后，根据关系类型将两个片段的核心节点进行连接，形成一棵完整的依存分析树。

确定了上述特征，即可使用SVM算法识别片段之间的关系类型，仍然将Libsvm作为分类器。各个关系的识别结果如表10.6所示。

表10.6 片段间依存关系的识别结果

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784866857-b243c1ea-abfe-40ce-a38d-ed9d4b767a8a.jpeg)

经过片段合并之后，得到完整的依存分析结果，并连接成整句依存分析结果。经过统计，关系准确率、搭配准确率和句子核心词三个指标对结果进行评价，结果如表10.7所示。

表10.7 整句的依存分析结果

|            | 依存关系 | 依存搭配 | 核心词  |
| ---------- | -------- | -------- | ------- |
| 整句准确率 | 0.645 5  | 0.695    | 7 0.701 |
| 片段准确率 | 0.675 7  | 0.729 2  | 0.754   |

表10.7中，第2行表示未对句子进行片段划分，直接对整句进行依存分析的结果，第3行是按照本文所描述的基于片段的句法分析所得到的结果。从结果中能够看出，基于片段的分析与整句分析相比，依存句法分析的准确率有了较大的提高，提升了3%～5%。表明基于片段的方法对句法分析的改善具有显著的效果。从分析结果中也能够看出，句子核心词的准确率提高幅度较大，这是因为片段的长度变小，减少了其他词汇的干扰，使句法分析能够更准确地找到句子的根节点。

### 10.1.2 共指消解

共指消解是确定在文本中哪些名词短语（NPs）指代相同真实世界实体的任务。指代是一类非常复杂的语言现象，它不仅包含人称代词指代和指示代词指代，还包括零指代和一般名词短语之间的指代，每种指代之间存在较大差异，采用单一方法很难解决指代的各种情况，因此只有在对每一种指代现象进行深入研究的基础上，才能够提出较好的解决方法。

指代消解是一个需要知识资源支撑的任务，需要多级语言知识，包括句法知识、语义知识、上下文知识、甚至领域知识，在当前自然语言处理水平下，要有效地得到所需的这些知识仍然不是一件容易的事情，需要一个长期的过程进行逐渐的积累。

汉语指代消解研究的起步较晚，目前的研究还不够深入。相比于英语指代消解的研究，在汉语中指代消解的工作相对较少。汉语和英语两种语言之间的差异，使得直接借鉴英文指代消解的处理方法存在一定的困难，甚至很难确定汉语中哪些指代消解系统是具有代表性的系统。

CoNLL-2012共享任务中排名第一位的共指消解器是Stanford NLP提供的混合“基于规则/机器学习”方法的开源工具，这一方法也称为带有词汇特征的“基于规则的多路筛选（Rule-Based Multi-Pass Sieve）”。其已被证明对机器学习方法很有用。这一指代消解工具具有如下4个特征。

❑ 指称（Mention）的检测。以前的工作已经表明，抽取高质量的指称（一个共指链中的NP）对指代消解的性能具有重要的作用。指代消解器的性能——召回率和准确率受限于指称检测器。为了提高指称检测器的精度，需要改善指称剪枝的策略，指称剪枝策略决定了消解器的精度。要改善指称检测器的召回，就需要提高句法树中的指称抽取，而指称检测的召回则取决于指称抽取策略和句法解析的质量。

❑ 预处理。指称检测之后，需要使用如语法分析器和命名实体（NE）识别器等预处理的工具，来计算所提取指称的特征。指代消解器的性能由这些工具产生输出的正确性所决定。

❑ 指代算法。为了更好地了解前文所说的混合方法，我们重点关注如下三个问题。第一，混合方法是否是必需的，也就是说，如果没有混合方法会怎么样；第二，在多通道筛选方法各筛选器对整体性能做出哪些贡献；第三，筛选器的排序对性能而言有哪些影响。

❑ 基于分类器的比较。在共享的任务中，Stanford指代消解器优于普遍使用的指称线对（MP）模式的系统，是一种用于训练确定两个NP是否共指的分类器。但是，不能说Stanford指代算法优于MP模式，因为“多路筛选”并不知道哪个组件对结果做出的贡献最大（如指称检测、特征计算、解算结果）。事实上，许多以前的工作都把重点放在系统比较，而不是模型/方法。

Stanford指代消解模块使用的训练、开发和测试集是CoNLL-2012共享任务数据集，如表10.8所示。

表10.8 训练、开发和测试集的统计结果

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784867898-019c10a8-3ddd-4562-a8d0-57360842c92f.jpeg)

如表10.8中，训练集用于学习概率，开发集为调整阈值，测试集用于评估指代消解器的结果。具体来说，解析器的得分是跨越三个记分程序（MUC、B3和基于实体的CEAF）F值的未加权平均数。值得一提的是，（1）解算器不奖励正确识别的指称。（2）指称被视为正确的，当且仅当有黄金（人工校对）指称和提取指称之间的精确匹配提取。此外，省略代词、系动词和同位语结构被排除在评估之外（官方的共同任务）。

下面给出Stanford的两步指代消解器的概述。注意共享任务数据集已经提供了分词和句法解析的结果，需要单独计算。

步骤1，指称检测，使用两步法构建指称检测器。首先，在抽取阶段，从句法树的所有NP和QP节点中抽取指称。然后，进行剪枝，识别和过滤掉那些错误的抽取项。算法使用了两种剪枝策略：基于启发式的剪枝和基于学习式的剪枝。启发式是基于规则的，如果是疑问代词、专有名词或数量词，就修剪掉候选指称项。学习式是基于统计的，修剪掉那些概率值低于阈值的指称项。阈值是由开发集确定的。

步骤2，基于筛选器的指代消解。筛选器是由一个或多个手工设计规则组成的，每个筛选器都建立了两个指称的共指关系。指代消解过滤器由一组有序的筛选器构成，序列根据筛选器的精度排列。首先出现的是最精确的筛子。当给定一个文本时，指代消解用多个筛选器过滤它：如果通过第i个筛选器，则使用第i个规则来建立共指关系。

指代消解的解算器由10个筛选器构成。汉语中心词（CHM）筛选器用于识别两个具有相同中心词的指称之间的共指关系。话题处理（DP）筛选器用于处理那些第一人称和第二人称的指称情况。抽取字符串匹配（ESM）筛选器用于一个非代名词指称到具有相同字符串的指称。精确构造（PC）筛选器识别那些基于词汇和语法信息的指称之间的共指现象，诸如一个指称是否是另一个的缩写，或者指称是否处于同位结构。此外，还并入汉语特定规则用于确定一个指称是否是其他命名实体的缩写。严格头匹配（SHM A-C）筛选器是三个头词（中心词）匹配筛选器，它包含基于头词匹配的、精度逐渐降低的共指规则。专有头词匹配（PHM）筛选器只适用于专有名词的严格头词匹配的宽松版本。代词（Pro）筛选器包含从训练集合得到的特征，用于解算诸如性别和数字等共指现象。最后，词汇对（LP）筛选器用于识别基于词汇特征的指代关系。例如，一个规则指定两个指称是共指的，如果其中心词的概率大于t，那么其中t是使用开发集确定的阈值。

注意：在任何的筛选器假定两个指称作为共指之前，通常使用语言约束规则来排除非共指的情况。这些约束被实现为单一的非共指规则，指定两个指称mi和mj，如果下列五个条件之一成立，则不能为共指：（1）它们满足在“i-within-i”的约束。（2）它们指向对话中的不同讲话者，即使是相同的字符串。（3）它们在一个系动词结构中。（4）mi由两个并列的NP构成（含有“and”），而mj是其中之一。（5）mi和mj是共指的概率低于某一阈值（从训练数据计算）。

步骤3，后处理。在将其发送到计分程序之前，需要对指代部分进行后处理。具体来说，删除（1）在同位语结构中，两个指称之间的所有共指链接，以及（2）孤立的簇。

Stanford CoreNLP开放了共指消解的程序。算法更多的细节参见Deterministic coreference resolution based on entity-centric, precision-ranked rules（下载地址：http://www.mitpressjournals.org/doi/pdf/10.1162/COLI_a_00152 ）。Stanford自然语言处理小组在有关共指消解的网页（URL: http://nlp.stanford.edu/software/dcoref.shtml）中给出上述算法的评测结果。

Stanford Core NLP v3.6.0的dcoref算法，在CoNLL2011共享任务集中，获得第一名。系统升级到v3.6.0版后使用v8.01评分器，评测数据如图10.1所示。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784869037-e080c487-2f6f-4332-a8c2-835d0ce564fc.jpeg)

图10.1 评测数据

从Stanford CoreNLP 3.5.2开始，该系统加入了对汉语共指消解的支持，并在网页中给出了使用说明。由于3.6.0版本做了升级，代码与3.5.2版本出现较大变化。经过调试后的代码如下。

（1）将英文的stanford-corenlp-3.6.0-models.jar库加入到项目lib中，如图10.2左图所示为加入models之后的lib目录。如图10.2右图所示为项目当前的models子目录，该目录的内容参见第1章的配置。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784870224-6bf66b37-d0ad-48ca-bc3e-2239b10aeb39.jpeg)

图10.2 加入stanford-corenlp-3.6.0-models.jar后的目录

（2）修改项目models/hcoref/properties目录下的配置文件，文件名为zh-coref-default. Properties，代码如下。

​    \# Pipeline options

​    annotators = segment, ssplit, pos, lemma, ner, parse, mention, coref

​    coref.sieves = ChineseHeadMatch, ExactStringMatch, PreciseConstructs, StrictHeadMatch1,

StrictHeadMatch2, StrictHeadMatch3, StrictHeadMatch4, PronounMatch

​    coref.input.type = raw

​    coref.postprocessing = true

​    coref.calculateFeatureImportance = false

​    coref.useConstituencyTree = true

​    \#coref.useConstituencyTree = false

​    coref.useSemantics = false

​    coref.md.type = RULE

​    coref.mode = hybrid

​    coref.path.word2vec =

​    coref.language = zh

​    coref.print.md.log = false

​    coref.defaultPronounAgreement = true

​    \# 这个路径必须指向stanford-corenIp-3.6.0-modeIs.jar中的对应文件

​    coref.big.gender.number = edu/stanford/nlp/models/dcoref/gender.data.gz

​    \# 后面路径都指向本地的models模块路径

​    coref.zh.dict = models/dcoref/zh-attributes.txt.gz

​    \# NER

​    ner.model = models/ner/chinese.misc.distsim.crf.ser.gz

​    ner.applyNumericClassifiers = false

​    ner.useSUTime = false

​    \# segment

​    customAnnotatorClass.segment = edu.stanford.nlp.pipeline.ChineseSegmenterAnnotator

​    segment.model = models/segmenter/chinese/ctb.gz

​    segment.sighanCorporaDict = models/segmenter/chinese

​    segment.serDictionary = models/segmenter/chinese/dict-chris6.ser.gz

​    segment.sighanPostProcessing = true

​    \# sentence split

​    ssplit.boundaryTokenRegex = [.]|[! ? ]+|[。]|[! ? ]+

​    \#pos

​    pos.model = models/pos-tagger/chinese-distsim/chinese-distsim.tagger

​    \#parse

​    parse.model = models/lexparser/chinesePCFG.ser.gz

（3）共指消解源代码如下。

​      package edu.stanford.nlp.ademo;

​      import java.util.Map;

​      import java.util.Properties;

​      import edu.stanford.nlp.hcoref.CorefCoreAnnotations;

​      import edu.stanford.nlp.hcoref.CorefCoreAnnotations.CorefChainAnnotation;

​      import edu.stanford.nlp.hcoref.data.CorefChain;

​      import edu.stanford.nlp.hcoref.data.Mention;

​      import edu.stanford.nlp.ling.CoreAnnotations;

​      import edu.stanford.nlp.pipeline.Annotation;

​      import edu.stanford.nlp.pipeline.StanfordCoreNLP;

​      import edu.stanford.nlp.util.CoreMap;

​      import edu.stanford.nlp.util.StringUtils;

​      public class CorefDemo {

​          public static void main(String[] args) {

​                  String text = "苹果公司很有名，这家公司的人都很努力。";

​                  args = new String[] {"-props", "models/hcoref/properties/zh-coref-

  default.properties" };

​                  Annotation document = new Annotation(text);

​                  Properties props = StringUtils.argsToProperties(args);

​                  StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

​                  pipeline.annotate(document);

​                  System.out.println("---");

​                  System.out.println("coref chains");

​                  for (CorefChain cc : document.get(CorefCoreAnnotations.CorefChainAnnotation.

  class).values()) {

​                    System.out.println("\t" + cc);

​                  }

​                  for (CoreMap sentence : document.get(CoreAnnotations.SentencesAnnotation.

  class)) {

​                    System.out.println("---");

​                    System.out.println("mentions");

​                    for (Mention m : sentence.get(CorefCoreAnnotations.CorefMentionsAnnotation.

  class)) {

​                        System.out.println("\t" + m);

​                    }

​                  }

​          }

​      }

（4）输出结果如下。

​        \---

​        coref chains

​        \---

​        mentions

​            苹果 公司

​            这 家 公司 的 人

​            这 家 公司
