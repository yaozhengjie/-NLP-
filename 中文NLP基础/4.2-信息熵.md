4.2 信息熵

如果说概率是对事件确定性的度量，那么信息（包括信息量和信息熵）就是对事物不确定性的度量。信息熵是由香农（C. E. Shannon）在1948年发表的论文《通信的数学理论（A Mathematical Theory of Communication）》中提出的概念。他借用热力学中热熵的概念（热熵是表示分子状态混乱程度的物理量），解决了对信息的量化度量问题，也常用来对不确定性进行度量。

### 4.2.1 信息量与信息熵

信息量在数学上表示为I(X)+-log P(X)

信息熵则被定义为对平均不确定性的度量。一个离散随机变量X的信息熵H(X)定义为：

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784587130-0905f857-244e-48cf-8783-33ec5e4f5a67.jpeg)

其中，约定0log(1/0)=0，对数若以2为底，则熵的单位是比特；若以e为底，则其单位是奈特。若无特殊说明，则本书以后章节均采用比特为单位。

❑ 信息熵的本质是信息量的期望。

❑ 信息熵是对随机变量不确定性的度量。随机变量X的熵越大，说明它的不确定性也越大。若随机变量退化为定值，则熵为0。

❑ 平均分布是“最不确定”的分布。

下面举两个例子来说明这一点。

例1 图4.1考虑一个取值为0或1的随机变量X，满足(0,1)分布，记p=P(x=1)。根据熵的定义，有：

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784587699-5bbe9138-ca6d-45ab-b445-759c0265418b.jpeg)

图4.1 (0,1)分布下随机变量的熵（p=P(X=1)）

H(X)+-plogp-(1-p)log(1-p)

如图4.1所示，当p=0或p=1时，我们肯定地知道X的取值，不确定性最小，H(X)=0。当p=0.5时，对X的取值的不确定性达到最大，此时H(X)=1。该例子验证了上文中熵的性质。

例2 记X、Y和Z分别为掷硬币、掷骰子，以及从54张扑克牌中随意抽取一张的结果。显然X的不确定性最小，Y的不确定性居中，而Z的不确定性最大。与之相应，这3个随机变量的熵之间也恰恰存在这样的关系，即H(X)<H(Y)<H(Z)。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784588168-2a37854b-09f7-4c68-8673-d412b0c0ad89.jpeg)

用|X|来记作X变量的取值个数，又称为变量的势。

熵的基本性质如下。

（1）H(X) >= 0。

（2）H(X) <= log|X|，等号成立的条件，当且仅当X的所有取值x有P(X=x)=1/|X|。

### 4.2.2 互信息、联合熵、条件熵

1．互信息

如图4.2所示，回到第2章出现的这幅图，通过该图理解互信息比较容易。一般而言，信道中总是存在着噪声和干扰，信源发出消息x，通过信道后信宿只可能收到由于干扰作用引起的某种变形y。信宿收到y后推测信源发出x的概率，这一过程可由后验概率p(x|y)来描述。相应的，信源发出x的概率p(x)称为先验概率。定义x的后验概率与先验概率比值的对数为y对x的互信息量（简称互信息）。其公式如下。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784588793-0c8edd30-5ed6-46fd-a5ac-9ef436ffb1a4.jpeg)

图4.2 通信系统模型

互信息定义：![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784589271-329b5b21-96cc-41d6-bb71-17c94f13f9ff.jpeg)

互信息的性质如下。

（1）互信息可以理解为，收信者收到信息X后，对信源Y的不确定性的消除。

（2）互信息+I（先验事件）-I（后验事件）+![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784589789-a287cb23-55a3-4a22-8134-88a19e172190.jpeg)。

（3）互信息是对称的。

平均互信息：![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784590222-0bb9f1e9-0ab1-40f6-8c5b-c48b6e6d8143.jpeg)。

平均互信息又称为信息增益。

2．联合熵

联合熵是借助联合概率分布对熵的自然推广，两个离散随机变量X和Y的联合熵定义为

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784590768-f88fb251-195c-4f5f-80a2-f5ac458044c8.jpeg)

3．条件熵

条件熵是利用条件概率分布对熵的一个延伸。随机变量X的熵是用它的概率分布P(X)来定义的。如果知道另一个随机变量Y的取值为y，那么X的后验分布即为P(X|Y=y)。利用条件分布可以定义给定Y=y时X的条件熵为

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784591375-a8316e08-4bc0-4fac-8b0b-57b2edc12d95.jpeg)

熵H(X)度量的是随机变量X的不确定性，条件熵H(X|Y=y)度量的则是已知Y=y后，X的不确定性。

熵的链式规则：

H (X, Y)+H(X)+H(Y|X)+H(Y)+H(X|Y)

I (X; Y)+H(X, Y)+H(X)+H(Y)

图4.3所示为联合熵、条件熵和互信息之间的关系。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784591733-da618990-ae47-4227-978a-00fe339573f1.jpeg)

图4.3 联合熵、条件熵和互信息之间的关系

边缘独立定理：从信息论角度为“边缘独立”这一概念提供了一个直观解释，即两个随机变量相互独立当且仅当它们之间的互信息为0。

接下来考虑3个变量X、Y和Z之间的条件独立关系。条件熵H(X|Z)是给定Z时X剩余的不确定性，如果进一步再给定Y，则X剩余的不确定性变为H(Z|Z, Y)。因此，这两者之差即是给定Z时观测Y的取值会带来的关于X的信息量，即

I (X; Y|Z)+H(X|Z)-H(X|Z, Y)

称为给定Z时Y关于X的信息，容易证明I(X; Y|Z) = I(Y; X|Z)。于是，I(X; Y|Z)也称为给定Z时X和Y之间的条件互信息。

定理4-1：对任意3个离散随机变量X、Y和Z，有

（1）I(X, Y|Z)>=0。

（2）H(X|Y, Z)<=H(X|Z)。

上述定理的意义在于，它从信息论角度为随机变量之间的“条件独立”这一概念提供了一个直观解释，即给定Z，两个随机变量X和Y相互条件独立，当且仅当它们的条件互信息为零，或者说，Y关于X的信息已全部包含在Z中，从而观测到Z后，再对y进行的观测不会带来关于X的更多信息。另外，如果X和Y在给定Z时相互不独立，则H(X|Z, Y)<H(X|Z)，即在已知Z的基础上对Y的进一步观测将会带来关于X的新信息，从而降低X的不确定性。

### 4.2.3 交叉熵和KL散度

1．交叉熵

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784592387-844ce835-1d18-4dd7-abf8-a6db6aa87090.jpeg)

（1）交叉熵常用来衡量两个概率分布的差异性。

（2）在Logistic中的交叉熵为其代价函数。

2．相对熵与变量独立

相对熵定义：对定义于随机变量X的状态空间Ωx上的两个概率分布P(X)和Q(X)，可以用相对熵来度量它们之间的差异，即有

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784592792-ec541830-00fb-44e6-b1d9-c334013f8c8f.jpeg)

其中，约定![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784593715-0d90c85a-1bd5-4a4b-983a-13eb0ffe3a4c.jpeg)。KL(P, Q)又称为P(X)和Q(X)之间的KL散度。但严格来讲它不是一个真正意义上的距离，因为KL(P, Q)≠KL(Q, P)。

定理4-2：设P(X)和Q(X)为定义在某个变量X的状态空间Ωx上的两个概率分布，则有：

KL(P, Q) >= 0

其中，当且仅当P与Q相同，即P(X=x)=Q(X=x), ∀x∈Ωx时等号成立。

推论，对于满足![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784594464-1169d5f2-fd91-4d35-90e1-1a09e257b121.jpeg)的非负函数f(X)，定义概率分布P*(X)为：

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784594812-d1d02738-0087-4f73-bd31-ecf1d450ae52.jpeg)

那么，对于任意其他的概率分布P(X)，则有：

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784595142-12502850-dcad-43d4-a647-2ff82e068b4d.jpeg)

其中，当且仅当P*与P相同时等号成立。

定理4-3：互信息与变量独立之间的两个关系。首先有如下定理，对任意两个离散随机变量X和Y有：

（1）I(X, Y)>=0。

（2）H(X|Y) <= H(X)。

上述两式当且仅当X与Y相互独立时等号成立。

图4.4所示为交叉熵和KL散度之间的关系。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784595820-6c4e3f03-8601-485d-8e30-a05ea4eee0af.jpeg)

图4.4 交叉熵和KL散度之间的关系

### 4.2.4 信息熵的NLP的意义

对于任何语言系统的抽象模型都是一个信息系统，引入信息熵的本质意义在于从信息论的角度来考察一个语言系统，并且对其行为（编码和解码）提供了统一的测度。这个测度的各个方面都可以通过表4.4来说明。

表4.4 信息熵概念与公式表

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784596644-85da3838-9363-4053-b5b8-b015f63ae106.jpeg)
