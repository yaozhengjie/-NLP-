4.3 NLP与概率图模型

概率图模型结合了概率论与图论的知识，用图模式（节点和边）表达基于概率相关关系的模型的总称。最早由图灵奖获得者Pearl开发出来。其动机来源于建立一套领域无关的通用自动（智能）推理理论，从中揭示智能推理的内在机制。

我们解决非确定性问题的传统思路都是利用概率论的思想，但是随着问题的复杂性不断增加，传统的概率方法显得越来越力不从心。图模型的引入使人们可以将复杂问题得到适当的分解：其中，变量表示为节点，变量与变量之间的关系表示为边（或弧），这样就使问题得以结构化。然后，根据图的结构进行训练和计算推理得出最终的结果。因此，概率图理论就自然地分为三个部分，分别为：概率图模型表示理论、概率图模型推理理论和概率图模型学习理论。

### 4.3.1 概率图模型的几个基本问题

前文已经提到，常用的概率图模型，无论是最简单的朴素贝叶斯模型还是比较复杂的最大熵、条件随机场模型，都包含如下三个基本的问题。

❑ 模型的表示。

表示理论将图模型分为如下两个类别：贝叶斯网络（Bayesian Network）和马尔科夫随机场（Markov Random Field）。其中，贝叶斯网络采用有向无环图（Directed Acyclic Graph）来表达事件的因果关系，而马尔科夫随机场则采用无向图（Undirected Graph）来表达变量间的相互作用。

如图4.5所示，一般来说，贝叶斯网络中每一个节点都对应于一个先验概率分布或者条件概率分布。因此，整体的联合分布可以直接分解为所有单个节点所对应的分布的乘积。有关贝叶斯网络的细节知识建议参考笔者的《机器学习算法原理与编程实践》一书，本节不详细说明。对于马尔科夫随机场，由于变量之间没有明确的因果关系，它的联合概率分布通常会表达为一系列势函数（Potential Function）的乘积。通常情况下，这些乘积的积分并不等于1。因此，还要对其进行归一化才能形成一个有效的概率分布。在NLP中，最常用到的就是各种基于马尔科夫性的各种概率图模型。本章介绍的隐马尔科夫模型、最大熵模型和条件随机场都是从马尔科夫模型发展出来的。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784596964-aee992d5-1fc0-4cba-8a80-122202123be8.jpeg)

图4.5 有向图与无向图

❑ 模型的学习。

所谓的学习是指将给定的图模型，首先形式化为数学公式：模型λ=(Y, A1, A2, …, An)，其中，Ai∈An是模型的参数，标记符号Y也称为状态或状态序列，即使用的标签：词性列表、组块标签列表、BIOES标注等。在自然语言处理中，必须确定使用的语料资源库，即学习所需的样本资源库（也叫作训练集），数学上形式化为O={o1, o2, …, oT}，其中，oi∈oT是模型的输入序列，也称为观测序列。

根据训练集可以得到每个观测序列对应不同标签(Y)的概率分布：P(O|λ)。然后套入模型，估计出模型的λ=(Y, A1, A2, …, An)。其中，Ai∈An，使得模型λ得到最大的概率分布：P*(O|λ)。

因此，模型的学习精度受如下三方面的影响：第一，语料库样本集对总体的代表性。这属于影响算法精度的外因，这部分本章暂不涉及，在后面章节会有详细讲解。第二，模型算法的理论基础及所针对的问题。这是内因之一。不同模型因为原理不同，能够处理的语言问题也不同，比如朴素贝叶斯模型在处理短文本分类方面精度很高；最大熵模型在处理中文词性标注问题上表现很好；条件随机场模型处理中文分词、语义组块等方面的精度很高；Semi-CRF在处理命名实体识别方便精度很高。第三，模型算法的复杂度。这是另一个内因，但不是本质性的，属于工程问题。一般来讲，模型算法的复杂度对参数估计的精度会产生影响，如果要求参数估计的精度越高，则算法一般就越复杂，学习效率也就越低，但在推断阶段的预测结果精度就越高。在中文分词上，几种不同实现的CRF精度就有一定的差异。

❑ 模型的预测。

经过模型学习阶段之后，模型λ=(Y, A1, A2, …, An)得到了一系列的学习参数Ai∈An，即后验概率为p*(O|λ)。此时，模型即可投于实际的应用，也称为预测（推理）阶段，信息论中称为解码问题。预测过程是对一个新的输入样本ox通过学习后的模型，选取最大后验概率指向的最有可能的标签或标签序列作为预测结果。

形式上表示为根据样本ox判定每种观测序列Y的概率，那么观测序列的条件概率p(Y|ox)中最大的那个p*(Y|ox)就是预测的结果。

对于概率图模型而言，上述三个环节中学习过程是最重要的过程。后面关于模型的章节中，都会重点剖析此阶段的模型构成与实现的细节。

### 4.3.2 产生式模型和判别式模型

如图4.6所示，主要的概率图模型：朴素贝叶斯模型（NB）、隐马尔科夫模型（HMM）、最大熵模型（ME）和条件随机场（CRF）。描绘了从单类预测到序列数据预测方面的联合概率分布与条件概率分布。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784597573-d48a5844-e076-46f8-bfa9-5716b99c757b.jpeg)

图4.6 主要的概率图模型中的关系

判别式模型和产生式模型的区别与联系如表4.5所示。

表4.5 判别式模型和产生式模型的区别与联系

|          | 判别式模型（Discriminative Models）                          | 产生式模型（Generative Models）                              |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 特点     | 在有限样本条件下建立判别函数，寻找不同数据间的最优分类面，目标是实现分类 | 首先建立样本的联合概率分布，再利用模型进行推理预测。要求已知样本无穷或尽可能地大 |
| 区别     | 估计条件概率分布（p(y\|x)）                                  | 估计联合概率分布（p(x, y)）                                  |
| 联系     | 产生式模型可得到判别式模型                                   | 但判别式模型得不到产生式模型                                 |
| 常见模型 | 最大熵模型条件随机场模型Logistic regressionLinear discriminant analysisSupport vector machinesBoostingLinear regressionNeural networks | 朴素贝叶斯模型隐马尔科夫模型Gaussian mixture modelAODELatent Dirichlet allocationRestricted Boltzmann Machine |
| 优势     | 1．面向分类边界的训练，比使用纯概率方法高级。2．能清晰地分辨出类别之间的差异特征。3．可用于多类的学习和识别。4．判别式模型比产生式模型简单，容易学习 | 1．面向整体数据的分布。2．能够反映同类数据本身的相似度。3．模型可以通过增量学习得到。4．可用于数据不完整的情况 |
| 劣势     | 1．不能反映训练数据本身的特性，只能用于类别识别。2．在训练时需要考虑所有的数据元组，当数据量很大时，该方法的效率并不高。3．缺乏灵活的建模工具和插入先验知识的方法。黑盒操作：变量间的关系不可视 | 1．产生式分类器需产生的所有变量的联合概率，资源使用量大。2．分类性能不高。类别识别精度有限。3．学习和计算过程复杂 |
| 性能     | 较高                                                         | 较低                                                         |
| NLP应用  | 所有的序列表注和结构化学习                                   | 文本分类、词性标注等                                         |

如果隐马尔科夫模型是朴素贝叶斯模型的序列化模型扩展，则条件随机场可以理解为最大熵模型的序列化扩展。这两个最大熵模型和条件随机场模型称为判别式方法。这些图模型的比较在图4.6中已给出。后面的内容将逐一详细介绍隐马尔科夫模型、最大熵模型和条件随机场模型。

### 4.3.3 统计语言模型与NLP算法设计

在多年自然语言处理算法开发与建模的实践中，人们逐渐形成一套基于统计理论的NLP算法设计方法论。这套方法论最基础的环节就是统计语言模型（Statistical Language Model），它被广泛用于NLP的各项任务中。

那么，什么是语言模型呢？第2章介绍过分词的语言模型，本章将扩展这个概念到任意结构的语言模型。简单地说，统计语言模型是用来计算句子中某种语言模式出现概率的统计模型。一般自然语言的统计单位是句子，所以也看作句子的概率模型。假设W=(w1,w2, …,wn)为一个句子，这个句子有n个词，也就是n个词汇按顺序构成的字符序列，这里表示为![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784597972-50e93c81-29e3-401b-acdf-0f85097fbea0.jpeg)。前面讲过，利用贝叶斯公式进行链式分解，w1, w2, …, wn的联合概率为

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784598852-967ad5eb-2019-4eb1-95bd-f8561298c56f.jpeg)

因为我们扩展了第2章的模型，需要设法计算![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784599170-55712361-5f9f-4e3c-a41a-8dca8d863d3e.jpeg)![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784599819-c3c60400-85bd-4763-9187-90d92ad371e8.jpeg)这些语言模型的参数，得到这些参数，即可得到![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784600207-84534356-cc6e-406e-a155-86b0ef70317a.jpeg)。

为了表示方便，将上述式子表示为语言模型的形式。考虑一个长度为n的句子，位置为k的词出现的概率与其前面的所有的词都相关，也就是说与它前面k-1个词都相关，其k元语言模型可以表示为

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784600729-b7a71456-40d1-4fc7-8b1b-bbfd0fbcfed6.jpeg)

利用贝叶斯公式得到：

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784601222-e3950120-7543-406a-92a0-7d6cb5fcd3b8.jpeg) （4.1）

假设有一个能够容纳所有语言现象的语料库，即统计样本足够大时，![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784601777-2d67640a-cc8a-4ba4-a1c1-77a9f364b0d4.jpeg)就变为表达形式：

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784602222-96139c45-1168-4c41-89fe-77433ce28d43.jpeg) （4.2）

其中，![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784602483-9a25a359-4343-462e-9ee7-6a85b097a34a.jpeg)和![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784602768-fb08365e-55d4-45a2-8e35-3e63f8420d78.jpeg)分别表示wk和![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784603303-ce50e912-f30b-4feb-8b16-2ecb4a1306a8.jpeg)在语料中出现的次数。

下面来计算一下这个概率。考虑一个给定的长度为n的句子就需要计算n个参数，不妨假设语料库对应词典D的大小（词汇量）为T。那么，如果考虑长度为n的任意句子理论上就有Tn种可能。而每种可能都要计算n个参数，那么总共就需要计算nTn个参数。而这些概率计算好后，还得保存下来。因此，存储这些信息也需要很大的内存开销。当然，这是纯粹数学上的估算，实际情况并不是这样，这涉及语言中的模式问题。假设我们认为某几个汉字成词的可能性仅与它前面的一个词有关，而与它前面两个词和所有后面的词无关（这就是我们后面讲的马尔科夫性），那么当前词与其前面的词就构成了一种二元的语言模型（2-gram）。

这个概念扩展到n元，即可得出如下定义：如果一种语言现象出现的概率与它n-1个词相关，而与其前面n个词和后面的词都无关，那么这就是一个n-gram语言模型。将这个模型形式化后，式（4.1）就变为

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784603886-721da2ab-80b7-4a4a-aee9-d9b7daa9a20e.jpeg) （4.3）

则式（4.2）变为

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784604358-1a342dcb-9a50-4f8d-bef6-bce81bcd08bc.jpeg) （4.4）

以n=2为例，就有：

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784604858-52b76a89-8aa1-4c64-b8a9-648e65958e6e.jpeg) （4.5）

n-gram语言模型中n的大小的选取，需要同时考虑计算复杂度和模型效果的两个因素。考虑汉语中词典大小D=300 000个词汇。模型参数数量与n的关系如表4.6所示。

表4.6 模型参数数量与n的关系

| n          | 模型参数数量 |
| ---------- | ------------ |
| 1(unigram) | 3×105        |
| 2(bigram)  | 9×1010       |
| 3(trigram) | 27×1015      |
| 4(4-gram)  | 81×1020      |

模型参数的量级是模型长度n的指数函数{O(Nn)}。显然n不能取得太大，实际应用中最多的是采用n=3的三元模型。

需要特别说明的是语言模型的平滑问题，内容如下。

❑ 若![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784605136-aa90116e-fe2f-46aa-b698-b2f710ecafc2.jpeg)，是否认为![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784605477-8b81ddae-3fc6-454f-9c17-0b1db556ca3c.jpeg)就等于0呢？

❑ 若![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784605932-048f5682-1a55-4b60-ac91-a0a0b8e4fab8.jpeg)，是否认为![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784606279-31d35093-936c-4c3c-aa9a-9361d06dc3ee.jpeg)就等于1呢？

显然不能！但这是一个无法回避的问题，无论你的语料库有多么大，平滑化技术都不可避免，还好，在后面介绍的算法中，这个问题已经得到解决。

总结起来，n-gram模型的主要工作是在模型的训练阶段，统计语料中各种语言模式（词汇、词性串）出现的次数，并通过一定的统计算法，将它们计算好之后就存储起来。在预测阶段，当需要计算一个新的句子的概率时，只需查找到句子中可能出现的语言模型的概率参数，并将它们连乘起来即可得到最终的结果。

这是NLP算法的通用策略，这里涉及两个问题：（1）如何设计语言模型。（2）如何求解算法策略。几乎所有的NLP实践都是围绕着这两个问题展开的。

在机器学习领域最常用的策略为：对所考虑的问题建模后，先为其构造一个目标函数，然后对这个目标函数进行优化。从而求得一组最优的参数，最后利用这组最优参数——语言模型来做出预测。

在实践中有些问题受到语言理论的影响，新诞生的语言理论改变了人们对原有问题的认识，这样就需要根据新的理论设计新的语言模型。例如，在汉语句法解析的问题上，并行着两种不同的句法理论，包括转换生成语法、依存句法等。对于不同的句法理论需要设计不同的语言模型，再根据语言模型找到不同的算法。

另外，有时候某些算法策略具有坚实的理论基础及较强的普适性。人们希望将这种算法策略能够用于某些新的领域。例如，将深度学习算法用于各种序列标注问题，并与其他算法进行比较优劣，最终做出选择。这种情况下，语言模型（特征）常常在多个算法之间不会改变，需要调整的是使算法适用于特定的语言模型。

### 4.3.4 极大似然估计

如上所述，在求解机器学习、深度学习、概率图模型中，计算过程一般分为：模型的学习阶段（也称为训练阶段）和模型的预测阶段。

在训练阶段，很多模型的学习函数都常被看作一个最优化问题。模型训练以训练集样本作为已知的总体分布，来计算模型参数过程。该阶段的计算特点是把训练集样本点的取值作为已知量，然后学习出样本点对应的参数，如神经网络中的神经元的权重向量、概率图模型的节点权重值等。

这样，模型的训练过程就变成了统计学中的参数估计过程。在统计学中，参数估计的方法有很多。如果已知某个随机样本的总体概率分布（训练集），但是其中具体的参数不清楚，则参数估计就是通过若干次试验，观察其结果，利用结果推断出参数大概值的方法，称为极大似然估计。

极大似然估计是建立在如下思想上的：已知某个参数能使这个样本出现的概率最大，我们当然不会再去选择其他小概率的样本，所以干脆就把这个参数作为估计的真实值。在讲解后面的内容之前，有必要再回顾一下极大似然估计的基本原理。

极大似然估计：设总体分布为f(X, θ), X1, X2, …, Xn为该总体采样得到的样本（语料库中的样本集）, θ为待估的参数。因为X1, X2, …, Xn独立同分布，那么它们的联合概率分布为

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784606559-73e6d577-9f1c-4a11-8426-92f9d48f145d.jpeg)

因为样本已经存在，这里x1, …, xn都被看作已知量，是固定的，θ被看作未知的、待估的参数；L(x, θ)就变成了θ的函数，也就是似然函数。求这个参数θ的值，使得似然函数取极大值，这种方法就是极大似然估计。

求极大似然函数估计值的一般步骤如下。

（1）写出似然函数。

（2）对似然函数取对数，并整理。

（3）求导数。

（4）解似然方程。

似然函数常写成若干式子连乘积的形式。在实践中，由于求导数的需要，最好先将似然函数取对数，因为函数的对数其极值点不发生变化，这样就把乘号变为了加号。得到的式子称为对数似然函数。若对数似然函数可导，可通过求导的方式解出下列方程组，得到驻点，然后分析该驻点的极值性。

对于统计语言模型而言，利用最大似然可把目标函数设为：

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784606984-4a19a7a7-fa31-4c4f-869c-f9e35596a730.jpeg)

其中，C表示语料（Corpus）, Context(w)表示词w的上下文（Context或Window），即w周边的词的集合，当Context(w)为空时，就取p(w|Context(w))=p(w)。特别的，对于前面介绍的n-gram模型有：

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784607368-68ddfae7-ef10-48b1-93a6-8cbf8093f431.jpeg)

按照最大对数似然的惯例，把目标函数设为：

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784607592-2cd91fb1-e9e4-4ddd-ae34-07bc575cd0ab.jpeg)

然后对这个函数求最大化。由上式可知，概率p(w|Context(w))已被视为关于w和Context(w)的函数，即

p (w|Context(w))=F(w, Context(w), θ)

其中，θ为待定参数集。这样一来，一旦对上式进行优化得到最优参数集θ*后，F也就唯一被确定了，以后任何概率p(w|Context(w))都可以通过函数F(w, Context(w), θ*）来计算。

与n-gram相比，这种方法不需要（事先计算并）保存所有的概率值，而是通过直接计算来获取，且通过选取合适的模型可使得θ中参数的个数远小于n-gram中模型参数的个数。很显然，对于这种方法：最关键的地方就在于函数F的构造。从4.4节开始，将介绍概率图模型中各种构造F的方法。
