1.4 整合词性标注模块

词性标注（Part-of-Speech Tagging或POS Tagging)，又称为词类标注，是指判断出在一个句子中每个词所扮演的语法角色。例如，表示人、事物、地点或抽象概念的名称就是名词；表示动作或状态变化的词为动词；用来描写或修饰名词性成分或表示概念的性质、状态、特征或属性的词称为形容词，等等。

在汉语中，常用词的词性都不是固定的，也就是说，一个词可能具有多个词性，并且对于每个不同的词性，汉语词汇都没有形态的变化，这就为确定中文词性带来困难；但在另一方面，从整体上看，大多数词语，特别是实词，一般只有一两个词性，而且位于第一位词性的频次远远高于第二位的词性，即使选取最高词频作为唯一词频，系统也可实现80%准确率。因此，中文词性标注中影响词性标注精度的因素主要是要正确判断文本中那些常用词的词性。

本节主要介绍几个中文词性标注系统的应用，一般而言，中文的词性标注算法比较统一，大多数使用HMM（隐马尔科夫模型）或最大熵算法，如前文中的结巴分词的词性标注实现。为了获得更高的精度，也有使用CRF算法的，如Ltp 3.3中的词性标注。虽然分词与词性标注是两个不同的模块，但是在一般的工程应用中，语料的中文分词和词性标注通常同时完成。为了讲解的连续性，本节仍使用Ltp 3.3中的词性标注模块。

目前流行的中文词性标签有两大类：北大词性标注集和宾州词性标注集（后面章节将具体讲解两类标注集的规范和转换方法）。两类标注方式各有千秋，为了更全面地反映中文标注的概况，我们使用Stanford大学的中文词性标注模块作为另一个中文词性标注系统。

### 1.4.1 Ltp 3.3词性标注

继续使用Ltp 3.3中文词性标注模块。图1.7列出的语言包列表中，词性标注模块的文件名为pos.model。这里使用1.3节的分词结果进行标注。

​        \# -＊- coding: utf-8 -＊-

​        import sys

​        import os

​        from pyltp import ＊

​        \# 设置 UTF-8输出环境

​        reload(sys)

​        sys.setdefaultencoding('utf-8')

​        sent = "在 包含 问题 的 所有 解 的 解空间树 中 , 按照 深度优先 搜索 的 策略 , 从 根节点 出

​    发 深度 探索 解空间树 。"

​        words = sent.split(" ")

​        postagger = Postagger() # 实例化词性标注类

​        postagger.load("X:\\ltp3.3\\pos.model") # 导入词性标注模型

​        postags = postagger.postag(words)

​        for word, postag in zip(words, postags):

​            print word+"/"+postag,

词性标注结果如下。

​        在/p 包含/v 问题/n 的/u 所有/b 解/v 的/u 解空间树/n 中/nd , /wp 按照/p 深度优先/d 搜索/v

​    的/u 策略/n , /wp 从/p 根节点/n 出发/v 深度/n 探索/v 解空间树/v 。/wp

标注的标签与原词用“/”分隔，每个标签都有其语法的意义。例如，“n”表示名词，“v”表示动词。Ltp的词性标注遵从北大词性标注规范。有关词性标注的标签意义后面将有专门的章节进行讲解。

### 1.4.2 安装StanfordNLP并编写Python接口类

下面介绍如何使用StanfordNLP系统来进行中文词性标注。在使用之前，需要安装jdk环境，Stanford-CoreNLP-3.6.0版本需要安装Java 8+的环境。有关源程序可从Oracle的JDK网站中下载：http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html。

如图1.10所示，方框内为必须下载的JDK Win x64位系统。因为篇幅所限，这里省略了JDK的安装和环境变量的设置。如果读者对此安装不很清楚，建议百度一下：配置JAVA环境变量。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784423450-64712c32-5da2-46b8-9c25-146d5f89a635.jpeg)

图1.10 下载JDK环境

部署完成JDK开发环境，接下来安装Stanford NLP的语言程序包。读者可从http://stanfordnlp.github.io/CoreNLP/网页下载全套语言处理模块。本书使用的是Stanford CoreNLP 3.6.0版。Stanford全套语言包下载链接如图1.11所示。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784424186-ab56312b-0361-4c84-895e-fa26f6be501a.jpeg)

图1.11 Stanford全套语言包下载链接

下载文件名为stanford-corenlp-full-2015-12-09.zip，但stanford-corenlp-full-2015-12-09.zip只携带了英文的语言模型包，中文部分的语言模型需要单独下载，下载网址为http://nlp.stanford.edu/software/CRF-NER.shtml。Stanford全套中文模型包下载链接如图1.12所示。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784425017-542697f3-55e6-42f4-9841-2bdd417629b7.jpeg)

图1.12 Stanford全套中文模型包下载链接

在X盘下建立一个名为stanford-corenlp的目录，将下载的stanford-corenlp-full-2015-12-09.zip解压到此目录中。可以看到如图1.13所示的文件夹。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784426059-43ee6d57-fcba-4b1c-a1ff-704468b352f0.jpeg)

图1.13 Stanford-CoreNLP目录（部分）

其中，stanford-corenlp.jar为主执行文件。下载的中文模型文件stanford-chinese-corenlp-2015-12-08-models.jar解压后的目录结构如图1.14所示。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784427272-2e265ead-47b9-4c4e-9cf5-367d77a5309a.jpeg)

图1.14 下载的中文模型文件stanford-chinese-corenlp-2015-12-08-models.jar解压后的目录结构

stanford-chinese-corenlp-2015-12-08-models.jar中的中文模型全部解压到X:\stanford-corenlp的models目录中。其中，pos-tagger目录下放置了词性标注的中文模型。

为了演示方便，本书默认的操作系统为Windows 10，读者可以从http://nlp.stanford. edu/software/tagger.shtml处下载stanford-postagger.zip包，并找到stanford-postagger.bat作为执行命令行的参考脚本。

​        java -mx300m -cp "stanford-postagger.jar; " edu.stanford.nlp.tagger.maxent.MaxentTagger

​    -model %1 -textFile %2

该命令行脚本可分为如下几部分。

​        java -mx300m  # 调用java程序，以及设置的最大内存

​        -cp "stanford-postagger.jar; " # 调用的 jar 包，3.6 版共有三个 jar 包，分别为

​    stanford-postagger.jar、slf4j-api.jar、slf4j-simple.jar

​        edu.stanford.nlp.tagger.maxent.MaxentTagger # 最大熵分类器

​        -model %1  # 最大熵模型文件

​        -textFile %2  # 最大熵输入文本文件

在命令行下，重新编辑bat文件，并使用1.3节分词的结果，代码如下。

​        java -mx300m -cp "X:\\ stanfordpostagger\\stanford-postagger.jar; X:\\

​    stanfordpostagger\\lib\\slf4j-api.jar; X:\\ stanfordpostagger\\lib\\slf4j-simple.

​    jar" edu.stanford.nlp.tagger.maxent.MaxentTagger -model "X:\\ stanfordpostagger\\

​    models\\chinese-distsim.tagger" -textFile postest.txt > result.txt

其中，postest.txt文件内容如下。

​        在 包含 问题 的 所有 解 的 解空间树 中 , 按照 深度优先 搜索 的 策略 , 从 根节点 出发 深度 探

​    索 解空间树 。

输出的结果文件result.txt如下。

​        在#P 包含#VV 问题#NN 的#DEC 所有#DT 解#VV 的#DEC 解空间树#NN 中#LC , #PU 按照#P 深

​    度优先#NN 搜索#NN 的#DEC 策略#NN , #PU 从#P 根节点#NN 出发#VV 深度#JJ 探索#NN 解空间树

​    \#VV 。#PU

为了在NLTK中使用方便，新建一个stanford.py文件，专门编写了一个StanfordCoreNLP的若干个接口类，用于python调用Java编写的NLP词性标注应用。

​        \# -＊- coding: utf-8 -＊-

​        import sys

​        import os

​        \# CoreNLP 3.6 jar包和中文模型包

​        \# ejml-0.23.jar、javax.json.jar、jollyday.jar、joda-time.jar、jollyday.jar、

​    protobuf.jar、slf4j-api.jar

​        \# slf4j-simple.jar、stanford-corenlp-3.6.0.jar、xom.jar

​        class StanfordCoreNLP():  # 所有StanfordNLP的父类

​            def __init__(self, jarpath):

​                self.root = jarpath

​                self.tempsrcpath = "tempsrc" # 输入临时文件路径

​                self.jarlist = ["ejml-0.23.jar", "javax.json.jar", "jollyday.jar", "joda-

​    time.jar", "protobuf.jar", "slf4j-api.jar", "slf4j-simple.jar", "stanford-corenlp-3.6

​    .0.jar", "xom.jar"]

​                self.jarpath = ""

​                self.buildjars()

​            def buildjars(self):  # 根据root路径构建所有的jar包路径

​                for jar in self.jarlist:

​                      self.jarpath += self.root+jar+"; "

​            def savefile(self, path, sent): # 创建临时文件存储路径

​                fp = open(path, "wb")

​                fp.write(sent)

​                fp.close()

​            def delfile(self, path): # 删除临时文件

​                os.remove(path)

​        class StanfordPOSTagger(StanfordCoreNLP):  # 词性标注子类

​            def __init__(self, jarpath, modelpath):

​                StanfordCoreNLP.__init__(self, jarpath)

​                self.modelpath = modelpath # 模型文件路径

​                self.classfier = "edu.stanford.nlp.tagger.maxent.MaxentTagger" # 词性标

​    注主类

​                self.delimiter = "/"  # 标签分隔符

​                self.__buildcmd()

​            def __buildcmd(self): # 构建命令行

​                self.cmdline = 'java -mx1g -cp "'+self.jarpath+'" '+self.classfier+'

​    -model "'+self.modelpath+'" -tagSeparator '+self.delimiter

​            def tag(self, sent):   #标注句子

​                self.savefile(self.tempsrcpath, sent)

​                tagtxt  =  os.popen(self.cmdline+"  -textFile  "+self.tempsrcpath, 'r').

​    read()  # 结果输出到变量中

​                self.delfile(self.tempsrcpath)

​                return   tagtxt

​            def tagfile(self, inputpath, outpath):# 标注文件

​                os.system(self.cmdline+' -textFile '+inputpath+' > '+outpath )

### 1.4.3 执行Stanford词性标注

使用Stanford POSTagger类来进行词性标注。

​        \# -＊- coding: utf-8 -＊-

​        import sys

​        import os

​        from stanford import StanfordPOSTagger

​        \# 设置 UTF-8输出环境

​        reload(sys)

​        sys.setdefaultencoding('utf-8')

​        root = 'E:/nltk_data/stanford-corenlp/'

​        modelpath = root+"models/pos-tagger/chinese-distsim/chinese-distsim.tagger"

​        st = StanfordPOSTagger(root, modelpath)

​        seg_sent = ’在 包含 问题 的 所有 解 的 解空间 树 中 , 按照 深度优先 搜索 的 策略 , 从 根节

​    点 出发 深度 探索 解空间 树 。'

​        taglist = st.tag(seg_sent)

​        print taglist

输出结果如下。

​        在/P 包含/VV 问题/NN 的/DEC 所有/DT 解/VV 的/DEC 解空间/NN 树/NN 中/LC , /PU 按照/P 深

​    度优先/NN 搜索/NN 的/DEC 策略/NN , /PU 从/P 根节点/NN 出发/VV 深度/JJ 探索/NN 解空间/NN 树

​    /NN 。/PU

在Stanford POSTagger的分词结果中，“P”表示介词；“NN”表示一般名词；“LC”表示方位词等，由于Ltp 3.3和Stanford使用的词性标签不同，这里对标注结果不做评估。在后面章节中会对词性标注集做一个专门的评估。
