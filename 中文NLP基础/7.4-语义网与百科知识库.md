7.4 语义网与百科知识库

HowNet的发布一度成为人工构建汉语知识库的标准。近些年，随着互联网的兴起，人们不再满足于中小规模的人工知识库的构建，而是转向大规模的、自动知识库的构建理论方向。本节以维基百科为例，从理论到实践介绍了语义网理论和目前较大规模的百科知识库。

### 7.4.1 语义网理论介绍

语义网（Semantic Web）是由万维网联盟的蒂姆·伯纳斯·李（Tim Berners-Lee）在1998年提出的一个概念。当时，作为对未来网络的一个设想，它的核心是，通过给万维网上的文档（如HTML文档、XML文档）添加能够被计算机所理解的语义“元数据”（Meta Data），从而使整个互联网成为一个机器可读的、通用的信息交换媒介。也就是说，语义网设计目标是使计算机能够理解词语和概念，而且还能够理解它们之间的逻辑关系，并凭此进行逻辑推理。

语义网概念的提出具有重要的意义。一方面，它为海量的万维网资源设计了机器可读、可处理的美好前景；另一方面，它借助原有的知识库理论（类似WordNet和HowNet），作为实现这个前景的现实方法。应该讲，语义网概念的提出将传统的、狭窄的领域知识库理论技术扩展到了互联网范围。

为此，蒂姆·伯纳斯·李提出了最初的语义网体系结构，图7.23给出了语义Web的体系结构，各层的功能自下而上逐渐增强。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784756141-84a79724-8839-42f0-b8fb-a31fae0c729c.jpeg)

图7.23 语义网体系结构

如图7.23所示，第1层。基础层，主要包含Unicode和URI（Uniform Resource Identifier）。它们是语义的字符编码和所属的网络标识，是构成语义网所必需的前提。Unicode代表了一种字符编码体系；URI类似各种资源的定位ID，是构成语义节点最基本的要素。

第2层：句法层。从蒂姆·伯纳斯·李的理论来看，是一系列由XML定义的相关规范，如XML文件、命名空间等。但从实践来看，是一系列使用自然语言方式编写的、适合人类阅读的网页文本。第2层的主要目标是将这类文本解析为最基本、最完整的语义形式，为下一层编码为机器可读的RDF做准备。

为了更好地理解，下面以一个自然语言文本转换为RDF的实例来说明。假设有几个句子，它的原始形式可能是一个复句，也可能是一个段落，等等。把这些句子分拆，变为最简单的SVO形式，如表7.14所示。

表7.14 RDF最简SVO列表

| <subject>（主语） | <predicate>（谓词） | <object>（宾语）       |
| ----------------- | ------------------- | ---------------------- |
| <Bob>             | <is a>              | <person>               |
| <Bob>             | <is a friend of>    | <Alice>                |
| <Bob>             | <is born on>        | <the 4th of July 1990> |
| <Bob>             | <is interested in>  | <the Mona Lisa>        |
| <the Mona Lisa>   | <was created by>    | <Leonardo da Vinci>    |

第3层：RDF层。资源描述框架（Resource Description Framework, RDF）是一种用于描述万维网资源信息的通用框架，如网页的内容、作者以及被创建和修改的日期等。RDF本质上是一种数据模型，由主体（subject，或主语）、谓词或属性（predicate或property）、客体或属性值（object或property value，或宾语）所构成的三元组来描述资源的元数据。RDF的定义很像自然语言中单句的最基本结构——SVO（Subject Verb Object）。所以，RDF天生就具有很强的语义表达能力。它可用于表达其他元数据，如图书的书目信息、自然语言句子，以及生物、化学等许多领域表达元数据。可以说，RDF已经成为知识表达的通用形式。

RDF的基本数据模型包括资源（resource）、属性（property）及陈述（statements）。确切地说，陈述是资源和属性的一种组合。

❑ 资源。一切能够使用RDF表示的对象都称为资源，包括网络上的所有信息、虚拟概念和现实事物等。资源用唯一的URI来表示，不同的资源拥有不同的URI，通常使用的URL只是它的一个子集。

❑ 属性。用来描述资源的特征或资源间的关系。每一个属性都有其意义，用于定义资源的属性值、描述属性所属的资源形态，以及与其他属性或资源的关系。

❑ 陈述。一条陈述包含三个部分，通常称为RDF三元组<主体，属性，客体>。其中，主体是被描述的资源，用URI表示。客体表示主体在该属性上的取值，可以是另外一个资源（由URI表示）或者一个文本。

接上例，将上述表格转换为语义图（见图7.24）的形式展示出来。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784756941-f2effe82-fac3-4c5f-9707-e8aeb9833fb1.jpeg)

图7.24 语义图

最后，生成机器可读的XML格式的RDF，代码如下。

​        <? xml version="1.0" encoding="utf-8"? >

​        <rdf:RDF

​                xmlns:dcterms="http://purl.org/dc/terms/"

​                xmlns:foaf="http://xmlns.com/foaf/0.1/"

​                xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"

​                xmlns:schema="http://schema.org/">

​          <rdf:Description rdf:about="http://example.org/bob#me">

​              <rdf:type rdf:resource="http://xmlns.com/foaf/0.1/Person"/>

​              <schema:birthDate   rdf:datatype="http://www.w3.org/2001/XMLSchema#date">

​    1990-07-04</schema:birthDate>

​              <foaf:knows rdf:resource="http://example.org/alice#me"/>

​              <foaf:topic_interest rdf:resource="http://www.wikidata.org/entity/Q12418"/>

​          </rdf:Description>

​          <rdf:Description rdf:about="http://www.wikidata.org/entity/Q12418">

​              <dcterms:title>Mona Lisa</dcterms:title>

​              <dcterms:creator rdf:resource="http://dbpedia.org/resource/Leonardo_da_Vinci"/>

​          </rdf:Description>

​          <rdf:Description  rdf:about="http://data.europeana.eu/item/04802/  243FA8618938

​    F4117025F17A8B813C5F9AA4D619">

​              <dcterms:subject rdf:resource="http://www.wikidata.org/entity/Q12418"/>

​          </rdf:Description>

​        </rdf:RDF>

因为篇幅有限，这里对生成的RDF不做进一步解释，希望进一步了解RDF规范的朋友可以从http://www.threedweb.cn/thread-1436-1-1.html下载详细的规范内容。

第4层：本体层。本体（Ontology）一词源于哲学领域，指事物的本身，引申为根本。在计算机科学领域，是指一种模型，用于描述由一组对象的类型（概念或者类）、属性及关系类型所构成的世界。有关本体的详细内容与HowNet中概念构建的差异不大，都有属性、属性值、Function（事件）等要素。目前，本体的构建仍旧依赖于手工处理，大规模的本体自动构建距离还比较遥远，因此不是本文的重点。有关本体的更多细节，建议参考W3C组织提供的相关规范和教程文档，读者可从如下网址下载：http://nkos.lib.szu.edu.cn/OWL2/OWL2PrimerSimplifiedChinese.htm

第5～7层分别是逻辑层（Logic）、验证层（Proof）和信任层（Trust）。逻辑层在前面各层的基础上进行逻辑推理操作。验证层根据逻辑陈述进行验证，以得出结论。信任层是语义网安全的组成部分，与加密不同的是，该层主要负责发布语义网所能支持的信任评估。目前第6层和第7层正处于设想阶段。

由于RDF三元组是语义网数据表示的基础。要实现从万维网到语义网的转变，构建海量的RDF数据集是一项基础性工作。虽然W3C定义了RDFs、OWL等语义网规范，但互联网中的绝大多数内容都是自然语言形式的文本，尚未转换为符合语义网规范的形式。因此，如何自动化地从现有的Web内容中抽取出符合语义网规范的语义内容是语义网走向实用化面临的难题之一。

不同的公司为此开发了一系列的技术，涉及信息抽取、分类、表达、存储、查询等。其中，这些技术的核心仍旧是自然语言处理的相关技术，即如何准确地解析网页中的句子，并把句子映射为RDFs的格式。乍一看，只要有准确的分词、词性标注、句法解析系统即可，似乎这个过程并不复杂。换句话说，只要句法解析系统的精度达到商业化的要求，这个目标即可实现。但在实践中，语义表达的方式和内容却复杂得多。完全自动化地解析映射，都不会达到很好的效果。语义网正如它的名字，如果不对语义本身有透彻的认识，想要实现RDFs或OWL的自动生成，恐怕只是一个美好的前景。

### 7.4.2 维基百科知识库

不过，这些公司的努力并没有白费。很多新的网络服务形式和信息挖掘模式被开发出来，其中最引人注目的是对百科知识库的信息挖掘。在所有大型的百科知识库中维基百科（Wikipedia）是历史最悠久、影响范围最大的一个。

维基百科是一个内容公开、自由编辑且多语言的网络百科全书协作项目，通过Wiki技术使得包括用户在内的所有人都可以简单地使用网页浏览器修改其中的内容。维基百科一词取自网站核心技术“Wiki”（一种可供多人协同写作的网络技术），以及具有百科全书之意的“Encyclopedia”，共同创造出新混成词“Wikipedia”，当前维基百科由维基媒体基金会负责营运。维基百科是以众包形式，由网络志愿者共同合作编写而成的，任何使用互联网进入维基百科者都可以编写和修改里面的文章。与传统的百科全书相比，在互联网上运作的维基百科其文字和绝大部分图片使用GNU自由文件许可协议和知识共享署名的方式共享3.0协议来提供对每个人来说自由且免费的信息，任何人都可以成为条目的作者，以及在遵守协议并标示来源后直接复制、使用及发布这些内容。

以中文维基百科为例，迄今为止，中文维基百科已有889 661个条目，但这个数量还不包括对话页、没有内部链接的条目、重定向页及其他名字空间的页面，如果加上这些页面，截至2016年7月，总共有超过472万页，并有超过4 210万次的编辑。参与人数方面，共有逾2 274 930名注册用户曾进行编辑，其中编辑超过1 000次的维基人共有1 154人以上，而他们的贡献约占中文维基百科编辑总次数的73.6%。

值得庆幸的是，由于维基百科来源于一个众包项目，任何用户都可以下载其全部资源。数据资源的下载说明可从URL: https://zh.wikipedia.org/zh-cn/Wikipedia的下载链接中找到，仅中文版就提供了如下8个不同的版本。

❑ 中文版的下载地址：http://download.wikipedia.com/zhwiki/。

❑ 文言文版的下载地址：http://download.wikipedia.com/zh_classicalwiki/。

❑ 粤语版的下载地址：http://download.wikipedia.com/zh_yuewiki/。

❑ 吴语版的下载地址：http://download.wikipedia.com/wuuwiki/。

❑ 赣语版的下载地址：http://download.wikipedia.com/ganwiki/。

❑ 客家话版的下载地址：http://download.wikipedia.com/hakwiki/。

❑ 闽南语版的下载地址：http://download.wikipedia.com/zh_min_nanwiki/。

❑ 闽东语版的下载地址：http://download.wikipedia.com/cdowiki/。

最新的简体中文版维基百科数据资源可以从https://dumps.wikimedia.org/zhwiki/latest/下载。所有数据文件都使用bzip 2、gz或7-zip进行了压缩。解压后的文件主要有两种格式：xml和sql。xml文件中的内容是以RDFs格式保存的三元组，sql格式中是对应的网页数据。

国内用户无法正常登录维基百科。因此，维基百科提供了一个本地浏览版的软件——WikiTaxi（http://www.wikitaxi.org/delphi/products/wikitaxi/index），有兴趣的读者可以使用这个软件从本地浏览中文维基百科的文件。

### 7.4.3 DBpedia抽取原理

DBpedia是语义网在RDF和OWL层的应用范例，它能从维基百科的词条里抽取出结构化的数据（RDF和OWL），强化了维基百科在语义方面的搜寻功能，并将用户的数据集链接到维基百科。通过专门的语义分析和提取技术，使维基百科的庞杂知识有了许多创新而有趣的应用，如手机版本、地图整合、多面向搜寻、关系查询、文件分类与标注等。DBpedia有如下几个重要的特点。

❑ DBpedia采用RDF语法表示和组织知识，并支持基于SPARQL语法的知识查询。目前，DBpedia知识库已包含超过10.3亿的RDF三元组。

❑ 采用OWL语言创建了DBpedia本体，更好地支持基于语义Web的知识组织活动。在维基百科中，元数据以信息框（InfoBox）的形式记录和保存，不同的信息框应用频度可能不同。DBpedia从这些InfoBox中找出一些最常用的数据项，分析相互关系，用手工方式创建原始数据集的本体库。

❑ 为了对数据集进行语义分类，提高分类效果，DBpedia支持4种知识分类方法：维基百科分类方法，可支持多达41.5万个类；YAGO分类方法，可支持多达28.6万个类；UMBEL（Upper Mapping and Binding Exchange Layer）分类方法，可支持多达2万个类型；DBpedia本体，支持170个大类和720个属性。

2014版的DBpedia知识库已经拥有超过458万个物件，包括1 445 000人、735 000个地点、123 000张唱片、8 700部电影、19 000种电脑游戏、241 000个组织、251 000种物种和6 000个疾病。其资料不仅被BBC、路透社、纽约时报所采用，也是Google、Yahoo等搜寻引擎检索的对象。最新版的DBpedia全部数据可以从http://downloads. dbpedia.org/2015-10/core-i18n/zh/下载。为了便于用户的使用，DBpedia还在GitHub上发布了一系列的项目，下载网址：https://github.com/dbpedia/。

在所有的应用中，最引人注目的是DBpedia的抽取架构，如图7.25所示。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784757577-0740028f-e54d-4101-984a-7a0d3975c463.jpeg)

图7.25 DBpedia抽取架构

图7.25中部分内容说明如下。

❑ Input（输入）：读取维基百科页面作为输入．

❑ Parsing（解析）：通过一个专门的维基解析器（Wiki Parser）来解析每个维基百科的页面。维基解析器将维基百科的页面转换为一个抽象的句法树（Abstract Syntax Tree）。

❑ Extraction（抽取）：每个维基百科页面的抽象句法树都被转移到一个抽取器中。DBpedia为不同类型的文档提供了不同的抽取器，如抽取标签、摘要、地理坐标等。

❑ Output（输出）：将收集到的RDF语句写入一个容器中，然后根据需求转换为不同的RDF格式，如N-Triples等。

需要进一步说明的是有关DBpedia抽取器的一些设计细节，DBpedia提取的方式主要可以分成如下四大类。

❑ 原Infobox抽取（Raw Infobox Extraction）：直接将维基百科条目中的Infobox转换为RDF（见图7.26）。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784758386-d5929623-cb30-4545-ab04-c4d639b3bf14.jpeg)

图7.26 Infobox抽取RDF

❑ 基于映射的Infobox抽取（Mapping-Based Infobox Extraction）：手动编写Infobox到维基百科在DBpedia本体的映射关系。需要注意的是，所有值都需要规范化为基本单位。

![img](https://cdn.nlark.com/yuque/0/2021/jpeg/21473765/1631784759080-0f796a59-4a3b-4608-b9ee-643e1643a1c9.jpeg)

图7.27 Infobox映射本体

❑ 特征抽取（Feature Extraction）：采用多种抽取器，其中的每一种都只从文本中提取一个单一的特征，如地理坐标或一些标签。

如果有明显的、特定的命名模式，则可以使用特点模式抽取器。在各种领域，有各自普遍使用的命名模式，如出版界有ISBN号、金融界有ISIN号，如果这些标识被用作URI的一部分来标识特定的资源，那么就可以使用简单的模式匹配算法生成RDF不同资源之间的RDF链接。例如，RDF的BookMashup标签采用ISBN号作为URI的一部分，《哈里波特与混血王子》一书的URI为http://www4.wiwiss. fu-berlin.de/bookmashup/books/0747581088。DBpedia遍历所有图书的ISBN号，生成一个与Book Mashup相应的URI，并创建DBpedia图书与生成的URI之间的owl:sameAs链接。应用此算法，DBpedia产生了9 000个内部RDF链接。

❑ 统计抽取（Statistical Extraction）：使用了一些有关属性抽取器从页面抽取汇总数据，以便提供基于网页链接或字数统计的数据。

在没有通用标识符的情况下，将事物的属性特征作为生成自动关联的依据。例如，DBpedia与GeoNames都包含地理名称，要映射两者的重合部分，可通过识别相关属性的方法来实现。GeoNames使用了基于属性的启发式算法，基于文章题名和语义信息如经纬度、国家、行政区划、地貌特征、人口和分类等，对两个数据集分别进行属性对比，最后得到70 500个相匹配的地名，使用owl:sameAs相互链接。其他算法如BBC的关联数据，采用了词表映射、基于图的属性相似度比较等多种方式，实现与DBpedia的映射链接。

由此可见，虽然DBpedia的规模已经非常庞大，但抽取内容依然以结构化数据（InfoBox）为主，统计抽取所占的比重并不大。这说明自动化语义解析的工作仍旧处于一个实验的阶段。

但是，基于统计的抽取方式已经成为实体关系抽取的重点。国内在这方面的研究已经达到了较高的水平，其中哈工大车万翔等的论文《实体关系自动抽取》中，使用SVM分类器进行的抽取已经达到了70%以上的精度。这方面的研究已经成为NLP研究的一个非常有特色的领域，并为NLP在问答系统的应用方面做了很好的资源储备。
